<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  <link rel="shortcut icon" href="../img/favicon.ico">
  <title>Backend - Keras Documentation</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../css/highlight.css">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Backend";
    var mkdocs_page_input_path = "backend.md";
    var mkdocs_page_url = "/backend/";
  </script>
  
  <script src="../js/jquery-2.1.1.min.js"></script>
  <script src="../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../js/highlight.pack.js"></script> 
  
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-61785484-1', 'keras.io');
      ga('send', 'pageview');
  </script>
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href=".." class="icon icon-home"> Keras Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
	  
          
            <li class="toctree-l1">
		
    <a class="" href="..">Home</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Getting started</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../getting-started/sequential-model-guide/">Guide to the Sequential model</a>
                </li>
                <li class="">
                    
    <a class="" href="../getting-started/functional-api-guide/">Guide to the Functional API</a>
                </li>
                <li class="">
                    
    <a class="" href="../getting-started/faq/">FAQ</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Models</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../models/about-keras-models/">About Keras models</a>
                </li>
                <li class="">
                    
    <a class="" href="../models/sequential/">Sequential</a>
                </li>
                <li class="">
                    
    <a class="" href="../models/model/">Model (functional API)</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Layers</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../layers/about-keras-layers/">About Keras layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/core/">Core Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/convolutional/">Convolutional Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/pooling/">Pooling Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/local/">Locally-connected Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/recurrent/">Recurrent Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/embeddings/">Embedding Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/advanced-activations/">Advanced Activations Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/normalization/">Normalization Layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/noise/">Noise layers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/wrappers/">Layer wrappers</a>
                </li>
                <li class="">
                    
    <a class="" href="../layers/writing-your-own-keras-layers/">Writing your own Keras layers</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Preprocessing</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../preprocessing/sequence/">Sequence Preprocessing</a>
                </li>
                <li class="">
                    
    <a class="" href="../preprocessing/text/">Text Preprocessing</a>
                </li>
                <li class="">
                    
    <a class="" href="../preprocessing/image/">Image Preprocessing</a>
                </li>
    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../objectives/">Objectives</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../metrics/">Metrics</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../optimizers/">Optimizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../activations/">Activations</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../callbacks/">Callbacks</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../datasets/">Datasets</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../applications/">Applications</a>
	    </li>
          
            <li class="toctree-l1 current">
		
    <a class="current" href="./">Backend</a>
    <ul class="subnav">
            
    <li class="toctree-l2"><a href="#keras-backends">Keras backends</a></li>
    
        <ul>
        
            <li><a class="toctree-l3" href="#what-is-a-backend">What is a "backend"?</a></li>
        
            <li><a class="toctree-l3" href="#switching-from-one-backend-to-another">Switching from one backend to another</a></li>
        
            <li><a class="toctree-l3" href="#using-the-abstract-keras-backend-to-write-new-code">Using the abstract Keras backend to write new code</a></li>
        
            <li><a class="toctree-l3" href="#backend-functions">Backend functions</a></li>
        
        </ul>
    

    </ul>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../initializations/">Initializations</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../regularizers/">Regularizers</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../constraints/">Constraints</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../visualization/">Visualization</a>
	    </li>
          
            <li class="toctree-l1">
		
    <a class="" href="../scikit-learn-api/">Scikit-learn API</a>
	    </li>
          
            <li class="toctree-l1">
		
    <span class="caption-text">Utils</span>
    <ul class="subnav">
                <li class="">
                    
    <a class="" href="../utils/data_utils/">Data Utils</a>
                </li>
                <li class="">
                    
    <a class="" href="../utils/io_utils/">I/O Utils</a>
                </li>
                <li class="">
                    
    <a class="" href="../utils/layer_utils/">Layer Utils</a>
                </li>
                <li class="">
                    
    <a class="" href="../utils/np_utils/">Numpy Utils</a>
                </li>
    </ul>
	    </li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="..">Keras Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="..">Docs</a> &raquo;</li>
    
      
    
    <li>Backend</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="http://github.com/fchollet/keras/edit/master/docs/backend.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="keras-backends">Keras backends</h1>
<h2 id="what-is-a-backend">What is a "backend"?</h2>
<p>Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the "backend engine" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.</p>
<p>At this time, Keras has two backend implementations available: the <strong>TensorFlow</strong> backend and the <strong>Theano</strong> backend.</p>
<ul>
<li><a href="http://www.tensorflow.org/">TensorFlow</a> is an open-source symbolic tensor manipulation framework developed by Google, Inc.</li>
<li><a href="http://deeplearning.net/software/theano/">Theano</a> is an open-source symbolic tensor manipulation framework developed by LISA/MILA Lab at Université de Montréal.</li>
</ul>
<p>In the future, we are likely to add more backend options. If you are interested in developing a new backend, get in touch!</p>
<hr />
<h2 id="switching-from-one-backend-to-another">Switching from one backend to another</h2>
<p>If you have run Keras at least once, you will find the Keras configuration file at:</p>
<p><code>~/.keras/keras.json</code></p>
<p>If it isn't there, you can create it.</p>
<p>The default configuration file looks like this:</p>
<pre><code>{
    &quot;image_dim_ordering&quot;: &quot;tf&quot;,
    &quot;epsilon&quot;: 1e-07,
    &quot;floatx&quot;: &quot;float32&quot;,
    &quot;backend&quot;: &quot;tensorflow&quot;
}
</code></pre>

<p>Simply change the field <code>backend</code> to either <code>"theano"</code> or <code>"tensorflow"</code>, and Keras will use the new configuration next time you run any Keras code.</p>
<p>You can also define the environment variable <code>KERAS_BACKEND</code> and this will
override what is defined in your config file :</p>
<pre><code class="bash">KERAS_BACKEND=tensorflow python -c &quot;from keras import backend&quot;
Using TensorFlow backend.
</code></pre>

<hr />
<h2 id="using-the-abstract-keras-backend-to-write-new-code">Using the abstract Keras backend to write new code</h2>
<p>If you want the Keras modules you write to be compatible with both Theano and TensorFlow, you have to write them via the abstract Keras backend API. Here's an intro.</p>
<p>You can import the backend module via:</p>
<pre><code class="python">from keras import backend as K
</code></pre>

<p>The code below instantiates an input placeholder. It's equivalent to <code>tf.placeholder()</code> or <code>T.matrix()</code>, <code>T.tensor3()</code>, etc.</p>
<pre><code class="python">input = K.placeholder(shape=(2, 4, 5))
# also works:
input = K.placeholder(shape=(None, 4, 5))
# also works:
input = K.placeholder(ndim=3)
</code></pre>

<p>The code below instantiates a shared variable. It's equivalent to <code>tf.variable()</code> or <code>theano.shared()</code>.</p>
<pre><code class="python">val = np.random.random((3, 4, 5))
var = K.variable(value=val)

# all-zeros variable:
var = K.zeros(shape=(3, 4, 5))
# all-ones:
var = K.ones(shape=(3, 4, 5))
</code></pre>

<p>Most tensor operations you will need can be done as you would in TensorFlow or Theano:</p>
<pre><code class="python">a = b + c * K.abs(d)
c = K.dot(a, K.transpose(b))
a = K.sum(b, axis=2)
a = K.softmax(b)
a = concatenate([b, c], axis=-1)
# etc...
</code></pre>

<hr />
<h2 id="backend-functions">Backend functions</h2>
<h3 id="epsilon">epsilon</h3>
<pre><code class="python">epsilon()
</code></pre>

<p>Returns the value of the fuzz
factor used in numeric expressions.</p>
<p><strong>Returns</strong></p>
<p>A float.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; keras.backend.epsilon()
1e-08
</code></pre>

<hr />
<h3 id="set_epsilon">set_epsilon</h3>
<pre><code class="python">set_epsilon(e)
</code></pre>

<p>Sets the value of the fuzz
factor used in numeric expressions.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>e</strong>: float. New value of epsilon.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; K.epsilon()
1e-08
&gt;&gt;&gt; K.set_epsilon(1e-05)
&gt;&gt;&gt; K.epsilon()
1e-05
</code></pre>

<hr />
<h3 id="floatx">floatx</h3>
<pre><code class="python">floatx()
</code></pre>

<p>Returns the default float type, as a string
(e.g. 'float16', 'float32', 'float64').</p>
<p><strong>Returns</strong></p>
<p>String, the current default float type.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; keras.backend.floatx()
'float32'
</code></pre>

<hr />
<h3 id="set_floatx">set_floatx</h3>
<pre><code class="python">set_floatx(floatx)
</code></pre>

<p>Sets the default float type.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>String</strong>: 'float16', 'float32', or 'float64'.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; K.floatx()
'float32'
&gt;&gt;&gt; K.set_floatx('float16')
&gt;&gt;&gt; K.floatx()
'float16'
</code></pre>

<hr />
<h3 id="cast_to_floatx">cast_to_floatx</h3>
<pre><code class="python">cast_to_floatx(x)
</code></pre>

<p>Cast a Numpy array to the default Keras float type.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Numpy array.</li>
</ul>
<p><strong>Returns</strong></p>
<p>The same Numpy array, cast to its new type.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; K.floatx()
'float32'
&gt;&gt;&gt; arr = numpy.array([1.0, 2.0], dtype='float64')
&gt;&gt;&gt; arr.dtype
dtype('float64')
&gt;&gt;&gt; new_arr = K.cast_to_floatx(arr)
&gt;&gt;&gt; new_arr
array([ 1.,  2.], dtype=float32)
&gt;&gt;&gt; new_arr.dtype
dtype('float32')
</code></pre>

<hr />
<h3 id="image_dim_ordering">image_dim_ordering</h3>
<pre><code class="python">image_dim_ordering()
</code></pre>

<p>Returns the default image dimension ordering
convention ('th' or 'tf').</p>
<p><strong>Returns</strong></p>
<p>A string, either <code>'th'</code> or <code>'tf'</code></p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; keras.backend.image_dim_ordering()
'th'
</code></pre>

<hr />
<h3 id="set_image_dim_ordering">set_image_dim_ordering</h3>
<pre><code class="python">set_image_dim_ordering(dim_ordering)
</code></pre>

<p>Sets the value of the image dimension
ordering convention ('th' or 'tf').</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>dim_ordering</strong>: string. <code>'th'</code> or <code>'tf'</code>.</li>
</ul>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; K.image_dim_ordering()
'th'
&gt;&gt;&gt; K.set_image_dim_ordering('tf')
&gt;&gt;&gt; K.image_dim_ordering()
'tf'
</code></pre>

<hr />
<h3 id="get_uid">get_uid</h3>
<pre><code class="python">get_uid(prefix='')
</code></pre>

<p>Provides a unique UID given a string prefix.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>prefix</strong>: string.</li>
</ul>
<p><strong>Returns</strong></p>
<p>An integer.</p>
<p><strong>Example</strong></p>
<pre><code>&gt;&gt;&gt; keras.backend.get_uid('dense')
&gt;&gt;&gt; 1
&gt;&gt;&gt; keras.backend.get_uid('dense')
&gt;&gt;&gt; 2
</code></pre>

<hr />
<h3 id="is_keras_tensor">is_keras_tensor</h3>
<pre><code class="python">is_keras_tensor(x)
</code></pre>

<p>Returns whether <code>x</code> is a Keras tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: a potential tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A boolean: whether the argument is a Keras tensor.</p>
<p><strong>Examples</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; np_var = numpy.array([1, 2])
&gt;&gt;&gt; K.is_keras_tensor(np_var)
False
&gt;&gt;&gt; keras_var = K.variable(np_var)
&gt;&gt;&gt; K.is_keras_tensor(keras_var)  # A variable is not a Tensor.
False
&gt;&gt;&gt; keras_placeholder = K.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; K.is_keras_tensor(keras_placeholder)  # A placeholder is a Tensor.
True
</code></pre>

<hr />
<h3 id="clear_session">clear_session</h3>
<pre><code class="python">clear_session()
</code></pre>

<p>Destroys the current TF graph and creates a new one.</p>
<p>Useful to avoid clutter from old models / layers.</p>
<hr />
<h3 id="manual_variable_initialization">manual_variable_initialization</h3>
<pre><code class="python">manual_variable_initialization(value)
</code></pre>

<p>Sets the manual variable initialization flag.</p>
<p>This boolean flag determines whether
variables should be initialized
as they are instantiated (default), or if
the user should handle the initialization
(e.g. via <code>tf.initialize_all_variables()</code>).</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>value</strong>: Python boolean.</li>
</ul>
<hr />
<h3 id="learning_phase">learning_phase</h3>
<pre><code class="python">learning_phase()
</code></pre>

<p>Returns the learning phase flag.</p>
<p>The learning phase flag is a bool tensor (0 = test, 1 = train)
to be passed as input to any Keras function
that uses a different behavior at train time and test time.</p>
<hr />
<h3 id="set_learning_phase">set_learning_phase</h3>
<pre><code class="python">set_learning_phase(value)
</code></pre>

<p>Sets the learning phase to a fixed value,
either 0 or 1 (integers).</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>value</code> is neither <code>0</code> nor <code>1</code>.</li>
</ul>
<hr />
<h3 id="is_sparse">is_sparse</h3>
<pre><code class="python">is_sparse(tensor)
</code></pre>

<p>Returns whether a tensor is a sparse tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>tensor</strong>: A tensor instance.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A boolean.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; a = K.placeholder((2, 2), sparse=False)
&gt;&gt;&gt; print(K.is_sparse(a))
False
&gt;&gt;&gt; b = K.placeholder((2, 2), sparse=True)
&gt;&gt;&gt; print(K.is_sparse(b))
True
</code></pre>

<hr />
<h3 id="to_dense">to_dense</h3>
<pre><code class="python">to_dense(tensor)
</code></pre>

<p>Converts a sparse tensor into a dense tensor
and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>tensor</strong>: A tensor instance (potentially sparse).</li>
</ul>
<p><strong>Returns</strong></p>
<p>A dense tensor.</p>
<p><strong>Examples</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; b = K.placeholder((2, 2), sparse=True)
&gt;&gt;&gt; print(K.is_sparse(b))
True
&gt;&gt;&gt; c = K.to_dense(b)
&gt;&gt;&gt; print(K.is_sparse(c))
False
</code></pre>

<hr />
<h3 id="variable">variable</h3>
<pre><code class="python">variable(value, dtype=None, name=None)
</code></pre>

<p>Instantiates a variable and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>value</strong>: Numpy array, initial value of the tensor.</li>
<li><strong>dtype</strong>: Tensor type.</li>
<li><strong>name</strong>: Optional name string for the tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A variable instance (with Keras metadata included).</p>
<p><strong>Examples</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; val = np.array([[1, 2], [3, 4]])
&gt;&gt;&gt; kvar = K.variable(value=val, dtype='float64', name='example_var')
&gt;&gt;&gt; K.dtype(kvar)
'float64'
&gt;&gt;&gt; print(kvar)
example_var
&gt;&gt;&gt; kvar.eval()
array([[ 1.,  2.],
   [ 3.,  4.]])
</code></pre>

<hr />
<h3 id="placeholder">placeholder</h3>
<pre><code class="python">placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None)
</code></pre>

<p>Instantiates a placeholder tensor and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>shape</strong>: Shape of the placeholder
(integer tuple, may include <code>None</code> entries).</li>
<li><strong>ndim</strong>: Number of axes of the tensor.
At least one of {<code>shape</code>, <code>ndim</code>} must be specified.
If both are specified, <code>shape</code> is used.</li>
<li><strong>dtype</strong>: Placeholder type.</li>
<li><strong>name</strong>: Optional name string for the placeholder.</li>
</ul>
<p><strong>Returns</strong></p>
<p>Tensor instance (with Keras metadata included).</p>
<p><strong>Examples</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; input_ph = K.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; input_ph._keras_shape
(2, 4, 5)
&gt;&gt;&gt; input_ph
&lt;tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32&gt;
</code></pre>

<hr />
<h3 id="shape">shape</h3>
<pre><code class="python">shape(x)
</code></pre>

<p>Returns the symbolic shape of a tensor or variable.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A symbolic shape (which is itself a tensor).</p>
<p><strong>Examples</strong></p>
<pre><code>__TensorFlow example__

&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; tf_session = K.get_session()
&gt;&gt;&gt; val = np.array([[1, 2], [3, 4]])
&gt;&gt;&gt; kvar = K.variable(value=val)
&gt;&gt;&gt; input = keras.backend.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; K.shape(kvar)
&lt;tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32&gt;
&gt;&gt;&gt; K.shape(input)
&lt;tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32&gt;
__To get integer shape (Instead, you can use K.int_shape(x))__

&gt;&gt;&gt; K.shape(kvar).eval(session=tf_session)
array([2, 2], dtype=int32)
&gt;&gt;&gt; K.shape(input).eval(session=tf_session)
array([2, 4, 5], dtype=int32)
</code></pre>

<hr />
<h3 id="int_shape">int_shape</h3>
<pre><code class="python">int_shape(x)
</code></pre>

<p>Returns the shape of a Keras tensor or a Keras variable as a tuple of
integers or None entries.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tuple of integers (or None entries).</p>
<p><strong>Examples</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; input = K.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; K.int_shape(input)
(2, 4, 5)
&gt;&gt;&gt; val = np.array([[1, 2], [3, 4]])
&gt;&gt;&gt; kvar = K.variable(value=val)
&gt;&gt;&gt; K.int_shape(kvar)
(2, 2)
</code></pre>

<hr />
<h3 id="ndim">ndim</h3>
<pre><code class="python">ndim(x)
</code></pre>

<p>Returns the number of axes in a tensor, as an integer.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>Integer (scalar), number of axes.</p>
<p><strong>Examples</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; input = K.placeholder(shape=(2, 4, 5))
&gt;&gt;&gt; val = np.array([[1, 2], [3, 4]])
&gt;&gt;&gt; kvar = K.variable(value=val)
&gt;&gt;&gt; K.ndim(input)
3
&gt;&gt;&gt; K.ndim(kvar)
2
</code></pre>

<hr />
<h3 id="dtype">dtype</h3>
<pre><code class="python">dtype(x)
</code></pre>

<p>Returns the dtype of a Keras tensor or variable, as a string.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>String, dtype of <code>x</code>.</p>
<p><strong>Examples</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; K.dtype(K.placeholder(shape=(2,4,5)))
'float32'
&gt;&gt;&gt; K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))
'float32'
&gt;&gt;&gt; K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))
'float64'
__Keras variable__

&gt;&gt;&gt; kvar = K.variable(np.array([[1, 2], [3, 4]]))
&gt;&gt;&gt; K.dtype(kvar)
'float32_ref'
&gt;&gt;&gt; kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
&gt;&gt;&gt; K.dtype(kvar)
'float32_ref'
</code></pre>

<hr />
<h3 id="eval">eval</h3>
<pre><code class="python">eval(x)
</code></pre>

<p>Evaluates the value of a variable.
Returns a Numpy array.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A Numpy array.</p>
<p><strong>Examples</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')
&gt;&gt;&gt; K.eval(kvar)
array([[ 1.,  2.],
   [ 3.,  4.]], dtype=float32)
</code></pre>

<hr />
<h3 id="zeros">zeros</h3>
<pre><code class="python">zeros(shape, dtype=None, name=None)
</code></pre>

<p>Instantiates an all-zeros variable and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>shape</strong>: Tuple of integers, shape of returned Keras variable</li>
<li><strong>dtype</strong>: String, data type of returned Keras variable</li>
<li><strong>name</strong>: String, name of returned Keras variable</li>
</ul>
<p><strong>Returns</strong></p>
<p>A variable (including Keras metadata), filled with <code>0.0</code>.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.zeros((3,4))
&gt;&gt;&gt; K.eval(kvar)
array([[ 0.,  0.,  0.,  0.],
   [ 0.,  0.,  0.,  0.],
   [ 0.,  0.,  0.,  0.]], dtype=float32)
</code></pre>

<hr />
<h3 id="ones">ones</h3>
<pre><code class="python">ones(shape, dtype=None, name=None)
</code></pre>

<p>Instantiates an all-ones tensor variable and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>shape</strong>: Tuple of integers, shape of returned Keras variable.</li>
<li><strong>dtype</strong>: String, data type of returned Keras variable.</li>
<li><strong>name</strong>: String, name of returned Keras variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A Keras variable, filled with <code>1.0</code>.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.ones((3,4))
&gt;&gt;&gt; K.eval(kvar)
array([[ 1.,  1.,  1.,  1.],
   [ 1.,  1.,  1.,  1.],
   [ 1.,  1.,  1.,  1.]], dtype=float32)
</code></pre>

<hr />
<h3 id="eye">eye</h3>
<pre><code class="python">eye(size, dtype=None, name=None)
</code></pre>

<p>Instantiate an identity matrix and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>size</strong>: Integer, number of rows/columns.</li>
<li><strong>dtype</strong>: String, data type of returned Keras variable.</li>
<li><strong>name</strong>: String, name of returned Keras variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A Keras variable, an identity matrix.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.eye(3)
&gt;&gt;&gt; K.eval(kvar)
array([[ 1.,  0.,  0.],
   [ 0.,  1.,  0.],
   [ 0.,  0.,  1.]], dtype=float32)
</code></pre>

<hr />
<h3 id="zeros_like">zeros_like</h3>
<pre><code class="python">zeros_like(x, dtype=None, name=None)
</code></pre>

<p>Instantiates an all-zeros Keras variable
of the same shape as another Keras variable or tensor and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Keras variable or Keras tensor.</li>
<li><strong>dtype</strong>: String, dtype of returned Keras variable.
 None uses the dtype of x.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A Keras variable with the shape of x filled with zeros.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.variable(np.random.random((2,3)))
&gt;&gt;&gt; kvar_zeros = K.zeros_like(kvar)
&gt;&gt;&gt; K.eval(kvar_zeros)
array([[ 0.,  0.,  0.],
   [ 0.,  0.,  0.]], dtype=float32)
</code></pre>

<hr />
<h3 id="ones_like">ones_like</h3>
<pre><code class="python">ones_like(x, dtype=None, name=None)
</code></pre>

<p>Instantiates an all-ones Keras variable
of the same shape as another Keras variable or tensor and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Keras variable or tensor.</li>
<li><strong>dtype</strong>: String, dtype of returned Keras variable.
 None uses the dtype of x.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A Keras variable with the shape of x filled with ones.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; kvar = K.variable(np.random.random((2,3)))
&gt;&gt;&gt; kvar_ones = K.ones_like(kvar)
&gt;&gt;&gt; K.eval(kvar_ones)
array([[ 1.,  1.,  1.],
   [ 1.,  1.,  1.]], dtype=float32)
</code></pre>

<hr />
<h3 id="random_uniform_variable">random_uniform_variable</h3>
<pre><code class="python">random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None)
</code></pre>

<p>Instantiates an Keras variable filled with
samples drawn from a uniform distribution and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>shape</strong>: Tuple of integers, shape of returned Keras variable.</li>
<li><strong>low</strong>: Float, lower boundary of the output inteval.</li>
<li><strong>high</strong>: Float, upper boundary of the output interval.</li>
<li><strong>dtype</strong>: String, dtype of returned Keras variable.</li>
<li><strong>name</strong>: String, name of returned Keras variable.</li>
<li><strong>seed</strong>: Integer, random seed.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A Keras variable, filled with drawn samples.</p>
<p><strong>Example</strong></p>
<pre><code class="python">__TensorFlow example__

&gt;&gt;&gt; kvar = K.random_uniform_variable((2,3), 0, 1)
&gt;&gt;&gt; kvar
&lt;tensorflow.python.ops.variables.Variable object at 0x10ab40b10&gt;
&gt;&gt;&gt; K.eval(kvar)
array([[ 0.10940075,  0.10047495,  0.476143  ],
   [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)
</code></pre>

<hr />
<h3 id="random_normal_variable">random_normal_variable</h3>
<pre><code class="python">random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None)
</code></pre>

<p>Instantiates an Keras variable filled with
samples drawn from a normal distribution and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>shape</strong>: Tuple of integers, shape of returned Keras variable.</li>
<li><strong>mean</strong>: Float, mean of the normal distribution.</li>
<li><strong>scale</strong>: Float, standard deviation of the normal distribution.</li>
<li><strong>dtype</strong>: String, dtype of returned Keras variable.</li>
<li><strong>name</strong>: String, name of returned Keras variable.</li>
<li><strong>seed</strong>: Integer, random seed.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A Keras variable, filled with drawn samples.</p>
<p><strong>Example</strong></p>
<pre><code class="python">__TensorFlow example__

&gt;&gt;&gt; kvar = K.random_normal_variable((2,3), 0, 1)
&gt;&gt;&gt; kvar
&lt;tensorflow.python.ops.variables.Variable object at 0x10ab12dd0&gt;
&gt;&gt;&gt; K.eval(kvar)
array([[ 1.19591331,  0.68685907, -0.63814116],
   [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)
</code></pre>

<hr />
<h3 id="count_params">count_params</h3>
<pre><code class="python">count_params(x)
</code></pre>

<p>Returns the number of scalars in a Keras variable.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Keras variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>Integer, the number of scalars in <code>x</code>.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; kvar = K.zeros((2,3))
&gt;&gt;&gt; K.count_params(kvar)
6
&gt;&gt;&gt; K.eval(kvar)
array([[ 0.,  0.,  0.],
   [ 0.,  0.,  0.]], dtype=float32)
</code></pre>

<hr />
<h3 id="cast">cast</h3>
<pre><code class="python">cast(x, dtype)
</code></pre>

<p>Casts a tensor to a different dtype and returns it.</p>
<p>You can cast a Keras variable but it still returns a Keras tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Keras tensor (or variable).</li>
<li><strong>dtype</strong>: String, either (<code>'float16'</code>, <code>'float32'</code>, or <code>'float64'</code>).</li>
</ul>
<p><strong>Returns</strong></p>
<p>Keras tensor with dtype <code>dtype</code>.</p>
<p><strong>Example</strong></p>
<pre><code class="python">&gt;&gt;&gt; from keras import backend as K
&gt;&gt;&gt; input = K.placeholder((2, 3), dtype='float32')
&gt;&gt;&gt; input
&lt;tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32&gt;
__It doesn't work in-place as below.__

&gt;&gt;&gt; K.cast(input, dtype='float16')
&lt;tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16&gt;
&gt;&gt;&gt; input
&lt;tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32&gt;
__you need to assign it.__

&gt;&gt;&gt; input = K.cast(input, dtype='float16')
&gt;&gt;&gt; input
&lt;tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16&gt;
</code></pre>

<hr />
<h3 id="dot">dot</h3>
<pre><code class="python">dot(x, y)
</code></pre>

<p>Multiplies 2 tensors (and/or variables) and returns a <em>tensor</em>.
When attempting to multiply a ND tensor
with a ND tensor, it reproduces the Theano behavior.
(e.g. (2, 3).(4, 3, 5) = (2, 4, 5))</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Tensor or variable.</li>
<li><strong>y</strong>: Tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor, dot product of <code>x</code> and <code>y</code>.</p>
<p><strong>Examples</strong></p>
<pre><code class="python">__dot product between tensors__

&gt;&gt;&gt; x = K.placeholder(shape=(2, 3))
&gt;&gt;&gt; y = K.placeholder(shape=(3, 4))
&gt;&gt;&gt; xy = K.dot(x, y)
&gt;&gt;&gt; xy
&lt;tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32&gt;
</code></pre>

<pre><code class="python">__dot product between tensors__

&gt;&gt;&gt; x = K.placeholder(shape=(32, 28, 3))
&gt;&gt;&gt; y = K.placeholder(shape=(3, 4))
&gt;&gt;&gt; xy = K.dot(x, y)
&gt;&gt;&gt; xy
&lt;tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32&gt;
</code></pre>

<pre><code class="python">__Theano-like behavior example__

&gt;&gt;&gt; x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)
&gt;&gt;&gt; y = K.ones((4, 3, 5))
&gt;&gt;&gt; xy = K.dot(x, y)
&gt;&gt;&gt; K.int_shape(xy)
(2, 4, 5)
</code></pre>

<hr />
<h3 id="batch_dot">batch_dot</h3>
<pre><code class="python">batch_dot(x, y, axes=None)
</code></pre>

<p>Batchwise dot product.</p>
<p><code>batch_dot</code> is used to compute dot product of <code>x</code> and <code>y</code> when
<code>x</code> and <code>y</code> are data in batch, i.e. in a shape of
<code>(batch_size, :)</code>.
<code>batch_dot</code> results in a tensor or variable with less dimensions
than the input. If the number of dimensions is reduced to 1,
we use <code>expand_dims</code> to make sure that ndim is at least 2.</p>
<p><strong>Arguments</strong></p>
<p>x, y: Keras tensors or variables with <code>ndim &gt;= 2</code>
- <strong>axes</strong>: list of (or single) int with target dimensions.
The lengths of <code>axes[0]</code> and <code>axes[1]</code> should be the same.</p>
<p><strong>Returns</strong></p>
<p>A tensor with shape equal to the concatenation of <code>x</code>'s shape
(less the dimension that was summed over) and <code>y</code>'s shape
(less the batch dimension and the dimension that was summed over).
If the final rank is 1, we reshape it to <code>(batch_size, 1)</code>.</p>
<p><strong>Examples</strong></p>
<p>Assume <code>x = [[1, 2], [3, 4]]</code> and <code>y = [[5, 6], [7, 8]]</code>
<code>batch_dot(x, y, axes=1) = [[17, 53]]</code> which is the main diagonal
of <code>x.dot(y.T)</code>, although we never have to calculate the off-diagonal
elements.</p>
<p>Shape inference:
Let <code>x</code>'s shape be <code>(100, 20)</code> and <code>y</code>'s shape be <code>(100, 30, 20)</code>.
If <code>axes</code> is (1, 2), to find the output shape of resultant tensor,
loop through each dimension in <code>x</code>'s shape and <code>y</code>'s shape:</p>
<ul>
<li><code>x.shape[0]</code> : 100 : append to output shape</li>
<li><code>x.shape[1]</code> : 20 : do not append to output shape,
dimension 1 of <code>x</code> has been summed over. (<code>dot_axes[0]</code> = 1)</li>
<li><code>y.shape[0]</code> : 100 : do not append to output shape,
always ignore first dimension of <code>y</code></li>
<li><code>y.shape[1]</code> : 30 : append to output shape</li>
<li><code>y.shape[2]</code> : 20 : do not append to output shape,
dimension 2 of <code>y</code> has been summed over. (<code>dot_axes[1]</code> = 2)
<code>output_shape</code> = <code>(100, 30)</code></li>
</ul>
<pre><code class="python">&gt;&gt;&gt; x_batch = K.ones(shape=(32, 20, 1))
&gt;&gt;&gt; y_batch = K.ones(shape=(32, 30, 20))
&gt;&gt;&gt; xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])
&gt;&gt;&gt; K.int_shape(xy_batch_dot)
(32, 1, 30)
</code></pre>

<hr />
<h3 id="transpose">transpose</h3>
<pre><code class="python">transpose(x)
</code></pre>

<p>Transposes a tensor and returns it.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: Tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<p><strong>Examples</strong></p>
<pre><code class="python">&gt;&gt;&gt; var = K.variable([[1, 2, 3], [4, 5, 6]])
&gt;&gt;&gt; K.eval(var)
array([[ 1.,  2.,  3.],
   [ 4.,  5.,  6.]], dtype=float32)
&gt;&gt;&gt; var_transposed = K.transpose(var)
&gt;&gt;&gt; K.eval(var_transposed)
array([[ 1.,  4.],
   [ 2.,  5.],
   [ 3.,  6.]], dtype=float32)
</code></pre>

<pre><code class="python">&gt;&gt;&gt; input = K.placeholder((2, 3))
&gt;&gt;&gt; input
&lt;tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32&gt;
&gt;&gt;&gt; input_transposed = K.transpose(input)
&gt;&gt;&gt; input_transposed
&lt;tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32&gt;

</code></pre>

<hr />
<h3 id="gather">gather</h3>
<pre><code class="python">gather(reference, indices)
</code></pre>

<p>Retrieves the elements of indices <code>indices</code>
in the tensor <code>reference</code>.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>reference</strong>: A tensor.</li>
<li><strong>indices</strong>: An integer tensor of indices.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor of same type as <code>reference</code>.</p>
<hr />
<h3 id="max">max</h3>
<pre><code class="python">max(x, axis=None, keepdims=False)
</code></pre>

<p>Maximum value in a tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
<li><strong>axis</strong>: An integer, the axis to find maximum values.</li>
<li><strong>keepdims</strong>: A boolean, whether to keep the dimensions or not.
If <code>keepdims</code> is <code>False</code>, the rank of the tensor is reduced
by 1. If <code>keepdims</code> is <code>True</code>,
the reduced dimension is retained with length 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor with maximum values of <code>x</code>.</p>
<hr />
<h3 id="min">min</h3>
<pre><code class="python">min(x, axis=None, keepdims=False)
</code></pre>

<p>Minimum value in a tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
<li><strong>axis</strong>: An integer, the axis to find minimum values.</li>
<li><strong>keepdims</strong>: A boolean, whether to keep the dimensions or not.
If <code>keepdims</code> is <code>False</code>, the rank of the tensor is reduced
by 1. If <code>keepdims</code> is <code>True</code>,
the reduced dimension is retained with length 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor with miminum values of <code>x</code>.</p>
<hr />
<h3 id="sum">sum</h3>
<pre><code class="python">sum(x, axis=None, keepdims=False)
</code></pre>

<p>Sum of the values in a tensor, alongside the specified axis.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
<li><strong>axis</strong>: An integer, the axis to sum over.</li>
<li><strong>keepdims</strong>: A boolean, whether to keep the dimensions or not.
If <code>keepdims</code> is <code>False</code>, the rank of the tensor is reduced
by 1. If <code>keepdims</code> is <code>True</code>,
the reduced dimension is retained with length 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor with sum of <code>x</code>.</p>
<hr />
<h3 id="prod">prod</h3>
<pre><code class="python">prod(x, axis=None, keepdims=False)
</code></pre>

<p>Multiplies the values in a tensor, alongside the specified axis.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
<li><strong>axis</strong>: An integer, the axis to compute the product.</li>
<li><strong>keepdims</strong>: A boolean, whether to keep the dimensions or not.
If <code>keepdims</code> is <code>False</code>, the rank of the tensor is reduced
by 1. If <code>keepdims</code> is <code>True</code>,
the reduced dimension is retained with length 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor with the product of elements of <code>x</code>.</p>
<hr />
<h3 id="var">var</h3>
<pre><code class="python">var(x, axis=None, keepdims=False)
</code></pre>

<p>Variance of a tensor, alongside the specified axis.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
<li><strong>axis</strong>: An integer, the axis to compute the variance.</li>
<li><strong>keepdims</strong>: A boolean, whether to keep the dimensions or not.
If <code>keepdims</code> is <code>False</code>, the rank of the tensor is reduced
by 1. If <code>keepdims</code> is <code>True</code>,
the reduced dimension is retained with length 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor with the variance of elements of <code>x</code>.</p>
<hr />
<h3 id="std">std</h3>
<pre><code class="python">std(x, axis=None, keepdims=False)
</code></pre>

<p>Standard deviation of a tensor, alongside the specified axis.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
<li><strong>axis</strong>: An integer, the axis to compute the standard deviation.</li>
<li><strong>keepdims</strong>: A boolean, whether to keep the dimensions or not.
If <code>keepdims</code> is <code>False</code>, the rank of the tensor is reduced
by 1. If <code>keepdims</code> is <code>True</code>,
the reduced dimension is retained with length 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor with the standard deviation of elements of <code>x</code>.</p>
<hr />
<h3 id="mean">mean</h3>
<pre><code class="python">mean(x, axis=None, keepdims=False)
</code></pre>

<p>Mean of a tensor, alongside the specified axis.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
<li><strong>axis</strong>: A list of integer. Axes to compute the mean.</li>
<li><strong>keepdims</strong>: A boolean, whether to keep the dimensions or not.
If <code>keepdims</code> is <code>False</code>, the rank of the tensor is reduced
by 1 for each entry in <code>axis</code>. If <code>keep_dims</code> is <code>True</code>,
the reduced dimensions are retained with length 1.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor with the mean of elements of <code>x</code>.</p>
<hr />
<h3 id="any">any</h3>
<pre><code class="python">any(x, axis=None, keepdims=False)
</code></pre>

<p>Bitwise reduction (logical OR).</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
<li><strong>axis</strong>: axis along which to perform the reduction.</li>
<li><strong>keepdims</strong>: whether the drop or broadcast the reduction axes.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A uint8 tensor (0s and 1s).</p>
<hr />
<h3 id="all">all</h3>
<pre><code class="python">all(x, axis=None, keepdims=False)
</code></pre>

<p>Bitwise reduction (logical AND).</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
<li><strong>axis</strong>: axis along which to perform the reduction.</li>
<li><strong>keepdims</strong>: whether the drop or broadcast the reduction axes.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A uint8 tensor (0s and 1s).</p>
<hr />
<h3 id="argmax">argmax</h3>
<pre><code class="python">argmax(x, axis=-1)
</code></pre>

<p>Returns the index of the maximum value along an axis.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
<li><strong>axis</strong>: axis along which to perform the reduction.</li>
<li><strong>keepdims</strong>: whether the drop or broadcast the reduction axes.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="argmin">argmin</h3>
<pre><code class="python">argmin(x, axis=-1)
</code></pre>

<p>Returns the index of the minimum value along an axis.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
<li><strong>axis</strong>: axis along which to perform the reduction.</li>
<li><strong>keepdims</strong>: whether the drop or broadcast the reduction axes.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="square">square</h3>
<pre><code class="python">square(x)
</code></pre>

<p>Element-wise square.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="abs">abs</h3>
<pre><code class="python">abs(x)
</code></pre>

<p>Element-wise absolute value.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="sqrt">sqrt</h3>
<pre><code class="python">sqrt(x)
</code></pre>

<p>Element-wise square root.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="exp">exp</h3>
<pre><code class="python">exp(x)
</code></pre>

<p>Element-wise exponential.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="log">log</h3>
<pre><code class="python">log(x)
</code></pre>

<p>Element-wise log.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="round">round</h3>
<pre><code class="python">round(x)
</code></pre>

<p>Element-wise rounding to the closest integer.</p>
<p>In case of tie, the rounding mode used is "half to even".</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="sign">sign</h3>
<pre><code class="python">sign(x)
</code></pre>

<p>Element-wise sign.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="pow">pow</h3>
<pre><code class="python">pow(x, a)
</code></pre>

<p>Element-wise exponentiation.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="clip">clip</h3>
<pre><code class="python">clip(x, min_value, max_value)
</code></pre>

<p>Element-wise value clipping.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="equal">equal</h3>
<pre><code class="python">equal(x, y)
</code></pre>

<p>Element-wise equality between two tensors.</p>
<p><strong>Returns</strong></p>
<p>A bool tensor.</p>
<hr />
<h3 id="not_equal">not_equal</h3>
<pre><code class="python">not_equal(x, y)
</code></pre>

<p>Element-wise inequality between two tensors.</p>
<p><strong>Returns</strong></p>
<p>A bool tensor.</p>
<hr />
<h3 id="greater">greater</h3>
<pre><code class="python">greater(x, y)
</code></pre>

<p>Element-wise truth value of (x &gt; y).</p>
<p><strong>Returns</strong></p>
<p>A bool tensor.</p>
<hr />
<h3 id="greater_equal">greater_equal</h3>
<pre><code class="python">greater_equal(x, y)
</code></pre>

<p>Element-wise truth value of (x &gt;= y).</p>
<p><strong>Returns</strong></p>
<p>A bool tensor.</p>
<hr />
<h3 id="lesser">lesser</h3>
<pre><code class="python">lesser(x, y)
</code></pre>

<p>Element-wise truth value of (x &lt; y).</p>
<p><strong>Returns</strong></p>
<p>A bool tensor.</p>
<hr />
<h3 id="lesser_equal">lesser_equal</h3>
<pre><code class="python">lesser_equal(x, y)
</code></pre>

<p>Element-wise truth value of (x &lt;= y).</p>
<p><strong>Returns</strong></p>
<p>A bool tensor.</p>
<hr />
<h3 id="maximum">maximum</h3>
<pre><code class="python">maximum(x, y)
</code></pre>

<p>Element-wise maximum of two tensors.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="minimum">minimum</h3>
<pre><code class="python">minimum(x, y)
</code></pre>

<p>Element-wise minimum of two tensors.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="sin">sin</h3>
<pre><code class="python">sin(x)
</code></pre>

<p>Computes sin of x element-wise.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="cos">cos</h3>
<pre><code class="python">cos(x)
</code></pre>

<p>Computes cos of x element-wise.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="normalize_batch_in_training">normalize_batch_in_training</h3>
<pre><code class="python">normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)
</code></pre>

<p>Computes mean and std for batch then apply batch_normalization on batch.</p>
<p><strong>Returns</strong></p>
<p>A tuple length of 3, <code>(normalized_tensor, mean, variance)</code>.</p>
<hr />
<h3 id="batch_normalization">batch_normalization</h3>
<pre><code class="python">batch_normalization(x, mean, var, beta, gamma, epsilon=0.001)
</code></pre>

<p>Applies batch normalization on x given mean, var, beta and gamma:</p>
<p>output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="concatenate">concatenate</h3>
<pre><code class="python">concatenate(tensors, axis=-1)
</code></pre>

<p>Concatenates a list of tensors alongside the specified axis.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="reshape">reshape</h3>
<pre><code class="python">reshape(x, shape)
</code></pre>

<p>Reshapes a tensor to the specified shape.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="permute_dimensions">permute_dimensions</h3>
<pre><code class="python">permute_dimensions(x, pattern)
</code></pre>

<p>Permutes axes in a tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>pattern</strong>: should be a tuple of
dimension indices, e.g. (0, 2, 1).</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="resize_images">resize_images</h3>
<pre><code class="python">resize_images(X, height_factor, width_factor, dim_ordering)
</code></pre>

<p>Resizes the images contained in a 4D tensor of shape
- <code>[batch, channels, height, width]</code> (for 'th' dim_ordering)
- <code>[batch, height, width, channels]</code> (for 'tf' dim_ordering)
by a factor of <code>(height_factor, width_factor)</code>. Both factors should be
positive integers.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="resize_volumes">resize_volumes</h3>
<pre><code class="python">resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering)
</code></pre>

<p>Resizes the volume contained in a 5D tensor of shape
- <code>[batch, channels, depth, height, width]</code> (for 'th' dim_ordering)
- <code>[batch, depth, height, width, channels]</code> (for 'tf' dim_ordering)
by a factor of <code>(depth_factor, height_factor, width_factor)</code>.
All three factors should be positive integers.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="repeat_elements">repeat_elements</h3>
<pre><code class="python">repeat_elements(x, rep, axis)
</code></pre>

<p>Repeats the elements of a tensor along an axis, like <code>np.repeat</code>.</p>
<p>If <code>x</code> has shape <code>(s1, s2, s3)</code> and <code>axis</code> is <code>1</code>, the output
will have shape <code>(s1, s2 * rep, s3)</code>.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="repeat">repeat</h3>
<pre><code class="python">repeat(x, n)
</code></pre>

<p>Repeats a 2D tensor.</p>
<p>if <code>x</code> has shape (samples, dim) and <code>n</code> is <code>2</code>,
the output will have shape <code>(samples, 2, dim)</code>.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="arange">arange</h3>
<pre><code class="python">arange(start, stop=None, step=1, dtype='int32')
</code></pre>

<p>Creates a 1-D tensor containing a sequence of integers.</p>
<p>The function arguments use the same convention as
Theano's arange: if only one argument is provided,
it is in fact the "stop" argument.</p>
<p>The default type of the returned tensor is <code>'int32'</code> to
match TensorFlow's default.</p>
<hr />
<h3 id="tile">tile</h3>
<pre><code class="python">tile(x, n)
</code></pre>

<p>Creates a tensor by tiling <code>x</code> by <code>n</code>.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable</li>
<li><strong>n</strong>: A list of integer. The length must be the same as the number of
dimensions in <code>x</code>.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tiled tensor.</p>
<hr />
<h3 id="flatten">flatten</h3>
<pre><code class="python">flatten(x)
</code></pre>

<p>Flatten a tensor.</p>
<p><strong>Returns</strong></p>
<p>A tensor, reshaped into 1-D</p>
<hr />
<h3 id="batch_flatten">batch_flatten</h3>
<pre><code class="python">batch_flatten(x)
</code></pre>

<p>Turn a n-D tensor into a 2D tensor where
the first dimension is conserved.</p>
<p>In other words, it flattens each data samples of a batch.</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="expand_dims">expand_dims</h3>
<pre><code class="python">expand_dims(x, dim=-1)
</code></pre>

<p>Adds a 1-sized dimension at index "dim".</p>
<p><strong>Returns</strong></p>
<p>A tensor with expended dimensions.</p>
<hr />
<h3 id="squeeze">squeeze</h3>
<pre><code class="python">squeeze(x, axis)
</code></pre>

<p>Removes a 1-dimension from the tensor at index "axis".</p>
<p><strong>Returns</strong></p>
<p>A tensor with the same data as <code>x</code> but reduced dimensions.</p>
<hr />
<h3 id="temporal_padding">temporal_padding</h3>
<pre><code class="python">temporal_padding(x, padding=1)
</code></pre>

<p>Pads the middle dimension of a 3D tensor
with "padding" zeros left and right.</p>
<p><strong>Returns</strong></p>
<p>A padded 3D tensor.</p>
<hr />
<h3 id="asymmetric_temporal_padding">asymmetric_temporal_padding</h3>
<pre><code class="python">asymmetric_temporal_padding(x, left_pad=1, right_pad=1)
</code></pre>

<p>Pad the middle dimension of a 3D tensor
with "left_pad" zeros left and "right_pad" right.</p>
<p><strong>Returns</strong></p>
<p>A padded 3D tensor.</p>
<hr />
<h3 id="spatial_2d_padding">spatial_2d_padding</h3>
<pre><code class="python">spatial_2d_padding(x, padding=(1, 1), dim_ordering='default')
</code></pre>

<p>Pads the 2nd and 3rd dimensions of a 4D tensor
with "padding[0]" and "padding[1]" (resp.) zeros left and right.</p>
<p><strong>Returns</strong></p>
<p>A padded 4D tensor.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="asymmetric_spatial_2d_padding">asymmetric_spatial_2d_padding</h3>
<pre><code class="python">asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1, left_pad=1, right_pad=1, dim_ordering='default')
</code></pre>

<p>Pad the rows and columns of a 4D tensor
with "top_pad", "bottom_pad", "left_pad", "right_pad" (resp.) zeros
rows on top, bottom; cols on left, right.</p>
<p><strong>Returns</strong></p>
<p>A padded 4D tensor.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="spatial_3d_padding">spatial_3d_padding</h3>
<pre><code class="python">spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default')
</code></pre>

<p>Pads 5D tensor with zeros for the depth, height, width dimension with
"padding[0]", "padding[1]" and "padding[2]" (resp.) zeros left and right</p>
<p>For 'tf' dim_ordering, the 2nd, 3rd and 4th dimension will be padded.
For 'th' dim_ordering, the 3rd, 4th and 5th dimension will be padded.</p>
<p><strong>Returns</strong></p>
<p>A padded 5D tensor.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="stack">stack</h3>
<pre><code class="python">stack(x)
</code></pre>

<p>Stacks a list of rank <code>R</code> tensors into a rank <code>R+1</code> tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="one_hot">one_hot</h3>
<pre><code class="python">one_hot(indices, nb_classes)
</code></pre>

<p>Input: nD integer tensor of shape <code>(batch_size, dim1, dim2, ... dim(n-1))</code>
- <strong>Output</strong>: (n + 1)D one hot representation of the input
with shape <code>(batch_size, dim1, dim2, ... dim(n-1), nb_classes)</code></p>
<p><strong>Returns</strong></p>
<p>The one-hot tensor.</p>
<hr />
<h3 id="reverse">reverse</h3>
<pre><code class="python">reverse(x, axes)
</code></pre>

<p>Reverse a tensor along the the specified axes</p>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="get_value">get_value</h3>
<pre><code class="python">get_value(x)
</code></pre>

<p>Returns the value of a variable.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A Numpy array.</p>
<hr />
<h3 id="batch_get_value">batch_get_value</h3>
<pre><code class="python">batch_get_value(xs)
</code></pre>

<p>Returns the value of more than one tensor variable.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: list of variables.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A list of Numpy arrays.</p>
<hr />
<h3 id="set_value">set_value</h3>
<pre><code class="python">set_value(x, value)
</code></pre>

<p>Sets the value of a variable,
from a Numpy array. It returns <code>None</code>.</p>
<hr />
<h3 id="batch_set_value">batch_set_value</h3>
<pre><code class="python">batch_set_value(tuples)
</code></pre>

<p>Sets the values of many tensor variables at once.
It returns <code>None</code>.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>tuples</strong>: a list of tuples <code>(tensor, value)</code>.
<code>value</code> should be a Numpy array.</li>
</ul>
<hr />
<h3 id="get_variable_shape">get_variable_shape</h3>
<pre><code class="python">get_variable_shape(x)
</code></pre>

<p>Returns shape of a variable.</p>
<p><strong>Arguments</strong></p>
<p>A variable.</p>
<p><strong>Returns</strong></p>
<p>A tuple of integers.</p>
<hr />
<h3 id="print_tensor">print_tensor</h3>
<pre><code class="python">print_tensor(x, message='')
</code></pre>

<p>Print the message and the tensor when evaluated and return the same
tensor.</p>
<hr />
<h3 id="function">function</h3>
<pre><code class="python">function(inputs, outputs, updates=[])
</code></pre>

<p>Instantiates a Keras function.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>inputs</strong>: list of placeholder/variable tensors.</li>
<li><strong>outputs</strong>: list of output tensors.</li>
<li><strong>updates</strong>: list of update tuples (old_tensor, new_tensor).</li>
</ul>
<hr />
<h3 id="gradients">gradients</h3>
<pre><code class="python">gradients(loss, variables)
</code></pre>

<p>Returns the gradients of <code>variables</code> (list of tensor variables)
with regard to <code>loss</code>.</p>
<hr />
<h3 id="stop_gradient">stop_gradient</h3>
<pre><code class="python">stop_gradient(variables)
</code></pre>

<p>Returns <code>variables</code> but with zero gradient with respect to every other
variables.</p>
<hr />
<h3 id="rnn">rnn</h3>
<pre><code class="python">rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)
</code></pre>

<p>Iterates over the time dimension of a tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>inputs</strong>: tensor of temporal data of shape <code>(samples, time, ...)</code>
(at least 3D).</li>
<li><strong>step_function</strong>:</li>
<li><strong>Parameters</strong>:<ul>
<li><strong>input</strong>: tensor with shape <code>(samples, ...)</code> (no time dimension),
representing input for the batch of samples at a certain
time step.</li>
<li><strong>states</strong>: list of tensors.</li>
</ul>
</li>
<li><strong>Returns</strong>:<ul>
<li><strong>output</strong>: tensor with shape <code>(samples, output_dim)</code>
(no time dimension).</li>
<li><strong>new_states</strong>: list of tensors, same length and shapes
as 'states'. The first state in the list must be the
output tensor at the previous timestep.</li>
</ul>
</li>
<li><strong>initial_states</strong>: tensor with shape (samples, output_dim)
(no time dimension),
containing the initial values for the states used in
the step function.</li>
<li><strong>go_backwards</strong>: boolean. If True, do the iteration over
the time dimension in reverse order.</li>
<li><strong>mask</strong>: binary tensor with shape <code>(samples, time, 1)</code>,
with a zero for every element that is masked.</li>
<li><strong>constants</strong>: a list of constant values passed at each step.</li>
<li><strong>unroll</strong>: whether to unroll the RNN or to use a symbolic loop (<code>while_loop</code> or <code>scan</code> depending on backend).</li>
<li><strong>input_length</strong>: not relevant in the TensorFlow implementation.
Must be specified if using unrolling with Theano.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tuple, <code>(last_output, outputs, new_states)</code>.</p>
<ul>
<li><strong>last_output</strong>: the latest output of the rnn, of shape <code>(samples, ...)</code></li>
<li><strong>outputs</strong>: tensor with shape <code>(samples, time, ...)</code> where each
    entry <code>outputs[s, t]</code> is the output of the step function
    at time <code>t</code> for sample <code>s</code>.</li>
<li><strong>new_states</strong>: list of tensors, latest states returned by
    the step function, of shape <code>(samples, ...)</code>.</li>
</ul>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if input dimension is less than 3.</li>
<li><strong>ValueError</strong>: if <code>unroll</code> is <code>True</code> but input timestep is not a fixed number.</li>
<li><strong>ValueError</strong>: if <code>mask</code> is provided (not <code>None</code>) but states is not provided
(<code>len(states)</code> == 0).</li>
</ul>
<hr />
<h3 id="switch">switch</h3>
<pre><code class="python">switch(condition, then_expression, else_expression)
</code></pre>

<p>Switches between two operations
depending on a scalar value (<code>int</code> or <code>bool</code>).
Note that both <code>then_expression</code> and <code>else_expression</code>
should be symbolic tensors of the <em>same shape</em>.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>condition</strong>: scalar tensor.</li>
<li><strong>then_expression</strong>: either a tensor, or a callable that returns a tensor.</li>
<li><strong>else_expression</strong>: either a tensor, or a callable that returns a tensor.</li>
</ul>
<p><strong>Returns</strong></p>
<p>The selected tensor.</p>
<hr />
<h3 id="in_train_phase">in_train_phase</h3>
<pre><code class="python">in_train_phase(x, alt)
</code></pre>

<p>Selects <code>x</code> in train phase, and <code>alt</code> otherwise.
Note that <code>alt</code> should have the <em>same shape</em> as <code>x</code>.</p>
<p><strong>Returns</strong></p>
<p>Either <code>x</code> or <code>alt</code> based on <code>K.learning_phase</code>.</p>
<hr />
<h3 id="in_test_phase">in_test_phase</h3>
<pre><code class="python">in_test_phase(x, alt)
</code></pre>

<p>Selects <code>x</code> in test phase, and <code>alt</code> otherwise.
Note that <code>alt</code> should have the <em>same shape</em> as <code>x</code>.</p>
<p><strong>Returns</strong></p>
<p>Either <code>x</code> or <code>alt</code> based on <code>K.learning_phase</code>.</p>
<hr />
<h3 id="relu">relu</h3>
<pre><code class="python">relu(x, alpha=0.0, max_value=None)
</code></pre>

<p>Rectified linear unit.
With default values, it returns element-wise <code>max(x, 0)</code>.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
<li><strong>alpha</strong>: A scalar, slope of negative section (default=<code>0.</code>).</li>
<li><strong>max_value</strong>: Saturation threshold.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="elu">elu</h3>
<pre><code class="python">elu(x, alpha=1.0)
</code></pre>

<p>Exponential linear unit.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tenor or variable to compute the activation function for.</li>
<li><strong>alpha</strong>: A scalar, slope of positive section.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="softmax">softmax</h3>
<pre><code class="python">softmax(x)
</code></pre>

<p>Softmax of a tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="softplus">softplus</h3>
<pre><code class="python">softplus(x)
</code></pre>

<p>Softplus of a tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="softsign">softsign</h3>
<pre><code class="python">softsign(x)
</code></pre>

<p>Softsign of a tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="categorical_crossentropy">categorical_crossentropy</h3>
<pre><code class="python">categorical_crossentropy(output, target, from_logits=False)
</code></pre>

<p>Categorical crossentropy between an output tensor
and a target tensor, where the target is a tensor of the same
shape as the output.</p>
<hr />
<h3 id="sparse_categorical_crossentropy">sparse_categorical_crossentropy</h3>
<pre><code class="python">sparse_categorical_crossentropy(output, target, from_logits=False)
</code></pre>

<p>Categorical crossentropy between an output tensor
and a target tensor, where the target is an integer tensor.</p>
<hr />
<h3 id="binary_crossentropy">binary_crossentropy</h3>
<pre><code class="python">binary_crossentropy(output, target, from_logits=False)
</code></pre>

<p>Binary crossentropy between an output tensor and a target tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>output</strong>: A tensor.</li>
<li><strong>target</strong>: A tensor with the same shape as <code>output</code>.</li>
<li><strong>from_logits</strong>: Whether <code>output</code> is expected to be a logits tensor.
By default, we consider that <code>output</code>
encodes a probability distribution.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="sigmoid">sigmoid</h3>
<pre><code class="python">sigmoid(x)
</code></pre>

<p>Element-wise sigmoid.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="hard_sigmoid">hard_sigmoid</h3>
<pre><code class="python">hard_sigmoid(x)
</code></pre>

<p>Segment-wise linear approximation of sigmoid.
Faster than sigmoid.
Returns <code>0.</code> if <code>x &lt; -2.5</code>, <code>1.</code> if <code>x &gt; 2.5</code>.
In <code>-2.5 &lt;= x &lt;= 2.5</code>, returns <code>0.2 * x + 0.5</code>.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="tanh">tanh</h3>
<pre><code class="python">tanh(x)
</code></pre>

<p>Element-wise tanh.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: A tensor or variable.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="dropout">dropout</h3>
<pre><code class="python">dropout(x, level, noise_shape=None, seed=None)
</code></pre>

<p>Sets entries in <code>x</code> to zero at random, while scaling the entire tensor.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: tensor</li>
<li><strong>level</strong>: fraction of the entries in the tensor
that will be set to 0.</li>
<li><strong>noise_shape</strong>: shape for randomly generated keep/drop flags,
must be broadcastable to the shape of <code>x</code></li>
<li><strong>seed</strong>: random seed to ensure determinism.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="l2_normalize">l2_normalize</h3>
<pre><code class="python">l2_normalize(x, axis)
</code></pre>

<p>Normalizes a tensor wrt the L2 norm alongside the specified axis.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
<li><strong>axis</strong>: axis along which to perform normalization.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="in_top_k">in_top_k</h3>
<pre><code class="python">in_top_k(predictions, targets, k)
</code></pre>

<p>Returns whether the <code>targets</code> are in the top <code>k</code> <code>predictions</code></p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>predictions</strong>: A tensor of shape <code>batch_size</code> x classes and type <code>float32</code>.</li>
<li><strong>targets</strong>: A tensor of shape batch_size and type <code>int32</code> or <code>int64</code>.</li>
<li><strong>k</strong>: An <code>int</code>, number of top elements to consider.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor of shape <code>batch_size</code> and type <code>bool</code>. <code>output_i</code> is <code>True</code> if
<code>targets_i</code> is within top-k values of <code>predictions_i</code></p>
<hr />
<h3 id="conv1d">conv1d</h3>
<pre><code class="python">conv1d(x, kernel, stride=1, border_mode='valid', image_shape=None, filter_shape=None)
</code></pre>

<p>1D convolution.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>kernel</strong>: kernel tensor.</li>
<li><strong>strides</strong>: stride integer.</li>
<li><strong>border_mode</strong>: string, <code>"same"</code> or <code>"valid"</code>.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor, result of 1D convolution.</p>
<hr />
<h3 id="conv2d">conv2d</h3>
<pre><code class="python">conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='default', image_shape=None, filter_shape=None, filter_dilation=(1, 1))
</code></pre>

<p>2D convolution.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>kernel</strong>: kernel tensor.</li>
<li><strong>strides</strong>: strides tuple.</li>
<li><strong>border_mode</strong>: string, <code>"same"</code> or <code>"valid"</code>.</li>
<li><strong>dim_ordering</strong>: <code>"tf"</code> or <code>"th"</code>.
Whether to use Theano or TensorFlow dimension ordering
for inputs/kernels/ouputs.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor, result of 2D convolution.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="deconv2d">deconv2d</h3>
<pre><code class="python">deconv2d(x, kernel, output_shape, strides=(1, 1), border_mode='valid', dim_ordering='default', image_shape=None, filter_shape=None)
</code></pre>

<p>2D deconvolution (i.e. transposed convolution).</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
<li><strong>kernel</strong>: kernel tensor.</li>
<li><strong>output_shape</strong>: 1D int tensor for the output shape.</li>
<li><strong>strides</strong>: strides tuple.</li>
<li><strong>border_mode</strong>: string, <code>"same"</code> or <code>"valid"</code>.</li>
<li><strong>dim_ordering</strong>: <code>"tf"</code> or <code>"th"</code>.
Whether to use Theano or TensorFlow dimension ordering
for inputs/kernels/ouputs.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor, result of transposed 2D convolution.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="atrous_conv2d">atrous_conv2d</h3>
<pre><code class="python">atrous_conv2d(x, kernel, rate=1, border_mode='valid', dim_ordering='default', image_shape=None, filter_shape=None)
</code></pre>

<p>Atrous 2D convolution. Also as known as dilated convolution.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>x</strong>: input tensor.</li>
<li><strong>kernel</strong>: kernel tensor.</li>
<li><strong>rate</strong>: integer &gt; 0, the sample stride.</li>
<li><strong>output_shape</strong>: 1D int tensor for the output shape.</li>
<li><strong>strides</strong>: strides tuple.</li>
<li><strong>border_mode</strong>: string, <code>"same"</code> or <code>"valid"</code>.</li>
<li><strong>dim_ordering</strong>: <code>"tf"</code> or <code>"th"</code>.
Whether to use Theano or TensorFlow dimension ordering
for inputs/kernels/ouputs.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor, result of atrous transposed 2D convolution.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="separable_conv2d">separable_conv2d</h3>
<pre><code class="python">separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), border_mode='valid', dim_ordering='default')
</code></pre>

<p>2-D convolution with separable filters.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="conv3d">conv3d</h3>
<pre><code class="python">conv3d(x, kernel, strides=(1, 1, 1), border_mode='valid', dim_ordering='default', volume_shape=None, filter_shape=None)
</code></pre>

<p>3D convolution.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>kernel</strong>: kernel tensor.</li>
<li><strong>strides</strong>: strides tuple.</li>
<li><strong>border_mode</strong>: string, <code>"same"</code> or <code>"valid"</code>.</li>
<li><strong>dim_ordering</strong>: <code>"tf"</code> or <code>"th"</code>.
Whether to use Theano or TensorFlow dimension ordering
for inputs/kernels/ouputs.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor, result of 3D convolution.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
</ul>
<hr />
<h3 id="pool2d">pool2d</h3>
<pre><code class="python">pool2d(x, pool_size, strides=(1, 1), border_mode='valid', dim_ordering='default', pool_mode='max')
</code></pre>

<p>2D Pooling.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>pool_size</strong>: tuple of 2 integers.</li>
<li><strong>strides</strong>: tuple of 2 integers.</li>
<li><strong>border_mode</strong>: one of <code>"valid"</code>, <code>"same"</code>.</li>
<li><strong>dim_ordering</strong>: one of <code>"th"</code>, <code>"tf"</code>.</li>
<li><strong>pool_mode</strong>: one of <code>"max"</code>, <code>"avg"</code>.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor, result of 2D pooling.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
<li><strong>ValueError</strong>: if <code>pool_mode</code> is neither <code>max</code> or <code>avg</code>.</li>
</ul>
<hr />
<h3 id="pool3d">pool3d</h3>
<pre><code class="python">pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid', dim_ordering='default', pool_mode='max')
</code></pre>

<p>3D Pooling.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>pool_size</strong>: tuple of 3 integers.</li>
<li><strong>strides</strong>: tuple of 3 integers.</li>
<li><strong>border_mode</strong>: one of <code>"valid"</code>, <code>"same"</code>.</li>
<li><strong>dim_ordering</strong>: one of <code>"th"</code>, <code>"tf"</code>.</li>
<li><strong>pool_mode</strong>: one of <code>"max"</code>, <code>"avg"</code>.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor, result of 3D pooling.</p>
<p><strong>Raises</strong></p>
<ul>
<li><strong>ValueError</strong>: if <code>dim_ordering</code> is neither <code>tf</code> or <code>th</code>.</li>
<li><strong>ValueError</strong>: if <code>pool_mode</code> is neither <code>max</code> or <code>avg</code>.</li>
</ul>
<hr />
<h3 id="random_normal">random_normal</h3>
<pre><code class="python">random_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None)
</code></pre>

<p>Returns a tensor with normal distribution</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>shape</strong>: A tuple of integers, the shape of tensor to create.</li>
<li><strong>mean</strong>: A float, mean of the normal distribution to draw samples.</li>
<li><strong>std</strong>: A float, standard deviation of the normal distribution
to draw samples.</li>
<li><strong>dtype</strong>: String, dtype of returned tensor.</li>
<li><strong>seed</strong>: Integer, random seed.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="random_uniform">random_uniform</h3>
<pre><code class="python">random_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None)
</code></pre>

<p>Returns a tensor with uniform distribution</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>shape</strong>: A tuple of integers, the shape of tensor to create.</li>
<li><strong>low</strong>: A float, lower boundary of the uniform distribution
to draw samples.</li>
<li><strong>high</strong>: A float, upper boundary of the uniform distribution
to draw samples.</li>
<li><strong>dtype</strong>: String, dtype of returned tensor.</li>
<li><strong>seed</strong>: Integer, random seed.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="random_binomial">random_binomial</h3>
<pre><code class="python">random_binomial(shape, p=0.0, dtype=None, seed=None)
</code></pre>

<p>Returns a tensor with binomlai distribution</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>shape</strong>: A tuple of integers, the shape of tensor to create.</li>
<li><strong>p</strong>: A float, <code>0. &lt;= p &lt;= 1</code>, probability of binomlai distribution.</li>
<li><strong>dtype</strong>: String, dtype of returned tensor.</li>
<li><strong>seed</strong>: Integer, random seed.</li>
</ul>
<p><strong>Returns</strong></p>
<p>A tensor.</p>
<hr />
<h3 id="ctc_batch_cost">ctc_batch_cost</h3>
<pre><code class="python">ctc_batch_cost(y_true, y_pred, input_length, label_length)
</code></pre>

<p>Runs CTC loss algorithm on each batch element.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>y_true</strong>: tensor <code>(samples, max_string_length)</code> containing the truth labels.</li>
<li><strong>y_pred</strong>: tensor <code>(samples, time_steps, num_categories)</code> containing the prediction,
    or output of the softmax.</li>
<li><strong>input_length</strong>: tensor <code>(samples, 1)</code> containing the sequence length for
    each batch item in <code>y_pred</code>.</li>
<li><strong>label_length</strong>: tensor <code>(samples, 1)</code> containing the sequence length for
    each batch item in <code>y_true</code>.</li>
</ul>
<p><strong>Returns</strong></p>
<p>Tensor with shape (samples,1) containing the
CTC loss of each element</p>
<hr />
<h3 id="ctc_decode">ctc_decode</h3>
<pre><code class="python">ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1)
</code></pre>

<p>Decodes the output of a softmax using either
   greedy (also known as best path) or a constrained dictionary
   search.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>y_pred</strong>: tensor <code>(samples, time_steps, num_categories)</code> containing the prediction,
    or output of the softmax.</li>
<li><strong>input_length</strong>: tensor <code>(samples, )</code> containing the sequence length for
    each batch item in <code>y_pred</code>.</li>
<li><strong>greedy</strong>: perform much faster best-path search if <code>true</code>. This does
    not use a dictionary</li>
<li><strong>beam_width</strong>: if <code>greedy</code> is <code>false</code>: a beam search decoder will be used
    with a beam of this width</li>
<li><strong>top_paths</strong>: if <code>greedy</code> is <code>false</code>: how many of the most probable paths will be returned</li>
</ul>
<p><strong>Returns</strong></p>
<ul>
<li><strong>Tuple</strong>:</li>
<li><strong>List</strong>: if <code>greedy</code> is <code>true</code>, returns a list of one element that contains
    the decoded sequence. If <code>false</code>, returns the <code>top_paths</code> most probable
    decoded sequences. Important: blank labels are returned as <code>-1</code>.
Tensor <code>(top_paths, )</code> that contains the log probability of each decoded sequence</li>
</ul>
<hr />
<h3 id="map_fn">map_fn</h3>
<pre><code class="python">map_fn(fn, elems, name=None)
</code></pre>

<p>Map the function fn over the elements elems and return the outputs.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>fn</strong>: Callable that will be called upon each element in elems</li>
<li><strong>elems</strong>: tensor</li>
<li><strong>name</strong>: A string name for the map node in the graph</li>
</ul>
<p><strong>Returns</strong></p>
<p>Tensor with first dimension equal to the elems and second depending on
fn</p>
<hr />
<h3 id="foldl">foldl</h3>
<pre><code class="python">foldl(fn, elems, initializer=None, name=None)
</code></pre>

<p>Reduce elems using fn to combine them from left to right.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>fn</strong>: Callable that will be called upon each element in elems and an
accumulator, for instance <code>lambda acc, x: acc + x</code></li>
<li><strong>elems</strong>: tensor</li>
<li><strong>initializer</strong>: The first value used (<code>elems[0]</code> in case of None)</li>
<li><strong>name</strong>: A string name for the foldl node in the graph</li>
</ul>
<p><strong>Returns</strong></p>
<p>Same type and shape as initializer</p>
<hr />
<h3 id="foldr">foldr</h3>
<pre><code class="python">foldr(fn, elems, initializer=None, name=None)
</code></pre>

<p>Reduce elems using fn to combine them from right to left.</p>
<p><strong>Arguments</strong></p>
<ul>
<li><strong>fn</strong>: Callable that will be called upon each element in elems and an
accumulator, for instance <code>lambda acc, x: acc + x</code></li>
<li><strong>elems</strong>: tensor</li>
<li><strong>initializer</strong>: The first value used (<code>elems[-1]</code> in case of None)</li>
<li><strong>name</strong>: A string name for the foldr node in the graph</li>
</ul>
<p><strong>Returns</strong></p>
<p>Same type and shape as initializer</p>
<hr />
<h3 id="backend">backend</h3>
<pre><code class="python">backend()
</code></pre>

<p>Publicly accessible method
for determining the current backend.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../initializations/" class="btn btn-neutral float-right" title="Initializations">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../applications/" class="btn btn-neutral" title="Applications"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="http://github.com/fchollet/keras" class="fa fa-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../applications/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../initializations/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../js/theme.js"></script>

</body>
</html>
