{
    "docs": [
        {
            "location": "/", 
            "text": "Keras: Deep Learning library for Theano and TensorFlow\n\n\nYou have just found Keras.\n\n\nKeras is a high-level neural networks library, written in Python and capable of running on top of either \nTensorFlow\n or \nTheano\n. It was developed with a focus on enabling fast experimentation. \nBeing able to go from idea to result with the least possible delay is key to doing good research.\n\n\nUse Keras if you need a deep learning library that:\n\n\n\n\nAllows for easy and fast prototyping (through total modularity, minimalism, and extensibility).\n\n\nSupports both convolutional networks and recurrent networks, as well as combinations of the two.\n\n\nSupports arbitrary connectivity schemes (including multi-input and multi-output training).\n\n\nRuns seamlessly on CPU and GPU.\n\n\n\n\nRead the documentation at \nKeras.io\n.\n\n\nKeras is compatible with: \nPython 2.7-3.5\n.\n\n\n\n\nGuiding principles\n\n\n\n\n\n\nModularity.\n A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as little restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.\n\n\n\n\n\n\nMinimalism.\n Each module should be kept short and simple. Every piece of code should be transparent upon first reading. No black magic: it hurts iteration speed and ability to innovate.\n\n\n\n\n\n\nEasy extensibility.\n New modules are dead simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.\n\n\n\n\n\n\nWork with Python\n. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.\n\n\n\n\n\n\n\n\nGetting started: 30 seconds to Keras\n\n\nThe core data structure of Keras is a \nmodel\n, a way to organize layers. The main type of model is the \nSequential\n model, a linear stack of layers. For more complex architectures, you should use the \nKeras functional API\n.\n\n\nHere's the \nSequential\n model:\n\n\nfrom keras.models import Sequential\n\nmodel = Sequential()\n\n\n\n\nStacking layers is as easy as \n.add()\n:\n\n\nfrom keras.layers import Dense, Activation\n\nmodel.add(Dense(output_dim=64, input_dim=100))\nmodel.add(Activation(\nrelu\n))\nmodel.add(Dense(output_dim=10))\nmodel.add(Activation(\nsoftmax\n))\n\n\n\n\nOnce your model looks good, configure its learning process with \n.compile()\n:\n\n\nmodel.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n\n\n\n\nIf you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).\n\n\nfrom keras.optimizers import SGD\nmodel.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))\n\n\n\n\nYou can now iterate on your training data in batches:\n\n\nmodel.fit(X_train, Y_train, nb_epoch=5, batch_size=32)\n\n\n\n\nAlternatively, you can feed batches to your model manually:\n\n\nmodel.train_on_batch(X_batch, Y_batch)\n\n\n\n\nEvaluate your performance in one line:\n\n\nloss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)\n\n\n\n\nOr generate predictions on new data:\n\n\nclasses = model.predict_classes(X_test, batch_size=32)\nproba = model.predict_proba(X_test, batch_size=32)\n\n\n\n\nBuilding a question answering system, an image classification model, a Neural Turing Machine, a word2vec embedder or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?\n\n\nFor a more in-depth tutorial about Keras, you can check out:\n\n\n\n\nGetting started with the Sequential model\n\n\nGetting started with the functional API\n\n\n\n\nIn the \nexamples folder\n of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.\n\n\n\n\nInstallation\n\n\nKeras uses the following dependencies:\n\n\n\n\nnumpy, scipy\n\n\npyyaml\n\n\nHDF5 and h5py (optional, required if you use model saving/loading functions)\n\n\nOptional but recommended if you use CNNs: cuDNN.\n\n\n\n\nWhen using the TensorFlow backend:\n\n\n\n\nTensorFlow\n\n\nSee installation instructions\n.\n\n\n\n\n\n\n\n\nWhen using the Theano backend:\n\n\n\n\nTheano\n\n\nSee installation instructions\n.\n\n\n\n\n\n\n\n\nTo install Keras, \ncd\n to the Keras folder and run the install command:\n\n\nsudo python setup.py install\n\n\n\n\nYou can also install Keras from PyPI:\n\n\nsudo pip install keras\n\n\n\n\n\n\nSwitching from TensorFlow to Theano\n\n\nBy default, Keras will use TensorFlow as its tensor manipulation library. \nFollow these instructions\n to configure the Keras backend.\n\n\n\n\nSupport\n\n\nYou can ask questions and join the development discussion:\n\n\n\n\nOn the \nKeras Google group\n.\n\n\nOn the \nKeras Gitter channel\n.\n\n\n\n\nYou can also post bug reports and feature requests in \nGithub issues\n. Make sure to read \nour guidelines\n first.\n\n\n\n\nWhy this name, Keras?\n\n\nKeras (\u03ba\u03ad\u03c1\u03b1\u03c2) means \nhorn\n in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the \nOdyssey\n, where dream spirits (\nOneiroi\n, singular \nOneiros\n) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words \u03ba\u03ad\u03c1\u03b1\u03c2 (horn) / \u03ba\u03c1\u03b1\u03af\u03bd\u03c9 (fulfill), and \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 (ivory) / \u1f10\u03bb\u03b5\u03c6\u03b1\u03af\u03c1\u03bf\u03bc\u03b1\u03b9 (deceive).\n\n\nKeras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).\n\n\n\n\n\"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.\"\n Homer, Odyssey 19. 562 ff (Shewring translation).", 
            "title": "Home"
        }, 
        {
            "location": "/#keras-deep-learning-library-for-theano-and-tensorflow", 
            "text": "", 
            "title": "Keras: Deep Learning library for Theano and TensorFlow"
        }, 
        {
            "location": "/#you-have-just-found-keras", 
            "text": "Keras is a high-level neural networks library, written in Python and capable of running on top of either  TensorFlow  or  Theano . It was developed with a focus on enabling fast experimentation.  Being able to go from idea to result with the least possible delay is key to doing good research.  Use Keras if you need a deep learning library that:   Allows for easy and fast prototyping (through total modularity, minimalism, and extensibility).  Supports both convolutional networks and recurrent networks, as well as combinations of the two.  Supports arbitrary connectivity schemes (including multi-input and multi-output training).  Runs seamlessly on CPU and GPU.   Read the documentation at  Keras.io .  Keras is compatible with:  Python 2.7-3.5 .", 
            "title": "You have just found Keras."
        }, 
        {
            "location": "/#guiding-principles", 
            "text": "Modularity.  A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as little restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.    Minimalism.  Each module should be kept short and simple. Every piece of code should be transparent upon first reading. No black magic: it hurts iteration speed and ability to innovate.    Easy extensibility.  New modules are dead simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.    Work with Python . No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.", 
            "title": "Guiding principles"
        }, 
        {
            "location": "/#getting-started-30-seconds-to-keras", 
            "text": "The core data structure of Keras is a  model , a way to organize layers. The main type of model is the  Sequential  model, a linear stack of layers. For more complex architectures, you should use the  Keras functional API .  Here's the  Sequential  model:  from keras.models import Sequential\n\nmodel = Sequential()  Stacking layers is as easy as  .add() :  from keras.layers import Dense, Activation\n\nmodel.add(Dense(output_dim=64, input_dim=100))\nmodel.add(Activation( relu ))\nmodel.add(Dense(output_dim=10))\nmodel.add(Activation( softmax ))  Once your model looks good, configure its learning process with  .compile() :  model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])  If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code).  from keras.optimizers import SGD\nmodel.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9, nesterov=True))  You can now iterate on your training data in batches:  model.fit(X_train, Y_train, nb_epoch=5, batch_size=32)  Alternatively, you can feed batches to your model manually:  model.train_on_batch(X_batch, Y_batch)  Evaluate your performance in one line:  loss_and_metrics = model.evaluate(X_test, Y_test, batch_size=32)  Or generate predictions on new data:  classes = model.predict_classes(X_test, batch_size=32)\nproba = model.predict_proba(X_test, batch_size=32)  Building a question answering system, an image classification model, a Neural Turing Machine, a word2vec embedder or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful?  For a more in-depth tutorial about Keras, you can check out:   Getting started with the Sequential model  Getting started with the functional API   In the  examples folder  of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.", 
            "title": "Getting started: 30 seconds to Keras"
        }, 
        {
            "location": "/#installation", 
            "text": "Keras uses the following dependencies:   numpy, scipy  pyyaml  HDF5 and h5py (optional, required if you use model saving/loading functions)  Optional but recommended if you use CNNs: cuDNN.   When using the TensorFlow backend:   TensorFlow  See installation instructions .     When using the Theano backend:   Theano  See installation instructions .     To install Keras,  cd  to the Keras folder and run the install command:  sudo python setup.py install  You can also install Keras from PyPI:  sudo pip install keras", 
            "title": "Installation"
        }, 
        {
            "location": "/#switching-from-tensorflow-to-theano", 
            "text": "By default, Keras will use TensorFlow as its tensor manipulation library.  Follow these instructions  to configure the Keras backend.", 
            "title": "Switching from TensorFlow to Theano"
        }, 
        {
            "location": "/#support", 
            "text": "You can ask questions and join the development discussion:   On the  Keras Google group .  On the  Keras Gitter channel .   You can also post bug reports and feature requests in  Github issues . Make sure to read  our guidelines  first.", 
            "title": "Support"
        }, 
        {
            "location": "/#why-this-name-keras", 
            "text": "Keras (\u03ba\u03ad\u03c1\u03b1\u03c2) means  horn  in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the  Odyssey , where dream spirits ( Oneiroi , singular  Oneiros ) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words \u03ba\u03ad\u03c1\u03b1\u03c2 (horn) / \u03ba\u03c1\u03b1\u03af\u03bd\u03c9 (fulfill), and \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 (ivory) / \u1f10\u03bb\u03b5\u03c6\u03b1\u03af\u03c1\u03bf\u03bc\u03b1\u03b9 (deceive).  Keras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).   \"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.\"  Homer, Odyssey 19. 562 ff (Shewring translation).", 
            "title": "Why this name, Keras?"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/", 
            "text": "Getting started with the Keras Sequential model\n\n\nThe \nSequential\n model is a linear stack of layers.\n\n\nYou can create a \nSequential\n model by passing a list of layer instances to the constructor:\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential([\n    Dense(32, input_dim=784),\n    Activation('relu'),\n    Dense(10),\n    Activation('softmax'),\n])\n\n\n\n\nYou can also simply add layers via the \n.add()\n method:\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))\n\n\n\n\n\n\nSpecifying the input shape\n\n\nThe model needs to know what input shape it should expect. For this reason, the first layer in a \nSequential\n model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:\n\n\n\n\npass an \ninput_shape\n argument to the first layer. This is a shape tuple (a tuple of integers or \nNone\n entries, where \nNone\n indicates that any positive integer may be expected). In \ninput_shape\n, the batch dimension is not included.\n\n\npass instead a \nbatch_input_shape\n argument, where the batch dimension is included. This is useful for specifying a fixed batch size (e.g. with stateful RNNs).\n\n\nsome 2D layers, such as \nDense\n, support the specification of their input shape via the argument \ninput_dim\n, and some 3D temporal layers support the arguments \ninput_dim\n and \ninput_length\n.\n\n\n\n\nAs such, the following three snippets are strictly equivalent:\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(784,)))\n\n\n\n\nmodel = Sequential()\nmodel.add(Dense(32, batch_input_shape=(None, 784)))\n# note that batch dimension is \nNone\n here,\n# so the model will be able to process batches of any size.\n\n\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=784))\n\n\n\n\nAnd so are the following three snippets:\n\n\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(10, 64)))\n\n\n\n\nmodel = Sequential()\nmodel.add(LSTM(32, batch_input_shape=(None, 10, 64)))\n\n\n\n\nmodel = Sequential()\nmodel.add(LSTM(32, input_length=10, input_dim=64))\n\n\n\n\n\n\nThe Merge layer\n\n\nMultiple \nSequential\n instances can be merged into a single output via a \nMerge\n layer. The output is a layer that can be added as first layer in a new \nSequential\n model. For instance, here's a model with two separate input branches getting merged:\n\n\nfrom keras.layers import Merge\n\nleft_branch = Sequential()\nleft_branch.add(Dense(32, input_dim=784))\n\nright_branch = Sequential()\nright_branch.add(Dense(32, input_dim=784))\n\nmerged = Merge([left_branch, right_branch], mode='concat')\n\nfinal_model = Sequential()\nfinal_model.add(merged)\nfinal_model.add(Dense(10, activation='softmax'))\n\n\n\n\n\n\nSuch a two-branch model can then be trained via e.g.:\n\n\nfinal_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\nfinal_model.fit([input_data_1, input_data_2], targets)  # we pass one data array per model input\n\n\n\n\nThe \nMerge\n layer supports a number of pre-defined modes:\n\n\n\n\nsum\n (default): element-wise sum\n\n\nconcat\n: tensor concatenation. You can specify the concatenation axis via the argument \nconcat_axis\n.\n\n\nmul\n: element-wise multiplication\n\n\nave\n: tensor average\n\n\ndot\n: dot product. You can specify which axes to reduce along via the argument \ndot_axes\n.\n\n\ncos\n: cosine proximity between vectors in 2D tensors.\n\n\n\n\nYou can also pass a function as the \nmode\n argument, allowing for arbitrary transformations:\n\n\nmerged = Merge([left_branch, right_branch], mode=lambda x: x[0] - x[1])\n\n\n\n\nNow you know enough to be able to define \nalmost\n any model with Keras. For complex models that cannot be expressed via \nSequential\n and \nMerge\n, you can use \nthe functional API\n.\n\n\n\n\nCompilation\n\n\nBefore training a model, you need to configure the learning process, which is done via the \ncompile\n method. It receives three arguments:\n\n\n\n\nan optimizer. This could be the string identifier of an existing optimizer (such as \nrmsprop\n or \nadagrad\n), or an instance of the \nOptimizer\n class. See: \noptimizers\n.\n\n\na loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as \ncategorical_crossentropy\n or \nmse\n), or it can be an objective function. See: \nobjectives\n.\n\n\na list of metrics. For any classification problem you will want to set this to \nmetrics=['accuracy']\n. A metric could be the string identifier of an existing metric or a custom metric function.  Custom metric function should return either a single tensor value or a dict \nmetric_name -\n metric_value\n. See: \nmetrics\n.\n\n\n\n\n# for a multi-class classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# for a binary classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# for a mean squared error regression problem\nmodel.compile(optimizer='rmsprop',\n              loss='mse')\n\n# for custom metrics\nimport keras.backend as K\n\ndef mean_pred(y_true, y_pred):\n    return K.mean(y_pred)\n\ndef false_rates(y_true, y_pred):\n    false_neg = ...\n    false_pos = ...\n    return {\n        'false_neg': false_neg,\n        'false_pos': false_pos,\n    }\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy', mean_pred, false_rates])\n\n\n\n\n\n\nTraining\n\n\nKeras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the \nfit\n function. \nRead its documentation here\n. \n\n\n# for a single-input model with 2 classes (binary):\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=784, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# generate dummy data\nimport numpy as np\ndata = np.random.random((1000, 784))\nlabels = np.random.randint(2, size=(1000, 1))\n\n# train the model, iterating on the data in batches\n# of 32 samples\nmodel.fit(data, labels, nb_epoch=10, batch_size=32)\n\n\n\n\n# for a multi-input model with 10 classes:\n\nleft_branch = Sequential()\nleft_branch.add(Dense(32, input_dim=784))\n\nright_branch = Sequential()\nright_branch.add(Dense(32, input_dim=784))\n\nmerged = Merge([left_branch, right_branch], mode='concat')\n\nmodel = Sequential()\nmodel.add(merged)\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# generate dummy data\nimport numpy as np\nfrom keras.utils.np_utils import to_categorical\ndata_1 = np.random.random((1000, 784))\ndata_2 = np.random.random((1000, 784))\n\n# these are integers between 0 and 9\nlabels = np.random.randint(10, size=(1000, 1))\n# we convert the labels to a binary matrix of size (1000, 10)\n# for use with categorical_crossentropy\nlabels = to_categorical(labels, 10)\n\n# train the model\n# note that we are passing a list of Numpy arrays as training data\n# since the model has 2 inputs\nmodel.fit([data_1, data_2], labels, nb_epoch=10, batch_size=32)\n\n\n\n\n\n\nExamples\n\n\nHere are a few examples to get you started!\n\n\nIn the examples folder, you will also find example models for real datasets:\n\n\n\n\nCIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation\n\n\nIMDB movie review sentiment classification: LSTM over sequences of words\n\n\nReuters newswires topic classification: Multilayer Perceptron (MLP)\n\n\nMNIST handwritten digits classification: MLP \n CNN\n\n\nCharacter-level text generation with LSTM\n\n\n\n\n...and more.\n\n\nMultilayer Perceptron (MLP) for multi-class softmax classification:\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\n# Dense(64) is a fully-connected layer with 64 hidden units.\n# in the first layer, you must specify the expected input data shape:\n# here, 20-dimensional vectors.\nmodel.add(Dense(64, input_dim=20, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, init='uniform'))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train,\n          nb_epoch=20,\n          batch_size=16)\nscore = model.evaluate(X_test, y_test, batch_size=16)\n\n\n\n\nAlternative implementation of a similar MLP:\n\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=20, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\n\n\n\n\nMLP for binary classification:\n\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=20, init='uniform', activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n\n\n\nVGG-like convnet:\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\n# input: 100x100 images with 3 channels -\n (3, 100, 100) tensors.\n# this applies 32 convolution filters of size 3x3 each.\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(3, 100, 100)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(32, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Convolution2D(64, 3, 3, border_mode='valid'))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n# Note: Keras does automatic shape inference.\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd)\n\nmodel.fit(X_train, Y_train, batch_size=32, nb_epoch=1)\n\n\n\n\nSequence classification with LSTM:\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 256, input_length=maxlen))\nmodel.add(LSTM(output_dim=128, activation='sigmoid', inner_activation='hard_sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, Y_train, batch_size=16, nb_epoch=10)\nscore = model.evaluate(X_test, Y_test, batch_size=16)\n\n\n\n\nArchitecture for learning image captions with a convnet and a Gated Recurrent Unit:\n\n\n(word-level embedding, caption of maximum length 16 words).\n\n\nNote that getting this to work well will require using a bigger convnet, initialized with pre-trained weights.\n\n\nmax_caption_len = 16\nvocab_size = 10000\n\n# first, let's define an image model that\n# will encode pictures into 128-dimensional vectors.\n# it should be initialized with pre-trained weights.\nimage_model = Sequential()\nimage_model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(3, 100, 100)))\nimage_model.add(Activation('relu'))\nimage_model.add(Convolution2D(32, 3, 3))\nimage_model.add(Activation('relu'))\nimage_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nimage_model.add(Convolution2D(64, 3, 3, border_mode='valid'))\nimage_model.add(Activation('relu'))\nimage_model.add(Convolution2D(64, 3, 3))\nimage_model.add(Activation('relu'))\nimage_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nimage_model.add(Flatten())\nimage_model.add(Dense(128))\n\n# let's load the weights from a save file.\nimage_model.load_weights('weight_file.h5')\n\n# next, let's define a RNN model that encodes sequences of words\n# into sequences of 128-dimensional word vectors.\nlanguage_model = Sequential()\nlanguage_model.add(Embedding(vocab_size, 256, input_length=max_caption_len))\nlanguage_model.add(GRU(output_dim=128, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(128)))\n\n# let's repeat the image vector to turn it into a sequence.\nimage_model.add(RepeatVector(max_caption_len))\n\n# the output of both models will be tensors of shape (samples, max_caption_len, 128).\n# let's concatenate these 2 vector sequences.\nmodel = Sequential()\nmodel.add(Merge([image_model, language_model], mode='concat', concat_axis=-1))\n# let's encode this vector sequence into a single vector\nmodel.add(GRU(256, return_sequences=False))\n# which will be used to compute a probability\n# distribution over what the next word in the caption should be!\nmodel.add(Dense(vocab_size))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n# \nimages\n is a numpy float array of shape (nb_samples, nb_channels=3, width, height).\n# \ncaptions\n is a numpy integer array of shape (nb_samples, max_caption_len)\n# containing word index sequences representing partial captions.\n# \nnext_words\n is a numpy float array of shape (nb_samples, vocab_size)\n# containing a categorical encoding (0s and 1s) of the next word in the corresponding\n# partial caption.\nmodel.fit([images, partial_captions], next_words, batch_size=16, nb_epoch=100)\n\n\n\n\nStacked LSTM for sequence classification\n\n\nIn this model, we stack 3 LSTM layers on top of each other,\nmaking the model capable of learning higher-level temporal representations.\n\n\nThe first two LSTMs return their full output sequences, but the last one only returns\nthe last step in its output sequence, thus dropping the temporal dimension\n(i.e. converting the input sequence into a single vector).\n\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnb_classes = 10\n\n# expected input data shape: (batch_size, timesteps, data_dim)\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True,\n               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\nmodel.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\nmodel.add(LSTM(32))  # return a single vector of dimension 32\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# generate dummy training data\nx_train = np.random.random((1000, timesteps, data_dim))\ny_train = np.random.random((1000, nb_classes))\n\n# generate dummy validation data\nx_val = np.random.random((100, timesteps, data_dim))\ny_val = np.random.random((100, nb_classes))\n\nmodel.fit(x_train, y_train,\n          batch_size=64, nb_epoch=5,\n          validation_data=(x_val, y_val))\n\n\n\n\nSame stacked LSTM model, rendered \"stateful\"\n\n\nA stateful recurrent model is one for which the internal states (memories) obtained after processing a batch\nof samples are reused as initial states for the samples of the next batch. This allows to process longer sequences\nwhile keeping computational complexity manageable.\n\n\nYou can read more about stateful RNNs in the FAQ.\n\n\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnb_classes = 10\nbatch_size = 32\n\n# expected input batch shape: (batch_size, timesteps, data_dim)\n# note that we have to provide the full batch_input_shape since the network is stateful.\n# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True, stateful=True,\n               batch_input_shape=(batch_size, timesteps, data_dim)))\nmodel.add(LSTM(32, return_sequences=True, stateful=True))\nmodel.add(LSTM(32, stateful=True))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# generate dummy training data\nx_train = np.random.random((batch_size * 10, timesteps, data_dim))\ny_train = np.random.random((batch_size * 10, nb_classes))\n\n# generate dummy validation data\nx_val = np.random.random((batch_size * 3, timesteps, data_dim))\ny_val = np.random.random((batch_size * 3, nb_classes))\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size, nb_epoch=5,\n          validation_data=(x_val, y_val))\n\n\n\n\nTwo merged LSTM encoders for classification over two parallel sequences\n\n\nIn this model, two input sequences are encoded into vectors by two separate LSTM modules.\n\n\nThese two vectors are then concatenated, and a fully connected network is trained on top of the concatenated representations.\n\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Merge, LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnb_classes = 10\n\nencoder_a = Sequential()\nencoder_a.add(LSTM(32, input_shape=(timesteps, data_dim)))\n\nencoder_b = Sequential()\nencoder_b.add(LSTM(32, input_shape=(timesteps, data_dim)))\n\ndecoder = Sequential()\ndecoder.add(Merge([encoder_a, encoder_b], mode='concat'))\ndecoder.add(Dense(32, activation='relu'))\ndecoder.add(Dense(nb_classes, activation='softmax'))\n\ndecoder.compile(loss='categorical_crossentropy',\n                optimizer='rmsprop',\n                metrics=['accuracy'])\n\n# generate dummy training data\nx_train_a = np.random.random((1000, timesteps, data_dim))\nx_train_b = np.random.random((1000, timesteps, data_dim))\ny_train = np.random.random((1000, nb_classes))\n\n# generate dummy validation data\nx_val_a = np.random.random((100, timesteps, data_dim))\nx_val_b = np.random.random((100, timesteps, data_dim))\ny_val = np.random.random((100, nb_classes))\n\ndecoder.fit([x_train_a, x_train_b], y_train,\n            batch_size=64, nb_epoch=5,\n            validation_data=([x_val_a, x_val_b], y_val))", 
            "title": "Guide to the Sequential model"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#getting-started-with-the-keras-sequential-model", 
            "text": "The  Sequential  model is a linear stack of layers.  You can create a  Sequential  model by passing a list of layer instances to the constructor:  from keras.models import Sequential\nfrom keras.layers import Dense, Activation\n\nmodel = Sequential([\n    Dense(32, input_dim=784),\n    Activation('relu'),\n    Dense(10),\n    Activation('softmax'),\n])  You can also simply add layers via the  .add()  method:  model = Sequential()\nmodel.add(Dense(32, input_dim=784))\nmodel.add(Activation('relu'))", 
            "title": "Getting started with the Keras Sequential model"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#specifying-the-input-shape", 
            "text": "The model needs to know what input shape it should expect. For this reason, the first layer in a  Sequential  model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. There are several possible ways to do this:   pass an  input_shape  argument to the first layer. This is a shape tuple (a tuple of integers or  None  entries, where  None  indicates that any positive integer may be expected). In  input_shape , the batch dimension is not included.  pass instead a  batch_input_shape  argument, where the batch dimension is included. This is useful for specifying a fixed batch size (e.g. with stateful RNNs).  some 2D layers, such as  Dense , support the specification of their input shape via the argument  input_dim , and some 3D temporal layers support the arguments  input_dim  and  input_length .   As such, the following three snippets are strictly equivalent:  model = Sequential()\nmodel.add(Dense(32, input_shape=(784,)))  model = Sequential()\nmodel.add(Dense(32, batch_input_shape=(None, 784)))\n# note that batch dimension is  None  here,\n# so the model will be able to process batches of any size.  model = Sequential()\nmodel.add(Dense(32, input_dim=784))  And so are the following three snippets:  model = Sequential()\nmodel.add(LSTM(32, input_shape=(10, 64)))  model = Sequential()\nmodel.add(LSTM(32, batch_input_shape=(None, 10, 64)))  model = Sequential()\nmodel.add(LSTM(32, input_length=10, input_dim=64))", 
            "title": "Specifying the input shape"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#the-merge-layer", 
            "text": "Multiple  Sequential  instances can be merged into a single output via a  Merge  layer. The output is a layer that can be added as first layer in a new  Sequential  model. For instance, here's a model with two separate input branches getting merged:  from keras.layers import Merge\n\nleft_branch = Sequential()\nleft_branch.add(Dense(32, input_dim=784))\n\nright_branch = Sequential()\nright_branch.add(Dense(32, input_dim=784))\n\nmerged = Merge([left_branch, right_branch], mode='concat')\n\nfinal_model = Sequential()\nfinal_model.add(merged)\nfinal_model.add(Dense(10, activation='softmax'))   Such a two-branch model can then be trained via e.g.:  final_model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\nfinal_model.fit([input_data_1, input_data_2], targets)  # we pass one data array per model input  The  Merge  layer supports a number of pre-defined modes:   sum  (default): element-wise sum  concat : tensor concatenation. You can specify the concatenation axis via the argument  concat_axis .  mul : element-wise multiplication  ave : tensor average  dot : dot product. You can specify which axes to reduce along via the argument  dot_axes .  cos : cosine proximity between vectors in 2D tensors.   You can also pass a function as the  mode  argument, allowing for arbitrary transformations:  merged = Merge([left_branch, right_branch], mode=lambda x: x[0] - x[1])  Now you know enough to be able to define  almost  any model with Keras. For complex models that cannot be expressed via  Sequential  and  Merge , you can use  the functional API .", 
            "title": "The Merge layer"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#compilation", 
            "text": "Before training a model, you need to configure the learning process, which is done via the  compile  method. It receives three arguments:   an optimizer. This could be the string identifier of an existing optimizer (such as  rmsprop  or  adagrad ), or an instance of the  Optimizer  class. See:  optimizers .  a loss function. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as  categorical_crossentropy  or  mse ), or it can be an objective function. See:  objectives .  a list of metrics. For any classification problem you will want to set this to  metrics=['accuracy'] . A metric could be the string identifier of an existing metric or a custom metric function.  Custom metric function should return either a single tensor value or a dict  metric_name -  metric_value . See:  metrics .   # for a multi-class classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# for a binary classification problem\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# for a mean squared error regression problem\nmodel.compile(optimizer='rmsprop',\n              loss='mse')\n\n# for custom metrics\nimport keras.backend as K\n\ndef mean_pred(y_true, y_pred):\n    return K.mean(y_pred)\n\ndef false_rates(y_true, y_pred):\n    false_neg = ...\n    false_pos = ...\n    return {\n        'false_neg': false_neg,\n        'false_pos': false_pos,\n    }\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy', mean_pred, false_rates])", 
            "title": "Compilation"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#training", 
            "text": "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the  fit  function.  Read its documentation here .   # for a single-input model with 2 classes (binary):\n\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=784, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# generate dummy data\nimport numpy as np\ndata = np.random.random((1000, 784))\nlabels = np.random.randint(2, size=(1000, 1))\n\n# train the model, iterating on the data in batches\n# of 32 samples\nmodel.fit(data, labels, nb_epoch=10, batch_size=32)  # for a multi-input model with 10 classes:\n\nleft_branch = Sequential()\nleft_branch.add(Dense(32, input_dim=784))\n\nright_branch = Sequential()\nright_branch.add(Dense(32, input_dim=784))\n\nmerged = Merge([left_branch, right_branch], mode='concat')\n\nmodel = Sequential()\nmodel.add(merged)\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# generate dummy data\nimport numpy as np\nfrom keras.utils.np_utils import to_categorical\ndata_1 = np.random.random((1000, 784))\ndata_2 = np.random.random((1000, 784))\n\n# these are integers between 0 and 9\nlabels = np.random.randint(10, size=(1000, 1))\n# we convert the labels to a binary matrix of size (1000, 10)\n# for use with categorical_crossentropy\nlabels = to_categorical(labels, 10)\n\n# train the model\n# note that we are passing a list of Numpy arrays as training data\n# since the model has 2 inputs\nmodel.fit([data_1, data_2], labels, nb_epoch=10, batch_size=32)", 
            "title": "Training"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#examples", 
            "text": "Here are a few examples to get you started!  In the examples folder, you will also find example models for real datasets:   CIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation  IMDB movie review sentiment classification: LSTM over sequences of words  Reuters newswires topic classification: Multilayer Perceptron (MLP)  MNIST handwritten digits classification: MLP   CNN  Character-level text generation with LSTM   ...and more.", 
            "title": "Examples"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#multilayer-perceptron-mlp-for-multi-class-softmax-classification", 
            "text": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\n# Dense(64) is a fully-connected layer with 64 hidden units.\n# in the first layer, you must specify the expected input data shape:\n# here, 20-dimensional vectors.\nmodel.add(Dense(64, input_dim=20, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, init='uniform'))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, init='uniform'))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train,\n          nb_epoch=20,\n          batch_size=16)\nscore = model.evaluate(X_test, y_test, batch_size=16)", 
            "title": "Multilayer Perceptron (MLP) for multi-class softmax classification:"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#alternative-implementation-of-a-similar-mlp", 
            "text": "model = Sequential()\nmodel.add(Dense(64, input_dim=20, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])", 
            "title": "Alternative implementation of a similar MLP:"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#mlp-for-binary-classification", 
            "text": "model = Sequential()\nmodel.add(Dense(64, input_dim=20, init='uniform', activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])", 
            "title": "MLP for binary classification:"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#vgg-like-convnet", 
            "text": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, MaxPooling2D\nfrom keras.optimizers import SGD\n\nmodel = Sequential()\n# input: 100x100 images with 3 channels -  (3, 100, 100) tensors.\n# this applies 32 convolution filters of size 3x3 each.\nmodel.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(3, 100, 100)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(32, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Convolution2D(64, 3, 3, border_mode='valid'))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(64, 3, 3))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n# Note: Keras does automatic shape inference.\nmodel.add(Dense(256))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd)\n\nmodel.fit(X_train, Y_train, batch_size=32, nb_epoch=1)", 
            "title": "VGG-like convnet:"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#sequence-classification-with-lstm", 
            "text": "from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 256, input_length=maxlen))\nmodel.add(LSTM(output_dim=128, activation='sigmoid', inner_activation='hard_sigmoid'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, Y_train, batch_size=16, nb_epoch=10)\nscore = model.evaluate(X_test, Y_test, batch_size=16)", 
            "title": "Sequence classification with LSTM:"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#architecture-for-learning-image-captions-with-a-convnet-and-a-gated-recurrent-unit", 
            "text": "(word-level embedding, caption of maximum length 16 words).  Note that getting this to work well will require using a bigger convnet, initialized with pre-trained weights.  max_caption_len = 16\nvocab_size = 10000\n\n# first, let's define an image model that\n# will encode pictures into 128-dimensional vectors.\n# it should be initialized with pre-trained weights.\nimage_model = Sequential()\nimage_model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(3, 100, 100)))\nimage_model.add(Activation('relu'))\nimage_model.add(Convolution2D(32, 3, 3))\nimage_model.add(Activation('relu'))\nimage_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nimage_model.add(Convolution2D(64, 3, 3, border_mode='valid'))\nimage_model.add(Activation('relu'))\nimage_model.add(Convolution2D(64, 3, 3))\nimage_model.add(Activation('relu'))\nimage_model.add(MaxPooling2D(pool_size=(2, 2)))\n\nimage_model.add(Flatten())\nimage_model.add(Dense(128))\n\n# let's load the weights from a save file.\nimage_model.load_weights('weight_file.h5')\n\n# next, let's define a RNN model that encodes sequences of words\n# into sequences of 128-dimensional word vectors.\nlanguage_model = Sequential()\nlanguage_model.add(Embedding(vocab_size, 256, input_length=max_caption_len))\nlanguage_model.add(GRU(output_dim=128, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(128)))\n\n# let's repeat the image vector to turn it into a sequence.\nimage_model.add(RepeatVector(max_caption_len))\n\n# the output of both models will be tensors of shape (samples, max_caption_len, 128).\n# let's concatenate these 2 vector sequences.\nmodel = Sequential()\nmodel.add(Merge([image_model, language_model], mode='concat', concat_axis=-1))\n# let's encode this vector sequence into a single vector\nmodel.add(GRU(256, return_sequences=False))\n# which will be used to compute a probability\n# distribution over what the next word in the caption should be!\nmodel.add(Dense(vocab_size))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n#  images  is a numpy float array of shape (nb_samples, nb_channels=3, width, height).\n#  captions  is a numpy integer array of shape (nb_samples, max_caption_len)\n# containing word index sequences representing partial captions.\n#  next_words  is a numpy float array of shape (nb_samples, vocab_size)\n# containing a categorical encoding (0s and 1s) of the next word in the corresponding\n# partial caption.\nmodel.fit([images, partial_captions], next_words, batch_size=16, nb_epoch=100)", 
            "title": "Architecture for learning image captions with a convnet and a Gated Recurrent Unit:"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#stacked-lstm-for-sequence-classification", 
            "text": "In this model, we stack 3 LSTM layers on top of each other,\nmaking the model capable of learning higher-level temporal representations.  The first two LSTMs return their full output sequences, but the last one only returns\nthe last step in its output sequence, thus dropping the temporal dimension\n(i.e. converting the input sequence into a single vector).   from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnb_classes = 10\n\n# expected input data shape: (batch_size, timesteps, data_dim)\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True,\n               input_shape=(timesteps, data_dim)))  # returns a sequence of vectors of dimension 32\nmodel.add(LSTM(32, return_sequences=True))  # returns a sequence of vectors of dimension 32\nmodel.add(LSTM(32))  # return a single vector of dimension 32\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# generate dummy training data\nx_train = np.random.random((1000, timesteps, data_dim))\ny_train = np.random.random((1000, nb_classes))\n\n# generate dummy validation data\nx_val = np.random.random((100, timesteps, data_dim))\ny_val = np.random.random((100, nb_classes))\n\nmodel.fit(x_train, y_train,\n          batch_size=64, nb_epoch=5,\n          validation_data=(x_val, y_val))", 
            "title": "Stacked LSTM for sequence classification"
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#same-stacked-lstm-model-rendered-stateful", 
            "text": "A stateful recurrent model is one for which the internal states (memories) obtained after processing a batch\nof samples are reused as initial states for the samples of the next batch. This allows to process longer sequences\nwhile keeping computational complexity manageable.  You can read more about stateful RNNs in the FAQ.  from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnb_classes = 10\nbatch_size = 32\n\n# expected input batch shape: (batch_size, timesteps, data_dim)\n# note that we have to provide the full batch_input_shape since the network is stateful.\n# the sample of index i in batch k is the follow-up for the sample i in batch k-1.\nmodel = Sequential()\nmodel.add(LSTM(32, return_sequences=True, stateful=True,\n               batch_input_shape=(batch_size, timesteps, data_dim)))\nmodel.add(LSTM(32, return_sequences=True, stateful=True))\nmodel.add(LSTM(32, stateful=True))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='rmsprop',\n              metrics=['accuracy'])\n\n# generate dummy training data\nx_train = np.random.random((batch_size * 10, timesteps, data_dim))\ny_train = np.random.random((batch_size * 10, nb_classes))\n\n# generate dummy validation data\nx_val = np.random.random((batch_size * 3, timesteps, data_dim))\ny_val = np.random.random((batch_size * 3, nb_classes))\n\nmodel.fit(x_train, y_train,\n          batch_size=batch_size, nb_epoch=5,\n          validation_data=(x_val, y_val))", 
            "title": "Same stacked LSTM model, rendered \"stateful\""
        }, 
        {
            "location": "/getting-started/sequential-model-guide/#two-merged-lstm-encoders-for-classification-over-two-parallel-sequences", 
            "text": "In this model, two input sequences are encoded into vectors by two separate LSTM modules.  These two vectors are then concatenated, and a fully connected network is trained on top of the concatenated representations.   from keras.models import Sequential\nfrom keras.layers import Merge, LSTM, Dense\nimport numpy as np\n\ndata_dim = 16\ntimesteps = 8\nnb_classes = 10\n\nencoder_a = Sequential()\nencoder_a.add(LSTM(32, input_shape=(timesteps, data_dim)))\n\nencoder_b = Sequential()\nencoder_b.add(LSTM(32, input_shape=(timesteps, data_dim)))\n\ndecoder = Sequential()\ndecoder.add(Merge([encoder_a, encoder_b], mode='concat'))\ndecoder.add(Dense(32, activation='relu'))\ndecoder.add(Dense(nb_classes, activation='softmax'))\n\ndecoder.compile(loss='categorical_crossentropy',\n                optimizer='rmsprop',\n                metrics=['accuracy'])\n\n# generate dummy training data\nx_train_a = np.random.random((1000, timesteps, data_dim))\nx_train_b = np.random.random((1000, timesteps, data_dim))\ny_train = np.random.random((1000, nb_classes))\n\n# generate dummy validation data\nx_val_a = np.random.random((100, timesteps, data_dim))\nx_val_b = np.random.random((100, timesteps, data_dim))\ny_val = np.random.random((100, nb_classes))\n\ndecoder.fit([x_train_a, x_train_b], y_train,\n            batch_size=64, nb_epoch=5,\n            validation_data=([x_val_a, x_val_b], y_val))", 
            "title": "Two merged LSTM encoders for classification over two parallel sequences"
        }, 
        {
            "location": "/getting-started/functional-api-guide/", 
            "text": "Getting started with the Keras functional API\n\n\nThe Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\n\n\nThis guide assumes that you are already familiar with the \nSequential\n model.\n\n\nLet's start with something simple.\n\n\n\n\nFirst example: fully connected network\n\n\nThe \nSequential\n model is probably a better choice to implement such a network, but it helps to start with something really simple.\n\n\n\n\nA layer instance is callable (on a tensor), and it returns a tensor\n\n\nInput tensor(s) and output tensor(s) can then be used to define a \nModel\n\n\nSuch a model can be trained just like Keras \nSequential\n models.\n\n\n\n\nfrom keras.layers import Input, Dense\nfrom keras.models import Model\n\n# this returns a tensor\ninputs = Input(shape=(784,))\n\n# a layer instance is callable on a tensor, and returns a tensor\nx = Dense(64, activation='relu')(inputs)\nx = Dense(64, activation='relu')(x)\npredictions = Dense(10, activation='softmax')(x)\n\n# this creates a model that includes\n# the Input layer and three Dense layers\nmodel = Model(input=inputs, output=predictions)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(data, labels)  # starts training\n\n\n\n\n\n\nAll models are callable, just like layers\n\n\nWith the functional API, it is easy to re-use trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just re-using the \narchitecture\n of the model, you are also re-using its weights.\n\n\nx = Input(shape=(784,))\n# this works, and returns the 10-way softmax we defined above.\ny = model(x)\n\n\n\n\nThis can allow, for instance, to quickly create models that can process \nsequences\n of inputs. You could turn an image classification model into a video classification model, in just one line.\n\n\nfrom keras.layers import TimeDistributed\n\n# input tensor for sequences of 20 timesteps,\n# each containing a 784-dimensional vector\ninput_sequences = Input(shape=(20, 784))\n\n# this applies our previous model to every timestep in the input sequences.\n# the output of the previous model was a 10-way softmax,\n# so the output of the layer below will be a sequence of 20 vectors of size 10.\nprocessed_sequences = TimeDistributed(model)(input_sequences)\n\n\n\n\n\n\nMulti-input and multi-output models\n\n\nHere's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.\n\n\nLet's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc.\nThe model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.\n\n\nHere's what our model looks like:\n\n\n\n\nLet's implement it with the functional API.\n\n\nThe main input will receive the headline, as a sequence of integers (each integer encodes a word).\nThe integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.\n\n\nfrom keras.layers import Input, Embedding, LSTM, Dense, merge\nfrom keras.models import Model\n\n# headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n# note that we can name any layer by passing it a \nname\n argument.\nmain_input = Input(shape=(100,), dtype='int32', name='main_input')\n\n# this embedding layer will encode the input sequence\n# into a sequence of dense 512-dimensional vectors.\nx = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n\n# a LSTM will transform the vector sequence into a single vector,\n# containing information about the entire sequence\nlstm_out = LSTM(32)(x)\n\n\n\n\nHere we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model.\n\n\nauxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)\n\n\n\n\nAt this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:\n\n\nauxiliary_input = Input(shape=(5,), name='aux_input')\nx = merge([lstm_out, auxiliary_input], mode='concat')\n\n# we stack a deep fully-connected network on top\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\n\n# and finally we add the main logistic regression layer\nmain_output = Dense(1, activation='sigmoid', name='main_output')(x)\n\n\n\n\nThis defines a model with two inputs and two outputs:\n\n\nmodel = Model(input=[main_input, auxiliary_input], output=[main_output, auxiliary_output])\n\n\n\n\nWe compile the model and assign a weight of 0.2 to the auxiliary loss.\nTo specify different \nloss_weights\n or \nloss\n for each different output, you can use a list or a dictionary.\nHere we pass a single loss as the \nloss\n argument, so the same loss will be used on all outputs.\n\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy',\n              loss_weights=[1., 0.2])\n\n\n\n\nWe can train the model by passing it lists of input arrays and target arrays:\n\n\nmodel.fit([headline_data, additional_data], [labels, labels],\n          nb_epoch=50, batch_size=32)\n\n\n\n\nSince our inputs and outputs are named (we passed them a \"name\" argument),\nWe could also have compiled the model via:\n\n\nmodel.compile(optimizer='rmsprop',\n              loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n              loss_weights={'main_output': 1., 'aux_output': 0.2})\n\n# and trained it via:\nmodel.fit({'main_input': headline_data, 'aux_input': additional_data},\n          {'main_output': labels, 'aux_output': labels},\n          nb_epoch=50, batch_size=32)\n\n\n\n\n\n\nShared layers\n\n\nAnother good use for the functional API are models that use shared layers. Let's take a look at shared layers.\n\n\nLet's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance).\n\n\nOne way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and adds a logistic regression of top, outputting a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs.\n\n\nBecause the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets.\n\n\nLet's build this with the functional API. We will take as input for a tweet a binary matrix of shape \n(140, 256)\n, i.e. a sequence of 140 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters).\n\n\nfrom keras.layers import Input, LSTM, Dense, merge\nfrom keras.models import Model\n\ntweet_a = Input(shape=(140, 256))\ntweet_b = Input(shape=(140, 256))\n\n\n\n\nTo share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want:\n\n\n# this layer can take as input a matrix\n# and will return a vector of size 64\nshared_lstm = LSTM(64)\n\n# when we reuse the same layer instance\n# multiple times, the weights of the layer\n# are also being reused\n# (it is effectively *the same* layer)\nencoded_a = shared_lstm(tweet_a)\nencoded_b = shared_lstm(tweet_b)\n\n# we can then concatenate the two vectors:\nmerged_vector = merge([encoded_a, encoded_b], mode='concat', concat_axis=-1)\n\n# and add a logistic regression on top\npredictions = Dense(1, activation='sigmoid')(merged_vector)\n\n# we define a trainable model linking the\n# tweet inputs to the predictions\nmodel = Model(input=[tweet_a, tweet_b], output=predictions)\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.fit([data_a, data_b], labels, nb_epoch=10)\n\n\n\n\nLet's pause to take a look at how to read the shared layer's output or output shape.\n\n\n\n\nThe concept of layer \"node\"\n\n\nWhenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a \"node\" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2...\n\n\nIn previous versions of Keras, you could obtain the output tensor of a layer instance via \nlayer.get_output()\n, or its output shape via \nlayer.output_shape\n. You still can (except \nget_output()\n has been replaced by the property \noutput\n). But what if a layer is connected to multiple inputs?\n\n\nAs long as a layer is only connected to one input, there is no confusion, and \n.output\n will return the one output of the layer:\n\n\na = Input(shape=(140, 256))\n\nlstm = LSTM(32)\nencoded_a = lstm(a)\n\nassert lstm.output == encoded_a\n\n\n\n\nNot so if the layer has multiple inputs:\n\n\na = Input(shape=(140, 256))\nb = Input(shape=(140, 256))\n\nlstm = LSTM(32)\nencoded_a = lstm(a)\nencoded_b = lstm(b)\n\nlstm.output\n\n\n\n\n AssertionError: Layer lstm_1 has multiple inbound nodes,\nhence the notion of \nlayer output\n is ill-defined.\nUse `get_output_at(node_index)` instead.\n\n\n\n\nOkay then. The following works:\n\n\nassert lstm.get_output_at(0) == encoded_a\nassert lstm.get_output_at(1) == encoded_b\n\n\n\n\nSimple enough, right?\n\n\nThe same is true for the properties \ninput_shape\n and \noutput_shape\n: as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of \"layer output/input shape\" is well defined, and that one shape will be returned by \nlayer.output_shape\n/\nlayer.input_shape\n. But if, for instance, you apply a same \nConvolution2D\n layer to an input of shape \n(3, 32, 32)\n, and then to an input of shape \n(3, 64, 64)\n, the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to:\n\n\na = Input(shape=(3, 32, 32))\nb = Input(shape=(3, 64, 64))\n\nconv = Convolution2D(16, 3, 3, border_mode='same')\nconved_a = conv(a)\n\n# only one input so far, the following will work:\nassert conv.input_shape == (None, 3, 32, 32)\n\nconved_b = conv(b)\n# now the `.input_shape` property wouldn't work, but this does:\nassert conv.get_input_shape_at(0) == (None, 3, 32, 32)\nassert conv.get_input_shape_at(1) == (None, 3, 64, 64)\n\n\n\n\n\n\nMore examples\n\n\nCode examples are still the best way to get started, so here are a few more.\n\n\nInception module\n\n\nFor more information about the Inception architecture, see \nGoing Deeper with Convolutions\n.\n\n\nfrom keras.layers import merge, Convolution2D, MaxPooling2D, Input\n\ninput_img = Input(shape=(3, 256, 256))\n\ntower_1 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(input_img)\ntower_1 = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(tower_1)\n\ntower_2 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(input_img)\ntower_2 = Convolution2D(64, 5, 5, border_mode='same', activation='relu')(tower_2)\n\ntower_3 = MaxPooling2D((3, 3), strides=(1, 1), border_mode='same')(input_img)\ntower_3 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(tower_3)\n\noutput = merge([tower_1, tower_2, tower_3], mode='concat', concat_axis=1)\n\n\n\n\nResidual connection on a convolution layer\n\n\nFor more information about residual networks, see \nDeep Residual Learning for Image Recognition\n.\n\n\nfrom keras.layers import merge, Convolution2D, Input\n\n# input tensor for a 3-channel 256x256 image\nx = Input(shape=(3, 256, 256))\n# 3x3 conv with 3 output channels (same as input channels)\ny = Convolution2D(3, 3, 3, border_mode='same')(x)\n# this returns x + y.\nz = merge([x, y], mode='sum')\n\n\n\n\nShared vision model\n\n\nThis model re-uses the same image-processing module on two inputs, to classify whether two MNIST digits are the same digit or different digits.\n\n\nfrom keras.layers import merge, Convolution2D, MaxPooling2D, Input, Dense, Flatten\nfrom keras.models import Model\n\n# first, define the vision modules\ndigit_input = Input(shape=(1, 27, 27))\nx = Convolution2D(64, 3, 3)(digit_input)\nx = Convolution2D(64, 3, 3)(x)\nx = MaxPooling2D((2, 2))(x)\nout = Flatten()(x)\n\nvision_model = Model(digit_input, out)\n\n# then define the tell-digits-apart model\ndigit_a = Input(shape=(1, 27, 27))\ndigit_b = Input(shape=(1, 27, 27))\n\n# the vision model will be shared, weights and all\nout_a = vision_model(digit_a)\nout_b = vision_model(digit_b)\n\nconcatenated = merge([out_a, out_b], mode='concat')\nout = Dense(1, activation='sigmoid')(concatenated)\n\nclassification_model = Model([digit_a, digit_b], out)\n\n\n\n\nVisual question answering model\n\n\nThis model can select the correct one-word answer when asked a natural-language question about a picture.\n\n\nIt works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers.\n\n\nfrom keras.layers import Convolution2D, MaxPooling2D, Flatten\nfrom keras.layers import Input, LSTM, Embedding, Dense, merge\nfrom keras.models import Model, Sequential\n\n# first, let's define a vision model using a Sequential model.\n# this model will encode an image into a vector.\nvision_model = Sequential()\nvision_model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same', input_shape=(3, 224, 224)))\nvision_model.add(Convolution2D(64, 3, 3, activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same'))\nvision_model.add(Convolution2D(128, 3, 3, activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Convolution2D(256, 3, 3, activation='relu', border_mode='same'))\nvision_model.add(Convolution2D(256, 3, 3, activation='relu'))\nvision_model.add(Convolution2D(256, 3, 3, activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Flatten())\n\n# now let's get a tensor with the output of our vision model:\nimage_input = Input(shape=(3, 224, 224))\nencoded_image = vision_model(image_input)\n\n# next, let's define a language model to encode the question into a vector.\n# each question will be at most 100 word long,\n# and we will index words as integers from 1 to 9999.\nquestion_input = Input(shape=(100,), dtype='int32')\nembedded_question = Embedding(input_dim=10000, output_dim=256, input_length=100)(question_input)\nencoded_question = LSTM(256)(embedded_question)\n\n# let's concatenate the question vector and the image vector:\nmerged = merge([encoded_question, encoded_image], mode='concat')\n\n# and let's train a logistic regression over 1000 words on top:\noutput = Dense(1000, activation='softmax')(merged)\n\n# this is our final model:\nvqa_model = Model(input=[image_input, question_input], output=output)\n\n# the next stage would be training this model on actual data.\n\n\n\n\nVideo question answering model\n\n\nNow that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. \"what sport is the boy playing?\" -\n \"football\").\n\n\nfrom keras.layers import TimeDistributed\n\nvideo_input = Input(shape=(100, 3, 224, 224))\n# this is our video encoded via the previously trained vision_model (weights are reused)\nencoded_frame_sequence = TimeDistributed(vision_model)(video_input)  # the output will be a sequence of vectors\nencoded_video = LSTM(256)(encoded_frame_sequence)  # the output will be a vector\n\n# this is a model-level representation of the question encoder, reusing the same weights as before:\nquestion_encoder = Model(input=question_input, output=encoded_question)\n\n# let's use it to encode the question:\nvideo_question_input = Input(shape=(100,), dtype='int32')\nencoded_video_question = question_encoder(video_question_input)\n\n# and this is our video question answering model:\nmerged = merge([encoded_video, encoded_video_question], mode='concat')\noutput = Dense(1000, activation='softmax')(merged)\nvideo_qa_model = Model(input=[video_input, video_question_input], output=output)", 
            "title": "Guide to the Functional API"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#getting-started-with-the-keras-functional-api", 
            "text": "The Keras functional API is the way to go for defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.  This guide assumes that you are already familiar with the  Sequential  model.  Let's start with something simple.", 
            "title": "Getting started with the Keras functional API"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#first-example-fully-connected-network", 
            "text": "The  Sequential  model is probably a better choice to implement such a network, but it helps to start with something really simple.   A layer instance is callable (on a tensor), and it returns a tensor  Input tensor(s) and output tensor(s) can then be used to define a  Model  Such a model can be trained just like Keras  Sequential  models.   from keras.layers import Input, Dense\nfrom keras.models import Model\n\n# this returns a tensor\ninputs = Input(shape=(784,))\n\n# a layer instance is callable on a tensor, and returns a tensor\nx = Dense(64, activation='relu')(inputs)\nx = Dense(64, activation='relu')(x)\npredictions = Dense(10, activation='softmax')(x)\n\n# this creates a model that includes\n# the Input layer and three Dense layers\nmodel = Model(input=inputs, output=predictions)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit(data, labels)  # starts training", 
            "title": "First example: fully connected network"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#all-models-are-callable-just-like-layers", 
            "text": "With the functional API, it is easy to re-use trained models: you can treat any model as if it were a layer, by calling it on a tensor. Note that by calling a model you aren't just re-using the  architecture  of the model, you are also re-using its weights.  x = Input(shape=(784,))\n# this works, and returns the 10-way softmax we defined above.\ny = model(x)  This can allow, for instance, to quickly create models that can process  sequences  of inputs. You could turn an image classification model into a video classification model, in just one line.  from keras.layers import TimeDistributed\n\n# input tensor for sequences of 20 timesteps,\n# each containing a 784-dimensional vector\ninput_sequences = Input(shape=(20, 784))\n\n# this applies our previous model to every timestep in the input sequences.\n# the output of the previous model was a 10-way softmax,\n# so the output of the layer below will be a sequence of 20 vectors of size 10.\nprocessed_sequences = TimeDistributed(model)(input_sequences)", 
            "title": "All models are callable, just like layers"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#multi-input-and-multi-output-models", 
            "text": "Here's a good use case for the functional API: models with multiple inputs and outputs. The functional API makes it easy to manipulate a large number of intertwined datastreams.  Let's consider the following model. We seek to predict how many retweets and likes a news headline will receive on Twitter. The main input to the model will be the headline itself, as a sequence of words, but to spice things up, our model will also have an auxiliary input, receiving extra data such as the time of day when the headline was posted, etc.\nThe model will also be supervised via two loss functions. Using the main loss function earlier in a model is a good regularization mechanism for deep models.  Here's what our model looks like:   Let's implement it with the functional API.  The main input will receive the headline, as a sequence of integers (each integer encodes a word).\nThe integers will be between 1 and 10,000 (a vocabulary of 10,000 words) and the sequences will be 100 words long.  from keras.layers import Input, Embedding, LSTM, Dense, merge\nfrom keras.models import Model\n\n# headline input: meant to receive sequences of 100 integers, between 1 and 10000.\n# note that we can name any layer by passing it a  name  argument.\nmain_input = Input(shape=(100,), dtype='int32', name='main_input')\n\n# this embedding layer will encode the input sequence\n# into a sequence of dense 512-dimensional vectors.\nx = Embedding(output_dim=512, input_dim=10000, input_length=100)(main_input)\n\n# a LSTM will transform the vector sequence into a single vector,\n# containing information about the entire sequence\nlstm_out = LSTM(32)(x)  Here we insert the auxiliary loss, allowing the LSTM and Embedding layer to be trained smoothly even though the main loss will be much higher in the model.  auxiliary_output = Dense(1, activation='sigmoid', name='aux_output')(lstm_out)  At this point, we feed into the model our auxiliary input data by concatenating it with the LSTM output:  auxiliary_input = Input(shape=(5,), name='aux_input')\nx = merge([lstm_out, auxiliary_input], mode='concat')\n\n# we stack a deep fully-connected network on top\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\nx = Dense(64, activation='relu')(x)\n\n# and finally we add the main logistic regression layer\nmain_output = Dense(1, activation='sigmoid', name='main_output')(x)  This defines a model with two inputs and two outputs:  model = Model(input=[main_input, auxiliary_input], output=[main_output, auxiliary_output])  We compile the model and assign a weight of 0.2 to the auxiliary loss.\nTo specify different  loss_weights  or  loss  for each different output, you can use a list or a dictionary.\nHere we pass a single loss as the  loss  argument, so the same loss will be used on all outputs.  model.compile(optimizer='rmsprop', loss='binary_crossentropy',\n              loss_weights=[1., 0.2])  We can train the model by passing it lists of input arrays and target arrays:  model.fit([headline_data, additional_data], [labels, labels],\n          nb_epoch=50, batch_size=32)  Since our inputs and outputs are named (we passed them a \"name\" argument),\nWe could also have compiled the model via:  model.compile(optimizer='rmsprop',\n              loss={'main_output': 'binary_crossentropy', 'aux_output': 'binary_crossentropy'},\n              loss_weights={'main_output': 1., 'aux_output': 0.2})\n\n# and trained it via:\nmodel.fit({'main_input': headline_data, 'aux_input': additional_data},\n          {'main_output': labels, 'aux_output': labels},\n          nb_epoch=50, batch_size=32)", 
            "title": "Multi-input and multi-output models"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#shared-layers", 
            "text": "Another good use for the functional API are models that use shared layers. Let's take a look at shared layers.  Let's consider a dataset of tweets. We want to build a model that can tell whether two tweets are from the same person or not (this can allow us to compare users by the similarity of their tweets, for instance).  One way to achieve this is to build a model that encodes two tweets into two vectors, concatenates the vectors and adds a logistic regression of top, outputting a probability that the two tweets share the same author. The model would then be trained on positive tweet pairs and negative tweet pairs.  Because the problem is symmetric, the mechanism that encodes the first tweet should be reused (weights and all) to encode the second tweet. Here we use a shared LSTM layer to encode the tweets.  Let's build this with the functional API. We will take as input for a tweet a binary matrix of shape  (140, 256) , i.e. a sequence of 140 vectors of size 256, where each dimension in the 256-dimensional vector encodes the presence/absence of a character (out of an alphabet of 256 frequent characters).  from keras.layers import Input, LSTM, Dense, merge\nfrom keras.models import Model\n\ntweet_a = Input(shape=(140, 256))\ntweet_b = Input(shape=(140, 256))  To share a layer across different inputs, simply instantiate the layer once, then call it on as many inputs as you want:  # this layer can take as input a matrix\n# and will return a vector of size 64\nshared_lstm = LSTM(64)\n\n# when we reuse the same layer instance\n# multiple times, the weights of the layer\n# are also being reused\n# (it is effectively *the same* layer)\nencoded_a = shared_lstm(tweet_a)\nencoded_b = shared_lstm(tweet_b)\n\n# we can then concatenate the two vectors:\nmerged_vector = merge([encoded_a, encoded_b], mode='concat', concat_axis=-1)\n\n# and add a logistic regression on top\npredictions = Dense(1, activation='sigmoid')(merged_vector)\n\n# we define a trainable model linking the\n# tweet inputs to the predictions\nmodel = Model(input=[tweet_a, tweet_b], output=predictions)\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.fit([data_a, data_b], labels, nb_epoch=10)  Let's pause to take a look at how to read the shared layer's output or output shape.", 
            "title": "Shared layers"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#the-concept-of-layer-node", 
            "text": "Whenever you are calling a layer on some input, you are creating a new tensor (the output of the layer), and you are adding a \"node\" to the layer, linking the input tensor to the output tensor. When you are calling the same layer multiple times, that layer owns multiple nodes indexed as 0, 1, 2...  In previous versions of Keras, you could obtain the output tensor of a layer instance via  layer.get_output() , or its output shape via  layer.output_shape . You still can (except  get_output()  has been replaced by the property  output ). But what if a layer is connected to multiple inputs?  As long as a layer is only connected to one input, there is no confusion, and  .output  will return the one output of the layer:  a = Input(shape=(140, 256))\n\nlstm = LSTM(32)\nencoded_a = lstm(a)\n\nassert lstm.output == encoded_a  Not so if the layer has multiple inputs:  a = Input(shape=(140, 256))\nb = Input(shape=(140, 256))\n\nlstm = LSTM(32)\nencoded_a = lstm(a)\nencoded_b = lstm(b)\n\nlstm.output   AssertionError: Layer lstm_1 has multiple inbound nodes,\nhence the notion of  layer output  is ill-defined.\nUse `get_output_at(node_index)` instead.  Okay then. The following works:  assert lstm.get_output_at(0) == encoded_a\nassert lstm.get_output_at(1) == encoded_b  Simple enough, right?  The same is true for the properties  input_shape  and  output_shape : as long as the layer has only one node, or as long as all nodes have the same input/output shape, then the notion of \"layer output/input shape\" is well defined, and that one shape will be returned by  layer.output_shape / layer.input_shape . But if, for instance, you apply a same  Convolution2D  layer to an input of shape  (3, 32, 32) , and then to an input of shape  (3, 64, 64) , the layer will have multiple input/output shapes, and you will have to fetch them by specifying the index of the node they belong to:  a = Input(shape=(3, 32, 32))\nb = Input(shape=(3, 64, 64))\n\nconv = Convolution2D(16, 3, 3, border_mode='same')\nconved_a = conv(a)\n\n# only one input so far, the following will work:\nassert conv.input_shape == (None, 3, 32, 32)\n\nconved_b = conv(b)\n# now the `.input_shape` property wouldn't work, but this does:\nassert conv.get_input_shape_at(0) == (None, 3, 32, 32)\nassert conv.get_input_shape_at(1) == (None, 3, 64, 64)", 
            "title": "The concept of layer \"node\""
        }, 
        {
            "location": "/getting-started/functional-api-guide/#more-examples", 
            "text": "Code examples are still the best way to get started, so here are a few more.", 
            "title": "More examples"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#inception-module", 
            "text": "For more information about the Inception architecture, see  Going Deeper with Convolutions .  from keras.layers import merge, Convolution2D, MaxPooling2D, Input\n\ninput_img = Input(shape=(3, 256, 256))\n\ntower_1 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(input_img)\ntower_1 = Convolution2D(64, 3, 3, border_mode='same', activation='relu')(tower_1)\n\ntower_2 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(input_img)\ntower_2 = Convolution2D(64, 5, 5, border_mode='same', activation='relu')(tower_2)\n\ntower_3 = MaxPooling2D((3, 3), strides=(1, 1), border_mode='same')(input_img)\ntower_3 = Convolution2D(64, 1, 1, border_mode='same', activation='relu')(tower_3)\n\noutput = merge([tower_1, tower_2, tower_3], mode='concat', concat_axis=1)", 
            "title": "Inception module"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#residual-connection-on-a-convolution-layer", 
            "text": "For more information about residual networks, see  Deep Residual Learning for Image Recognition .  from keras.layers import merge, Convolution2D, Input\n\n# input tensor for a 3-channel 256x256 image\nx = Input(shape=(3, 256, 256))\n# 3x3 conv with 3 output channels (same as input channels)\ny = Convolution2D(3, 3, 3, border_mode='same')(x)\n# this returns x + y.\nz = merge([x, y], mode='sum')", 
            "title": "Residual connection on a convolution layer"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#shared-vision-model", 
            "text": "This model re-uses the same image-processing module on two inputs, to classify whether two MNIST digits are the same digit or different digits.  from keras.layers import merge, Convolution2D, MaxPooling2D, Input, Dense, Flatten\nfrom keras.models import Model\n\n# first, define the vision modules\ndigit_input = Input(shape=(1, 27, 27))\nx = Convolution2D(64, 3, 3)(digit_input)\nx = Convolution2D(64, 3, 3)(x)\nx = MaxPooling2D((2, 2))(x)\nout = Flatten()(x)\n\nvision_model = Model(digit_input, out)\n\n# then define the tell-digits-apart model\ndigit_a = Input(shape=(1, 27, 27))\ndigit_b = Input(shape=(1, 27, 27))\n\n# the vision model will be shared, weights and all\nout_a = vision_model(digit_a)\nout_b = vision_model(digit_b)\n\nconcatenated = merge([out_a, out_b], mode='concat')\nout = Dense(1, activation='sigmoid')(concatenated)\n\nclassification_model = Model([digit_a, digit_b], out)", 
            "title": "Shared vision model"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#visual-question-answering-model", 
            "text": "This model can select the correct one-word answer when asked a natural-language question about a picture.  It works by encoding the question into a vector, encoding the image into a vector, concatenating the two, and training on top a logistic regression over some vocabulary of potential answers.  from keras.layers import Convolution2D, MaxPooling2D, Flatten\nfrom keras.layers import Input, LSTM, Embedding, Dense, merge\nfrom keras.models import Model, Sequential\n\n# first, let's define a vision model using a Sequential model.\n# this model will encode an image into a vector.\nvision_model = Sequential()\nvision_model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same', input_shape=(3, 224, 224)))\nvision_model.add(Convolution2D(64, 3, 3, activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same'))\nvision_model.add(Convolution2D(128, 3, 3, activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Convolution2D(256, 3, 3, activation='relu', border_mode='same'))\nvision_model.add(Convolution2D(256, 3, 3, activation='relu'))\nvision_model.add(Convolution2D(256, 3, 3, activation='relu'))\nvision_model.add(MaxPooling2D((2, 2)))\nvision_model.add(Flatten())\n\n# now let's get a tensor with the output of our vision model:\nimage_input = Input(shape=(3, 224, 224))\nencoded_image = vision_model(image_input)\n\n# next, let's define a language model to encode the question into a vector.\n# each question will be at most 100 word long,\n# and we will index words as integers from 1 to 9999.\nquestion_input = Input(shape=(100,), dtype='int32')\nembedded_question = Embedding(input_dim=10000, output_dim=256, input_length=100)(question_input)\nencoded_question = LSTM(256)(embedded_question)\n\n# let's concatenate the question vector and the image vector:\nmerged = merge([encoded_question, encoded_image], mode='concat')\n\n# and let's train a logistic regression over 1000 words on top:\noutput = Dense(1000, activation='softmax')(merged)\n\n# this is our final model:\nvqa_model = Model(input=[image_input, question_input], output=output)\n\n# the next stage would be training this model on actual data.", 
            "title": "Visual question answering model"
        }, 
        {
            "location": "/getting-started/functional-api-guide/#video-question-answering-model", 
            "text": "Now that we have trained our image QA model, we can quickly turn it into a video QA model. With appropriate training, you will be able to show it a short video (e.g. 100-frame human action) and ask a natural language question about the video (e.g. \"what sport is the boy playing?\" -  \"football\").  from keras.layers import TimeDistributed\n\nvideo_input = Input(shape=(100, 3, 224, 224))\n# this is our video encoded via the previously trained vision_model (weights are reused)\nencoded_frame_sequence = TimeDistributed(vision_model)(video_input)  # the output will be a sequence of vectors\nencoded_video = LSTM(256)(encoded_frame_sequence)  # the output will be a vector\n\n# this is a model-level representation of the question encoder, reusing the same weights as before:\nquestion_encoder = Model(input=question_input, output=encoded_question)\n\n# let's use it to encode the question:\nvideo_question_input = Input(shape=(100,), dtype='int32')\nencoded_video_question = question_encoder(video_question_input)\n\n# and this is our video question answering model:\nmerged = merge([encoded_video, encoded_video_question], mode='concat')\noutput = Dense(1000, activation='softmax')(merged)\nvideo_qa_model = Model(input=[video_input, video_question_input], output=output)", 
            "title": "Video question answering model"
        }, 
        {
            "location": "/getting-started/faq/", 
            "text": "Keras FAQ: Frequently Asked Keras Questions\n\n\n\n\nHow should I cite Keras?\n\n\nHow can I run Keras on GPU?\n\n\nHow can I save a Keras model?\n\n\nWhy is the training loss much higher than the testing loss?\n\n\nHow can I visualize the output of an intermediate layer?\n\n\nHow can I use Keras with datasets that don't fit in memory?\n\n\nHow can I interrupt training when the validation loss isn't decreasing anymore?\n\n\nHow is the validation split computed?\n\n\nIs the data shuffled during training?\n\n\nHow can I record the training / validation loss / accuracy at each epoch?\n\n\nHow can I \"freeze\" layers?\n\n\nHow can I use stateful RNNs?\n\n\nHow can I remove a layer from a Sequential model?\n\n\nHow can I use pre-trained models in Keras?\n\n\n\n\n\n\nHow should I cite Keras?\n\n\nPlease cite Keras in your publications if it helps your research. Here is an example BibTeX entry:\n\n\n@misc{chollet2015keras,\n  title={Keras},\n  author={Chollet, Fran\\c{c}ois},\n  year={2015},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/fchollet/keras}},\n}\n\n\n\n\nHow can I run Keras on GPU?\n\n\nIf you are running on the TensorFlow backend, your code will automatically run on GPU if any available GPU is detected.\nIf you are running on the Theano backend, you can use one of the following methods:\n\n\nMethod 1: use Theano flags.\n\n\nTHEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py\n\n\n\n\nThe name 'gpu' might have to be changed depending on your device's identifier (e.g. \ngpu0\n, \ngpu1\n, etc).\n\n\nMethod 2: set up your \n.theanorc\n: \nInstructions\n\n\nMethod 3: manually set \ntheano.config.device\n, \ntheano.config.floatX\n at the beginning of your code:\n\n\nimport theano\ntheano.config.device = 'gpu'\ntheano.config.floatX = 'float32'\n\n\n\n\n\n\nHow can I save a Keras model?\n\n\nIt is not recommended to use pickle or cPickle to save a Keras model.\n\n\nYou can use \nmodel.save(filepath)\n to save a Keras model into a single HDF5 file which will contain:\n\n\n\n\nthe architecture of the model, allowing to re-create the model\n\n\nthe weights of the model\n\n\nthe training configuration (loss, optimizer)\n\n\nthe state of the optimizer, allowing to resume training exactly where you left off.\n\n\n\n\nYou can then use \nkeras.models.load_model(filepath)\n to reinstantiate your model.\n\nload_model\n will also take care of compiling the model using the saved training configuration\n(unless the model was never compiled in the first place).\n\n\nExample:\n\n\nfrom keras.models import load_model\n\nmodel.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model\n\n# returns a compiled model\n# identical to the previous one\nmodel = load_model('my_model.h5')\n\n\n\n\nIf you only need to save the \narchitecture of a model\n, and not its weights or its training configuration, you can do:\n\n\n# save as JSON\njson_string = model.to_json()\n\n# save as YAML\nyaml_string = model.to_yaml()\n\n\n\n\nThe generated JSON / YAML files are human-readable and can be manually edited if needed.\n\n\nYou can then build a fresh model from this data:\n\n\n# model reconstruction from JSON:\nfrom keras.models import model_from_json\nmodel = model_from_json(json_string)\n\n# model reconstruction from YAML\nmodel = model_from_yaml(yaml_string)\n\n\n\n\nIf you need to save the \nweights of a model\n, you can do so in HDF5 with the code below.\n\n\nNote that you will first need to install HDF5 and the Python library h5py, which do not come bundled with Keras.\n\n\nmodel.save_weights('my_model_weights.h5')\n\n\n\n\nAssuming you have code for instantiating your model, you can then load the weights you saved into a model with the \nsame\n architecture:\n\n\nmodel.load_weights('my_model_weights.h5')\n\n\n\n\nIf you need to load weights into a \ndifferent\n architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load weights by \nlayer name\n:\n\n\nmodel.load_weights('my_model_weights.h5', by_name=True)\n\n\n\n\nFor example:\n\n\n\nAssume original model looks like this:\n    model = Sequential()\n    model.add(Dense(2, input_dim=3, name=\ndense_1\n))\n    model.add(Dense(3, name=\ndense_2\n))\n    ...\n    model.save_weights(fname)\n\n\n\n# new model\nmodel = Sequential()\nmodel.add(Dense(2, input_dim=3, name=\ndense_1\n))  # will be loaded\nmodel.add(Dense(10, name=\nnew_dense\n))  # will not be loaded\n\n# load weights from first model; will only affect the first layer, dense_1.\nmodel.load_weights(fname, by_name=True)\n\n\n\n\n\n\nWhy is the training loss much higher than the testing loss?\n\n\nA Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.\n\n\nBesides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.\n\n\n\n\nHow can I visualize the output of an intermediate layer?\n\n\nYou can build a Keras function that will return the output of a certain layer given a certain input, for example:\n\n\nfrom keras import backend as K\n\n# with a Sequential model\nget_3rd_layer_output = K.function([model.layers[0].input],\n                                  [model.layers[3].output])\nlayer_output = get_3rd_layer_output([X])[0]\n\n\n\n\nSimilarly, you could build a Theano and TensorFlow function directly.\n\n\nNote that if your model has a different behavior in training and testing phase (e.g. if it uses \nDropout\n, \nBatchNormalization\n, etc.), you will need\nto pass the learning phase flag to your function:\n\n\nget_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n                                  [model.layers[3].output])\n\n# output in test mode = 0\nlayer_output = get_3rd_layer_output([X, 0])[0]\n\n# output in train mode = 1\nlayer_output = get_3rd_layer_output([X, 1])[0]\n\n\n\n\nAnother more flexible way of getting output from intermediate layers is to use the \nfunctional API\n. For example, if you have created an autoencoder for MNIST:\n\n\ninputs = Input(shape=(784,))\nencoded = Dense(32, activation='relu')(inputs)\ndecoded = Dense(784)(encoded)\nmodel = Model(input=inputs, output=decoded)\n\n\n\n\nAfter compiling and training the model, you can get the output of the data from the encoder like this:\n\n\nencoder = Model(input=inputs, output=encoded)\nX_encoded = encoder.predict(X)\n\n\n\n\n\n\nHow can I use Keras with datasets that don't fit in memory?\n\n\nYou can do batch training using \nmodel.train_on_batch(X, y)\n and \nmodel.test_on_batch(X, y)\n. See the \nmodels documentation\n.\n\n\nAlternatively, you can write a generator that yields batches of training data and use the method \nmodel.fit_generator(data_generator, samples_per_epoch, nb_epoch)\n.\n\n\nYou can see batch training in action in our \nCIFAR10 example\n.\n\n\n\n\nHow can I interrupt training when the validation loss isn't decreasing anymore?\n\n\nYou can use an \nEarlyStopping\n callback:\n\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\nmodel.fit(X, y, validation_split=0.2, callbacks=[early_stopping])\n\n\n\n\nFind out more in the \ncallbacks documentation\n.\n\n\n\n\nHow is the validation split computed?\n\n\nIf you set the \nvalidation_split\n argument in \nmodel.fit\n to e.g. 0.1, then the validation data used will be the \nlast 10%\n of the data. If you set it to 0.25, it will be the last 25% of the data, etc.\n\n\n\n\nIs the data shuffled during training?\n\n\nYes, if the \nshuffle\n argument in \nmodel.fit\n is set to \nTrue\n (which is the default), the training data will be randomly shuffled at each epoch.\n\n\nValidation data is never shuffled.\n\n\n\n\nHow can I record the training / validation loss / accuracy at each epoch?\n\n\nThe \nmodel.fit\n method returns an \nHistory\n callback, which has a \nhistory\n attribute containing the lists of successive losses and other metrics.\n\n\nhist = model.fit(X, y, validation_split=0.2)\nprint(hist.history)\n\n\n\n\n\n\nHow can I \"freeze\" Keras layers?\n\n\nTo \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input.\n\n\nYou can pass a \ntrainable\n argument (boolean) to a layer constructor to set a layer to be non-trainable:\n\n\nfrozen_layer = Dense(32, trainable=False)\n\n\n\n\nAdditionally, you can set the \ntrainable\n property of a layer to \nTrue\n or \nFalse\n after instantiation. For this to take effect, you will need to call \ncompile()\n on your model after modifying the \ntrainable\n property. Here's an example:\n\n\nx = Input(shape=(32,))\nlayer = Dense(32)\nlayer.trainable = False\ny = layer(x)\n\nfrozen_model = Model(x, y)\n# in the model below, the weights of `layer` will not be updated during training\nfrozen_model.compile(optimizer='rmsprop', loss='mse')\n\nlayer.trainable = True\ntrainable_model = Model(x, y)\n# with this model the weights of the layer will be updated during training\n# (which will also affect the above model since it uses the same layer instance)\ntrainable_model.compile(optimizer='rmsprop', loss='mse')\n\nfrozen_model.fit(data, labels)  # this does NOT update the weights of `layer`\ntrainable_model.fit(data, labels)  # this updates the weights of `layer`\n\n\n\n\n\n\nHow can I use stateful RNNs?\n\n\nMaking a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.\n\n\nWhen using stateful RNNs, it is therefore assumed that:\n\n\n\n\nall batches have the same number of samples\n\n\nIf \nX1\n and \nX2\n are successive batches of samples, then \nX2[i]\n is the follow-up sequence to \nX1[i]\n, for every \ni\n.\n\n\n\n\nTo use statefulness in RNNs, you need to:\n\n\n\n\nexplicitly specify the batch size you are using, by passing a \nbatch_input_shape\n argument to the first layer in your model. It should be a tuple of integers, e.g. \n(32, 10, 16)\n for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep.\n\n\nset \nstateful=True\n in your RNN layer(s).\n\n\n\n\nTo reset the states accumulated:\n\n\n\n\nuse \nmodel.reset_states()\n to reset the states of all layers in the model\n\n\nuse \nlayer.reset_states()\n to reset the states of a specific stateful RNN layer\n\n\n\n\nExample:\n\n\n\nX  # this is our input data, of shape (32, 21, 16)\n# we will feed it to our model in sequences of length 10\n\nmodel = Sequential()\nmodel.add(LSTM(32, batch_input_shape=(32, 10, 16), stateful=True))\nmodel.add(Dense(16, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n# we train the network to predict the 11th timestep given the first 10:\nmodel.train_on_batch(X[:, :10, :], np.reshape(X[:, 10, :], (32, 16)))\n\n# the state of the network has changed. We can feed the follow-up sequences:\nmodel.train_on_batch(X[:, 10:20, :], np.reshape(X[:, 20, :], (32, 16)))\n\n# let's reset the states of the LSTM layer:\nmodel.reset_states()\n\n# another way to do it in this case:\nmodel.layers[0].reset_states()\n\n\n\n\nNotes that the methods \npredict\n, \nfit\n, \ntrain_on_batch\n, \npredict_classes\n, etc. will \nall\n update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction.\n\n\n\n\nHow can I remove a layer from a Sequential model?\n\n\nYou can remove the last added layer in a Sequential model by calling \n.pop()\n:\n\n\nmodel = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=784))\nmodel.add(Dense(32, activation='relu'))\n\nprint(len(model.layers))  # \n2\n\n\nmodel.pop()\nprint(len(model.layers))  # \n1\n\n\n\n\n\n\n\nHow can I use pre-trained models in Keras?\n\n\nCode and pre-trained weights are available for the following image classification models:\n\n\n\n\nVGG16\n\n\nVGG19\n\n\nResNet50\n\n\nInception v3\n\n\n\n\nThey can be imported from the module \nkeras.applications\n:\n\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.inception_v3 import InceptionV3\n\nmodel = VGG16(weights='imagenet', include_top=True)\n\n\n\n\nFor a few simple usage examples, see \nthe documentation for the Applications module\n.\n\n\nFor a detailed example of how to use such a pre-trained model for feature extraction or for fine-tuning, see \nthis blog post\n.\n\n\nThe VGG16 model is also the basis for several Keras example scripts:\n\n\n\n\nStyle transfer\n\n\nFeature visualization\n\n\nDeep dream", 
            "title": "FAQ"
        }, 
        {
            "location": "/getting-started/faq/#keras-faq-frequently-asked-keras-questions", 
            "text": "How should I cite Keras?  How can I run Keras on GPU?  How can I save a Keras model?  Why is the training loss much higher than the testing loss?  How can I visualize the output of an intermediate layer?  How can I use Keras with datasets that don't fit in memory?  How can I interrupt training when the validation loss isn't decreasing anymore?  How is the validation split computed?  Is the data shuffled during training?  How can I record the training / validation loss / accuracy at each epoch?  How can I \"freeze\" layers?  How can I use stateful RNNs?  How can I remove a layer from a Sequential model?  How can I use pre-trained models in Keras?", 
            "title": "Keras FAQ: Frequently Asked Keras Questions"
        }, 
        {
            "location": "/getting-started/faq/#how-should-i-cite-keras", 
            "text": "Please cite Keras in your publications if it helps your research. Here is an example BibTeX entry:  @misc{chollet2015keras,\n  title={Keras},\n  author={Chollet, Fran\\c{c}ois},\n  year={2015},\n  publisher={GitHub},\n  howpublished={\\url{https://github.com/fchollet/keras}},\n}", 
            "title": "How should I cite Keras?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-run-keras-on-gpu", 
            "text": "If you are running on the TensorFlow backend, your code will automatically run on GPU if any available GPU is detected.\nIf you are running on the Theano backend, you can use one of the following methods:  Method 1: use Theano flags.  THEANO_FLAGS=device=gpu,floatX=float32 python my_keras_script.py  The name 'gpu' might have to be changed depending on your device's identifier (e.g.  gpu0 ,  gpu1 , etc).  Method 2: set up your  .theanorc :  Instructions  Method 3: manually set  theano.config.device ,  theano.config.floatX  at the beginning of your code:  import theano\ntheano.config.device = 'gpu'\ntheano.config.floatX = 'float32'", 
            "title": "How can I run Keras on GPU?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-save-a-keras-model", 
            "text": "It is not recommended to use pickle or cPickle to save a Keras model.  You can use  model.save(filepath)  to save a Keras model into a single HDF5 file which will contain:   the architecture of the model, allowing to re-create the model  the weights of the model  the training configuration (loss, optimizer)  the state of the optimizer, allowing to resume training exactly where you left off.   You can then use  keras.models.load_model(filepath)  to reinstantiate your model. load_model  will also take care of compiling the model using the saved training configuration\n(unless the model was never compiled in the first place).  Example:  from keras.models import load_model\n\nmodel.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\ndel model  # deletes the existing model\n\n# returns a compiled model\n# identical to the previous one\nmodel = load_model('my_model.h5')  If you only need to save the  architecture of a model , and not its weights or its training configuration, you can do:  # save as JSON\njson_string = model.to_json()\n\n# save as YAML\nyaml_string = model.to_yaml()  The generated JSON / YAML files are human-readable and can be manually edited if needed.  You can then build a fresh model from this data:  # model reconstruction from JSON:\nfrom keras.models import model_from_json\nmodel = model_from_json(json_string)\n\n# model reconstruction from YAML\nmodel = model_from_yaml(yaml_string)  If you need to save the  weights of a model , you can do so in HDF5 with the code below.  Note that you will first need to install HDF5 and the Python library h5py, which do not come bundled with Keras.  model.save_weights('my_model_weights.h5')  Assuming you have code for instantiating your model, you can then load the weights you saved into a model with the  same  architecture:  model.load_weights('my_model_weights.h5')  If you need to load weights into a  different  architecture (with some layers in common), for instance for fine-tuning or transfer-learning, you can load weights by  layer name :  model.load_weights('my_model_weights.h5', by_name=True)  For example:  \nAssume original model looks like this:\n    model = Sequential()\n    model.add(Dense(2, input_dim=3, name= dense_1 ))\n    model.add(Dense(3, name= dense_2 ))\n    ...\n    model.save_weights(fname) \n\n# new model\nmodel = Sequential()\nmodel.add(Dense(2, input_dim=3, name= dense_1 ))  # will be loaded\nmodel.add(Dense(10, name= new_dense ))  # will not be loaded\n\n# load weights from first model; will only affect the first layer, dense_1.\nmodel.load_weights(fname, by_name=True)", 
            "title": "How can I save a Keras model?"
        }, 
        {
            "location": "/getting-started/faq/#why-is-the-training-loss-much-higher-than-the-testing-loss", 
            "text": "A Keras model has two modes: training and testing. Regularization mechanisms, such as Dropout and L1/L2 weight regularization, are turned off at testing time.  Besides, the training loss is the average of the losses over each batch of training data. Because your model is changing over time, the loss over the first batches of an epoch is generally higher than over the last batches. On the other hand, the testing loss for an epoch is computed using the model as it is at the end of the epoch, resulting in a lower loss.", 
            "title": "Why is the training loss much higher than the testing loss?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-visualize-the-output-of-an-intermediate-layer", 
            "text": "You can build a Keras function that will return the output of a certain layer given a certain input, for example:  from keras import backend as K\n\n# with a Sequential model\nget_3rd_layer_output = K.function([model.layers[0].input],\n                                  [model.layers[3].output])\nlayer_output = get_3rd_layer_output([X])[0]  Similarly, you could build a Theano and TensorFlow function directly.  Note that if your model has a different behavior in training and testing phase (e.g. if it uses  Dropout ,  BatchNormalization , etc.), you will need\nto pass the learning phase flag to your function:  get_3rd_layer_output = K.function([model.layers[0].input, K.learning_phase()],\n                                  [model.layers[3].output])\n\n# output in test mode = 0\nlayer_output = get_3rd_layer_output([X, 0])[0]\n\n# output in train mode = 1\nlayer_output = get_3rd_layer_output([X, 1])[0]  Another more flexible way of getting output from intermediate layers is to use the  functional API . For example, if you have created an autoencoder for MNIST:  inputs = Input(shape=(784,))\nencoded = Dense(32, activation='relu')(inputs)\ndecoded = Dense(784)(encoded)\nmodel = Model(input=inputs, output=decoded)  After compiling and training the model, you can get the output of the data from the encoder like this:  encoder = Model(input=inputs, output=encoded)\nX_encoded = encoder.predict(X)", 
            "title": "How can I visualize the output of an intermediate layer?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-use-keras-with-datasets-that-dont-fit-in-memory", 
            "text": "You can do batch training using  model.train_on_batch(X, y)  and  model.test_on_batch(X, y) . See the  models documentation .  Alternatively, you can write a generator that yields batches of training data and use the method  model.fit_generator(data_generator, samples_per_epoch, nb_epoch) .  You can see batch training in action in our  CIFAR10 example .", 
            "title": "How can I use Keras with datasets that don't fit in memory?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-interrupt-training-when-the-validation-loss-isnt-decreasing-anymore", 
            "text": "You can use an  EarlyStopping  callback:  from keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=2)\nmodel.fit(X, y, validation_split=0.2, callbacks=[early_stopping])  Find out more in the  callbacks documentation .", 
            "title": "How can I interrupt training when the validation loss isn't decreasing anymore?"
        }, 
        {
            "location": "/getting-started/faq/#how-is-the-validation-split-computed", 
            "text": "If you set the  validation_split  argument in  model.fit  to e.g. 0.1, then the validation data used will be the  last 10%  of the data. If you set it to 0.25, it will be the last 25% of the data, etc.", 
            "title": "How is the validation split computed?"
        }, 
        {
            "location": "/getting-started/faq/#is-the-data-shuffled-during-training", 
            "text": "Yes, if the  shuffle  argument in  model.fit  is set to  True  (which is the default), the training data will be randomly shuffled at each epoch.  Validation data is never shuffled.", 
            "title": "Is the data shuffled during training?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-record-the-training-validation-loss-accuracy-at-each-epoch", 
            "text": "The  model.fit  method returns an  History  callback, which has a  history  attribute containing the lists of successive losses and other metrics.  hist = model.fit(X, y, validation_split=0.2)\nprint(hist.history)", 
            "title": "How can I record the training / validation loss / accuracy at each epoch?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-freeze-keras-layers", 
            "text": "To \"freeze\" a layer means to exclude it from training, i.e. its weights will never be updated. This is useful in the context of fine-tuning a model, or using fixed embeddings for a text input.  You can pass a  trainable  argument (boolean) to a layer constructor to set a layer to be non-trainable:  frozen_layer = Dense(32, trainable=False)  Additionally, you can set the  trainable  property of a layer to  True  or  False  after instantiation. For this to take effect, you will need to call  compile()  on your model after modifying the  trainable  property. Here's an example:  x = Input(shape=(32,))\nlayer = Dense(32)\nlayer.trainable = False\ny = layer(x)\n\nfrozen_model = Model(x, y)\n# in the model below, the weights of `layer` will not be updated during training\nfrozen_model.compile(optimizer='rmsprop', loss='mse')\n\nlayer.trainable = True\ntrainable_model = Model(x, y)\n# with this model the weights of the layer will be updated during training\n# (which will also affect the above model since it uses the same layer instance)\ntrainable_model.compile(optimizer='rmsprop', loss='mse')\n\nfrozen_model.fit(data, labels)  # this does NOT update the weights of `layer`\ntrainable_model.fit(data, labels)  # this updates the weights of `layer`", 
            "title": "How can I \"freeze\" Keras layers?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-use-stateful-rnns", 
            "text": "Making a RNN stateful means that the states for the samples of each batch will be reused as initial states for the samples in the next batch.  When using stateful RNNs, it is therefore assumed that:   all batches have the same number of samples  If  X1  and  X2  are successive batches of samples, then  X2[i]  is the follow-up sequence to  X1[i] , for every  i .   To use statefulness in RNNs, you need to:   explicitly specify the batch size you are using, by passing a  batch_input_shape  argument to the first layer in your model. It should be a tuple of integers, e.g.  (32, 10, 16)  for a 32-samples batch of sequences of 10 timesteps with 16 features per timestep.  set  stateful=True  in your RNN layer(s).   To reset the states accumulated:   use  model.reset_states()  to reset the states of all layers in the model  use  layer.reset_states()  to reset the states of a specific stateful RNN layer   Example:  \nX  # this is our input data, of shape (32, 21, 16)\n# we will feed it to our model in sequences of length 10\n\nmodel = Sequential()\nmodel.add(LSTM(32, batch_input_shape=(32, 10, 16), stateful=True))\nmodel.add(Dense(16, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n# we train the network to predict the 11th timestep given the first 10:\nmodel.train_on_batch(X[:, :10, :], np.reshape(X[:, 10, :], (32, 16)))\n\n# the state of the network has changed. We can feed the follow-up sequences:\nmodel.train_on_batch(X[:, 10:20, :], np.reshape(X[:, 20, :], (32, 16)))\n\n# let's reset the states of the LSTM layer:\nmodel.reset_states()\n\n# another way to do it in this case:\nmodel.layers[0].reset_states()  Notes that the methods  predict ,  fit ,  train_on_batch ,  predict_classes , etc. will  all  update the states of the stateful layers in a model. This allows you to do not only stateful training, but also stateful prediction.", 
            "title": "How can I use stateful RNNs?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-remove-a-layer-from-a-sequential-model", 
            "text": "You can remove the last added layer in a Sequential model by calling  .pop() :  model = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=784))\nmodel.add(Dense(32, activation='relu'))\n\nprint(len(model.layers))  #  2 \n\nmodel.pop()\nprint(len(model.layers))  #  1", 
            "title": "How can I remove a layer from a Sequential model?"
        }, 
        {
            "location": "/getting-started/faq/#how-can-i-use-pre-trained-models-in-keras", 
            "text": "Code and pre-trained weights are available for the following image classification models:   VGG16  VGG19  ResNet50  Inception v3   They can be imported from the module  keras.applications :  from keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.inception_v3 import InceptionV3\n\nmodel = VGG16(weights='imagenet', include_top=True)  For a few simple usage examples, see  the documentation for the Applications module .  For a detailed example of how to use such a pre-trained model for feature extraction or for fine-tuning, see  this blog post .  The VGG16 model is also the basis for several Keras example scripts:   Style transfer  Feature visualization  Deep dream", 
            "title": "How can I use pre-trained models in Keras?"
        }, 
        {
            "location": "/models/about-keras-models/", 
            "text": "About Keras models\n\n\nThere are two types of models available in Keras: \nthe Sequential model\n and \nthe Model class used with functional API\n.\n\n\nThese models have a number of methods in common:\n\n\n\n\nmodel.summary()\n: prints a summary representation of your model.\n\n\nmodel.get_config()\n: returns a dictionary containing the configuration of the model. The model can be reinstantiated from its config via:\n\n\n\n\nconfig = model.get_config()\nmodel = Model.from_config(config)\n# or, for Sequential:\nmodel = Sequential.from_config(config)\n\n\n\n\n\n\nmodel.get_weights()\n: returns a list of all weight tensors in the model, as Numpy arrays.\n\n\nmodel.set_weights(weights)\n: sets the values of the weights of the model, from a list of Numpy arrays. The arrays in the list should have the same shape as those returned by \nget_weights()\n.\n\n\nmodel.to_json()\n: returns a representation of the model as a JSON string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the JSON string via:\n\n\n\n\nfrom models import model_from_json\n\njson_string = model.to_json()\nmodel = model_from_json(json_string)\n\n\n\n\n\n\nmodel.to_yaml()\n: returns a representation of the model as a YAML string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the YAML string via:\n\n\n\n\nfrom models import model_from_yaml\n\nyaml_string = model.to_yaml()\nmodel = model_from_yaml(yaml_string)\n\n\n\n\n\n\nmodel.save_weights(filepath)\n: saves the weights of the model as a HDF5 file.\n\n\nmodel.load_weights(filepath, by_name=False)\n: loads the weights of the model from a HDF5 file (created by \nsave_weights\n). By default, the architecture is expected to be unchanged. To load weights into a different architecture (with some layers in common), use \nby_name=True\n to load only those layers with the same name.", 
            "title": "About Keras models"
        }, 
        {
            "location": "/models/about-keras-models/#about-keras-models", 
            "text": "There are two types of models available in Keras:  the Sequential model  and  the Model class used with functional API .  These models have a number of methods in common:   model.summary() : prints a summary representation of your model.  model.get_config() : returns a dictionary containing the configuration of the model. The model can be reinstantiated from its config via:   config = model.get_config()\nmodel = Model.from_config(config)\n# or, for Sequential:\nmodel = Sequential.from_config(config)   model.get_weights() : returns a list of all weight tensors in the model, as Numpy arrays.  model.set_weights(weights) : sets the values of the weights of the model, from a list of Numpy arrays. The arrays in the list should have the same shape as those returned by  get_weights() .  model.to_json() : returns a representation of the model as a JSON string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the JSON string via:   from models import model_from_json\n\njson_string = model.to_json()\nmodel = model_from_json(json_string)   model.to_yaml() : returns a representation of the model as a YAML string. Note that the representation does not include the weights, only the architecture. You can reinstantiate the same model (with reinitialized weights) from the YAML string via:   from models import model_from_yaml\n\nyaml_string = model.to_yaml()\nmodel = model_from_yaml(yaml_string)   model.save_weights(filepath) : saves the weights of the model as a HDF5 file.  model.load_weights(filepath, by_name=False) : loads the weights of the model from a HDF5 file (created by  save_weights ). By default, the architecture is expected to be unchanged. To load weights into a different architecture (with some layers in common), use  by_name=True  to load only those layers with the same name.", 
            "title": "About Keras models"
        }, 
        {
            "location": "/models/sequential/", 
            "text": "The Sequential model API\n\n\nTo get started, read \nthis guide to the Keras Sequential model\n.\n\n\nUseful attributes of Model\n\n\n\n\nmodel.layers\n is a list of the layers added to the model.\n\n\n\n\n\n\nSequential model methods\n\n\ncompile\n\n\ncompile(self, optimizer, loss, metrics=None, sample_weight_mode=None)\n\n\n\n\nConfigures the learning process.\n\n\nArguments\n\n\n\n\noptimizer\n: str (name of optimizer) or optimizer object.\n    See \noptimizers\n.\n\n\nloss\n: str (name of objective function) or objective function.\n    See \nobjectives\n.\n\n\nmetrics\n: list of metrics to be evaluated by the model\n    during training and testing.\n    Typically you will use \nmetrics=['accuracy']\n.\n    See \nmetrics\n.\n\n\nsample_weight_mode\n: if you need to do timestep-wise\n    sample weighting (2D weights), set this to \"temporal\".\n    \"None\" defaults to sample-wise weights (1D).\n\n\nkwargs\n: for Theano backend, these are passed into K.function.\n    Ignored for Tensorflow backend.\n\n\n\n\nExample\n\n\n    model = Sequential()\n    model.add(Dense(32, input_shape=(500,)))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer='rmsprop',\n          loss='categorical_crossentropy',\n          metrics=['accuracy'])\n\n\n\n\n\n\nfit\n\n\nfit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n\n\n\n\nTrains the model for a fixed number of epochs.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).\n\n\ny\n: labels, as a Numpy array.\n\n\nbatch_size\n: integer. Number of samples per gradient update.\n\n\nnb_epoch\n: integer, the number of epochs to train the model.\n\n\nverbose\n: 0 for no logging to stdout,\n    1 for progress bar logging, 2 for one log line per epoch.\n\n\ncallbacks\n: list of \nkeras.callbacks.Callback\n instances.\n    List of callbacks to apply during training.\n    See \ncallbacks\n.\n\n\nvalidation_split\n: float (0. \n x \n 1).\n    Fraction of the data to use as held-out validation data.\n\n\nvalidation_data\n: tuple (x_val, y_val) or tuple\n    (x_val, y_val, val_sample_weights) to be used as held-out\n    validation data. Will override validation_split.\n\n\nshuffle\n: boolean or str (for 'batch').\n    Whether to shuffle the samples at each epoch.\n    'batch' is a special option for dealing with the\n    limitations of HDF5 data; it shuffles in batch-sized chunks.\n\n\nclass_weight\n: dictionary mapping classes to a weight value,\n    used for scaling the loss function (during training only).\n\n\nsample_weight\n: Numpy array of weights for\n    the training samples, used for scaling the loss function\n    (during training only). You can either pass a flat (1D)\n    Numpy array with the same length as the input samples\n\n\n(1\n:1 mapping between weights and samples),\nor in the case of temporal data,\nyou can pass a 2D array with shape (samples, sequence_length),\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify\nsample_weight_mode=\"temporal\" in compile().\n\n\n\n\n\n\ninitial_epoch\n: epoch at which to start training\n    (useful for resuming a previous training run)\n\n\n\n\nReturns\n\n\nA \nHistory\n object. Its \nHistory.history\n attribute is\na record of training loss values and metrics values\nat successive epochs, as well as validation loss values\nand validation metrics values (if applicable).\n\n\n\n\nevaluate\n\n\nevaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n\n\n\n\nComputes the loss on some input data, batch by batch.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).\n\n\ny\n: labels, as a Numpy array.\n\n\nbatch_size\n: integer. Number of samples per gradient update.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\nsample_weight\n: sample weights, as a Numpy array.\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\npredict\n\n\npredict(self, x, batch_size=32, verbose=0)\n\n\n\n\nGenerates output predictions for the input samples,\nprocessing the samples in a batched way.\n\n\nArguments\n\n\n\n\nx\n: the input data, as a Numpy array.\n\n\nbatch_size\n: integer.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\n\n\nReturns\n\n\nA Numpy array of predictions.\n\n\n\n\npredict_classes\n\n\npredict_classes(self, x, batch_size=32, verbose=1)\n\n\n\n\nGenerate class predictions for the input samples\nbatch by batch.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).\n\n\nbatch_size\n: integer.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\n\n\nReturns\n\n\nA numpy array of class predictions.\n\n\n\n\npredict_proba\n\n\npredict_proba(self, x, batch_size=32, verbose=1)\n\n\n\n\nGenerates class probability predictions for the input samples\nbatch by batch.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).\n\n\nbatch_size\n: integer.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\n\n\nReturns\n\n\nA Numpy array of probability predictions.\n\n\n\n\ntrain_on_batch\n\n\ntrain_on_batch(self, x, y, class_weight=None, sample_weight=None)\n\n\n\n\nSingle gradient update over one batch of samples.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).\n\n\ny\n: labels, as a Numpy array.\n\n\nclass_weight\n: dictionary mapping classes to a weight value,\n    used for scaling the loss function (during training only).\n\n\nsample_weight\n: sample weights, as a Numpy array.\n\n\n\n\nReturns\n\n\nScalar training loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\ntest_on_batch\n\n\ntest_on_batch(self, x, y, sample_weight=None)\n\n\n\n\nEvaluates the model over a single batch of samples.\n\n\nArguments\n\n\n\n\nx\n: input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).\n\n\ny\n: labels, as a Numpy array.\n\n\nsample_weight\n: sample weights, as a Numpy array.\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\npredict_on_batch\n\n\npredict_on_batch(self, x)\n\n\n\n\nReturns predictions for a single batch of samples.\n\n\n\n\nfit_generator\n\n\nfit_generator(self, generator, samples_per_epoch, nb_epoch, verbose=1, callbacks=None, validation_data=None, nb_val_samples=None, class_weight=None, max_q_size=10, nb_worker=1, pickle_safe=False, initial_epoch=0)\n\n\n\n\nFits the model on data generated batch-by-batch by\na Python generator.\nThe generator is run in parallel to the model, for efficiency.\nFor instance, this allows you to do real-time data augmentation\non images on CPU in parallel to training your model on GPU.\n\n\nArguments\n\n\n\n\ngenerator\n: a generator.\n    The output of the generator must be either\n\n\na tuple (inputs, targets)\n\n\na tuple (inputs, targets, sample_weights).\nAll arrays should contain the same number of samples.\nThe generator is expected to loop over its data\nindefinitely. An epoch finishes when \nsamples_per_epoch\n\nsamples have been seen by the model.\n\n\n\n\n\n\nsamples_per_epoch\n: integer, number of samples to process before\n    going to the next epoch.\n\n\nnb_epoch\n: integer, total number of iterations on the data.\n\n\nverbose\n: verbosity mode, 0, 1, or 2.\n\n\ncallbacks\n: list of callbacks to be called during training.\n\n\nvalidation_data\n: this can be either\n\n\na generator for the validation data\n\n\na tuple (inputs, targets)\n\n\na tuple (inputs, targets, sample_weights).\n\n\n\n\n\n\nnb_val_samples\n: only relevant if \nvalidation_data\n is a generator.\n    number of samples to use from validation generator\n    at the end of every epoch.\n\n\nclass_weight\n: dictionary mapping class indices to a weight\n    for the class.\n\n\nmax_q_size\n: maximum size for the generator queue\n\n\nnb_worker\n: maximum number of processes to spin up\n\n\npickle_safe\n: if True, use process based threading. Note that because\n    this implementation relies on multiprocessing, you should not pass\n    non picklable arguments to the generator as they can't be passed\n    easily to children processes.\n\n\ninitial_epoch\n: epoch at which to start training\n    (useful for resuming a previous training run)\n\n\n\n\nReturns\n\n\nA \nHistory\n object.\n\n\nExample\n\n\ndef generate_arrays_from_file(path):\n    while 1:\n    f = open(path)\n    for line in f:\n        # create Numpy arrays of input data\n        # and labels, from each line in the file\n        x, y = process_line(line)\n        yield (x, y)\n    f.close()\n\nmodel.fit_generator(generate_arrays_from_file('/my_file.txt'),\n        samples_per_epoch=10000, nb_epoch=10)\n\n\n\n\n\n\nevaluate_generator\n\n\nevaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n\n\n\n\nEvaluates the model on a data generator. The generator should\nreturn the same kind of data as accepted by \ntest_on_batch\n.\n\n\nArguments\n\n\n\n\ngenerator\n:\n    generator yielding tuples (inputs, targets)\n    or (inputs, targets, sample_weights)\n\n\nval_samples\n:\n    total number of samples to generate from \ngenerator\n\n    before returning.\n\n\nmax_q_size\n: maximum size for the generator queue\n\n\nnb_worker\n: maximum number of processes to spin up\n\n\npickle_safe\n: if True, use process based threading. Note that because\n    this implementation relies on multiprocessing, you should not pass non\n    non picklable arguments to the generator as they can't be passed\n    easily to children processes.\n\n\n\n\n\n\npredict_generator\n\n\npredict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n\n\n\n\nGenerates predictions for the input samples from a data generator.\nThe generator should return the same kind of data as accepted by\n\npredict_on_batch\n.\n\n\nArguments\n\n\n\n\ngenerator\n: generator yielding batches of input samples.\n\n\nval_samples\n: total number of samples to generate from \ngenerator\n\n    before returning.\n\n\nmax_q_size\n: maximum size for the generator queue\n\n\nnb_worker\n: maximum number of processes to spin up\n\n\npickle_safe\n: if True, use process based threading. Note that because\n    this implementation relies on multiprocessing, you should not pass non\n    non picklable arguments to the generator as they can't be passed\n    easily to children processes.\n\n\n\n\nReturns\n\n\nA Numpy array of predictions.", 
            "title": "Sequential"
        }, 
        {
            "location": "/models/sequential/#the-sequential-model-api", 
            "text": "To get started, read  this guide to the Keras Sequential model .", 
            "title": "The Sequential model API"
        }, 
        {
            "location": "/models/sequential/#useful-attributes-of-model", 
            "text": "model.layers  is a list of the layers added to the model.", 
            "title": "Useful attributes of Model"
        }, 
        {
            "location": "/models/sequential/#sequential-model-methods", 
            "text": "", 
            "title": "Sequential model methods"
        }, 
        {
            "location": "/models/sequential/#compile", 
            "text": "compile(self, optimizer, loss, metrics=None, sample_weight_mode=None)  Configures the learning process.  Arguments   optimizer : str (name of optimizer) or optimizer object.\n    See  optimizers .  loss : str (name of objective function) or objective function.\n    See  objectives .  metrics : list of metrics to be evaluated by the model\n    during training and testing.\n    Typically you will use  metrics=['accuracy'] .\n    See  metrics .  sample_weight_mode : if you need to do timestep-wise\n    sample weighting (2D weights), set this to \"temporal\".\n    \"None\" defaults to sample-wise weights (1D).  kwargs : for Theano backend, these are passed into K.function.\n    Ignored for Tensorflow backend.   Example      model = Sequential()\n    model.add(Dense(32, input_shape=(500,)))\n    model.add(Dense(10, activation='softmax'))\n    model.compile(optimizer='rmsprop',\n          loss='categorical_crossentropy',\n          metrics=['accuracy'])", 
            "title": "compile"
        }, 
        {
            "location": "/models/sequential/#fit", 
            "text": "fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)  Trains the model for a fixed number of epochs.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).  y : labels, as a Numpy array.  batch_size : integer. Number of samples per gradient update.  nb_epoch : integer, the number of epochs to train the model.  verbose : 0 for no logging to stdout,\n    1 for progress bar logging, 2 for one log line per epoch.  callbacks : list of  keras.callbacks.Callback  instances.\n    List of callbacks to apply during training.\n    See  callbacks .  validation_split : float (0.   x   1).\n    Fraction of the data to use as held-out validation data.  validation_data : tuple (x_val, y_val) or tuple\n    (x_val, y_val, val_sample_weights) to be used as held-out\n    validation data. Will override validation_split.  shuffle : boolean or str (for 'batch').\n    Whether to shuffle the samples at each epoch.\n    'batch' is a special option for dealing with the\n    limitations of HDF5 data; it shuffles in batch-sized chunks.  class_weight : dictionary mapping classes to a weight value,\n    used for scaling the loss function (during training only).  sample_weight : Numpy array of weights for\n    the training samples, used for scaling the loss function\n    (during training only). You can either pass a flat (1D)\n    Numpy array with the same length as the input samples  (1 :1 mapping between weights and samples),\nor in the case of temporal data,\nyou can pass a 2D array with shape (samples, sequence_length),\nto apply a different weight to every timestep of every sample.\nIn this case you should make sure to specify\nsample_weight_mode=\"temporal\" in compile().    initial_epoch : epoch at which to start training\n    (useful for resuming a previous training run)   Returns  A  History  object. Its  History.history  attribute is\na record of training loss values and metrics values\nat successive epochs, as well as validation loss values\nand validation metrics values (if applicable).", 
            "title": "fit"
        }, 
        {
            "location": "/models/sequential/#evaluate", 
            "text": "evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)  Computes the loss on some input data, batch by batch.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).  y : labels, as a Numpy array.  batch_size : integer. Number of samples per gradient update.  verbose : verbosity mode, 0 or 1.  sample_weight : sample weights, as a Numpy array.   Returns  Scalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/sequential/#predict", 
            "text": "predict(self, x, batch_size=32, verbose=0)  Generates output predictions for the input samples,\nprocessing the samples in a batched way.  Arguments   x : the input data, as a Numpy array.  batch_size : integer.  verbose : verbosity mode, 0 or 1.   Returns  A Numpy array of predictions.", 
            "title": "predict"
        }, 
        {
            "location": "/models/sequential/#predict_classes", 
            "text": "predict_classes(self, x, batch_size=32, verbose=1)  Generate class predictions for the input samples\nbatch by batch.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).  batch_size : integer.  verbose : verbosity mode, 0 or 1.   Returns  A numpy array of class predictions.", 
            "title": "predict_classes"
        }, 
        {
            "location": "/models/sequential/#predict_proba", 
            "text": "predict_proba(self, x, batch_size=32, verbose=1)  Generates class probability predictions for the input samples\nbatch by batch.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).  batch_size : integer.  verbose : verbosity mode, 0 or 1.   Returns  A Numpy array of probability predictions.", 
            "title": "predict_proba"
        }, 
        {
            "location": "/models/sequential/#train_on_batch", 
            "text": "train_on_batch(self, x, y, class_weight=None, sample_weight=None)  Single gradient update over one batch of samples.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).  y : labels, as a Numpy array.  class_weight : dictionary mapping classes to a weight value,\n    used for scaling the loss function (during training only).  sample_weight : sample weights, as a Numpy array.   Returns  Scalar training loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.", 
            "title": "train_on_batch"
        }, 
        {
            "location": "/models/sequential/#test_on_batch", 
            "text": "test_on_batch(self, x, y, sample_weight=None)  Evaluates the model over a single batch of samples.  Arguments   x : input data, as a Numpy array or list of Numpy arrays\n    (if the model has multiple inputs).  y : labels, as a Numpy array.  sample_weight : sample weights, as a Numpy array.   Returns  Scalar test loss (if the model has no metrics)\nor list of scalars (if the model computes other metrics).\nThe attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.", 
            "title": "test_on_batch"
        }, 
        {
            "location": "/models/sequential/#predict_on_batch", 
            "text": "predict_on_batch(self, x)  Returns predictions for a single batch of samples.", 
            "title": "predict_on_batch"
        }, 
        {
            "location": "/models/sequential/#fit_generator", 
            "text": "fit_generator(self, generator, samples_per_epoch, nb_epoch, verbose=1, callbacks=None, validation_data=None, nb_val_samples=None, class_weight=None, max_q_size=10, nb_worker=1, pickle_safe=False, initial_epoch=0)  Fits the model on data generated batch-by-batch by\na Python generator.\nThe generator is run in parallel to the model, for efficiency.\nFor instance, this allows you to do real-time data augmentation\non images on CPU in parallel to training your model on GPU.  Arguments   generator : a generator.\n    The output of the generator must be either  a tuple (inputs, targets)  a tuple (inputs, targets, sample_weights).\nAll arrays should contain the same number of samples.\nThe generator is expected to loop over its data\nindefinitely. An epoch finishes when  samples_per_epoch \nsamples have been seen by the model.    samples_per_epoch : integer, number of samples to process before\n    going to the next epoch.  nb_epoch : integer, total number of iterations on the data.  verbose : verbosity mode, 0, 1, or 2.  callbacks : list of callbacks to be called during training.  validation_data : this can be either  a generator for the validation data  a tuple (inputs, targets)  a tuple (inputs, targets, sample_weights).    nb_val_samples : only relevant if  validation_data  is a generator.\n    number of samples to use from validation generator\n    at the end of every epoch.  class_weight : dictionary mapping class indices to a weight\n    for the class.  max_q_size : maximum size for the generator queue  nb_worker : maximum number of processes to spin up  pickle_safe : if True, use process based threading. Note that because\n    this implementation relies on multiprocessing, you should not pass\n    non picklable arguments to the generator as they can't be passed\n    easily to children processes.  initial_epoch : epoch at which to start training\n    (useful for resuming a previous training run)   Returns  A  History  object.  Example  def generate_arrays_from_file(path):\n    while 1:\n    f = open(path)\n    for line in f:\n        # create Numpy arrays of input data\n        # and labels, from each line in the file\n        x, y = process_line(line)\n        yield (x, y)\n    f.close()\n\nmodel.fit_generator(generate_arrays_from_file('/my_file.txt'),\n        samples_per_epoch=10000, nb_epoch=10)", 
            "title": "fit_generator"
        }, 
        {
            "location": "/models/sequential/#evaluate_generator", 
            "text": "evaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)  Evaluates the model on a data generator. The generator should\nreturn the same kind of data as accepted by  test_on_batch .  Arguments   generator :\n    generator yielding tuples (inputs, targets)\n    or (inputs, targets, sample_weights)  val_samples :\n    total number of samples to generate from  generator \n    before returning.  max_q_size : maximum size for the generator queue  nb_worker : maximum number of processes to spin up  pickle_safe : if True, use process based threading. Note that because\n    this implementation relies on multiprocessing, you should not pass non\n    non picklable arguments to the generator as they can't be passed\n    easily to children processes.", 
            "title": "evaluate_generator"
        }, 
        {
            "location": "/models/sequential/#predict_generator", 
            "text": "predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)  Generates predictions for the input samples from a data generator.\nThe generator should return the same kind of data as accepted by predict_on_batch .  Arguments   generator : generator yielding batches of input samples.  val_samples : total number of samples to generate from  generator \n    before returning.  max_q_size : maximum size for the generator queue  nb_worker : maximum number of processes to spin up  pickle_safe : if True, use process based threading. Note that because\n    this implementation relies on multiprocessing, you should not pass non\n    non picklable arguments to the generator as they can't be passed\n    easily to children processes.   Returns  A Numpy array of predictions.", 
            "title": "predict_generator"
        }, 
        {
            "location": "/models/model/", 
            "text": "Model class API\n\n\nIn the functional API, given an input tensor and output tensor, you can instantiate a \nModel\n via:\n\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\n\na = Input(shape=(32,))\nb = Dense(32)(a)\nmodel = Model(input=a, output=b)\n\n\n\n\nThis model will include all layers required in the computation of \nb\n given \na\n.\n\n\nIn the case of multi-input or multi-output models, you can use lists as well:\n\n\nmodel = Model(input=[a1, a2], output=[b1, b3, b3])\n\n\n\n\nFor a detailed introduction of what \nModel\n can do, read \nthis guide to the Keras functional API\n.\n\n\nUseful attributes of Model\n\n\n\n\nmodel.layers\n is a flattened list of the layers comprising the model graph.\n\n\nmodel.inputs\n is the list of input tensors.\n\n\nmodel.outputs\n is the list of output tensors.\n\n\n\n\nMethods\n\n\ncompile\n\n\ncompile(self, optimizer, loss, metrics=None, loss_weights=None, sample_weight_mode=None)\n\n\n\n\nConfigures the model for training.\n\n\nArguments\n\n\n\n\noptimizer\n: str (name of optimizer) or optimizer object.\n    See \noptimizers\n.\n\n\nloss\n: str (name of objective function) or objective function.\n    See \nobjectives\n.\n    If the model has multiple outputs, you can use a different loss\n    on each output by passing a dictionary or a list of objectives.\n\n\nmetrics\n: list of metrics to be evaluated by the model\n    during training and testing.\n    Typically you will use \nmetrics=['accuracy']\n.\n    To specify different metrics for different outputs of a\n    multi-output model, you could also pass a dictionary,\n    such as \nmetrics={'output_a': 'accuracy'}\n.\n\n\nsample_weight_mode\n: if you need to do timestep-wise\n    sample weighting (2D weights), set this to \"temporal\".\n    \"None\" defaults to sample-wise weights (1D).\n    If the model has multiple outputs, you can use a different\n    \nsample_weight_mode\n on each output by passing a\n    dictionary or a list of modes.\n\n\nkwargs\n: when using the Theano backend, these arguments\n    are passed into K.function. Ignored for Tensorflow backend.\n\n\n\n\n\n\nfit\n\n\nfit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)\n\n\n\n\nTrains the model for a fixed number of epochs (iterations on a dataset).\n\n\nArguments\n\n\n\n\nx\n: Numpy array of training data,\n    or list of Numpy arrays if the model has multiple inputs.\n    If all inputs in the model are named,\n    you can also pass a dictionary\n    mapping input names to Numpy arrays.\n\n\ny\n: Numpy array of target data,\n    or list of Numpy arrays if the model has multiple outputs.\n    If all outputs in the model are named,\n    you can also pass a dictionary\n    mapping output names to Numpy arrays.\n\n\nbatch_size\n: integer. Number of samples per gradient update.\n\n\nnb_epoch\n: integer, the number of times to iterate\n    over the training data arrays.\n\n\nverbose\n: 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = verbose, 2 = one log line per epoch.\n\n\n\n\n\n\ncallbacks\n: list of callbacks to be called during training.\n    See \ncallbacks\n.\n\n\nvalidation_split\n: float between 0 and 1:\n    fraction of the training data to be used as validation data.\n    The model will set apart this fraction of the training data,\n    will not train on it, and will evaluate\n    the loss and any model metrics\n    on this data at the end of each epoch.\n\n\nvalidation_data\n: data on which to evaluate\n    the loss and any model metrics\n    at the end of each epoch. The model will not\n    be trained on this data.\n    This could be a tuple (x_val, y_val)\n    or a tuple (x_val, y_val, val_sample_weights).\n\n\nshuffle\n: boolean, whether to shuffle the training data\n    before each epoch.\n\n\nclass_weight\n: optional dictionary mapping\n    class indices (integers) to\n    a weight (float) to apply to the model's loss for the samples\n    from this class during training.\n    This can be useful to tell the model to \"pay more attention\" to\n    samples from an under-represented class.\n\n\nsample_weight\n: optional array of the same length as x, containing\n    weights to apply to the model's loss for each sample.\n    In the case of temporal data, you can pass a 2D array\n    with shape (samples, sequence_length),\n    to apply a different weight to every timestep of every sample.\n    In this case you should make sure to specify\n    sample_weight_mode=\"temporal\" in compile().\n\n\ninitial_epoch\n: epoch at which to start training\n    (useful for resuming a previous training run)\n\n\n\n\nReturns\n\n\nA \nHistory\n instance. Its \nhistory\n attribute contains\nall information collected during training.\n\n\n\n\nevaluate\n\n\nevaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)\n\n\n\n\nReturns the loss value and metrics values for the model\nin test mode. Computation is done in batches.\n\n\nArguments\n\n\n\n\nx\n: Numpy array of test data,\n    or list of Numpy arrays if the model has multiple inputs.\n    If all inputs in the model are named,\n    you can also pass a dictionary\n    mapping input names to Numpy arrays.\n\n\ny\n: Numpy array of target data,\n    or list of Numpy arrays if the model has multiple outputs.\n    If all outputs in the model are named,\n    you can also pass a dictionary\n    mapping output names to Numpy arrays.\n\n\nbatch_size\n: integer. Number of samples per gradient update.\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\npredict\n\n\npredict(self, x, batch_size=32, verbose=0)\n\n\n\n\nGenerates output predictions for the input samples,\nprocessing the samples in a batched way.\n\n\nArguments\n\n\n\n\nx\n: the input data, as a Numpy array\n    (or list of Numpy arrays if the model has multiple outputs).\n\n\nbatch_size\n: integer.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\n\n\nReturns\n\n\nA Numpy array of predictions.\n\n\n\n\ntrain_on_batch\n\n\ntrain_on_batch(self, x, y, sample_weight=None, class_weight=None)\n\n\n\n\nRuns a single gradient update on a single batch of data.\n\n\nArguments\n\n\n\n\nx\n: Numpy array of training data,\n    or list of Numpy arrays if the model has multiple inputs.\n    If all inputs in the model are named,\n    you can also pass a dictionary\n    mapping input names to Numpy arrays.\n\n\ny\n: Numpy array of target data,\n    or list of Numpy arrays if the model has multiple outputs.\n    If all outputs in the model are named,\n    you can also pass a dictionary\n    mapping output names to Numpy arrays.\n\n\nsample_weight\n: optional array of the same length as x, containing\n    weights to apply to the model's loss for each sample.\n    In the case of temporal data, you can pass a 2D array\n    with shape (samples, sequence_length),\n    to apply a different weight to every timestep of every sample.\n    In this case you should make sure to specify\n    sample_weight_mode=\"temporal\" in compile().\n\n\nclass_weight\n: optional dictionary mapping\n    lass indices (integers) to\n    a weight (float) to apply to the model's loss for the samples\n    from this class during training.\n    This can be useful to tell the model to \"pay more attention\" to\n    samples from an under-represented class.\n\n\n\n\nReturns\n\n\nScalar training loss\n(if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\ntest_on_batch\n\n\ntest_on_batch(self, x, y, sample_weight=None)\n\n\n\n\nTest the model on a single batch of samples.\n\n\nArguments\n\n\n\n\nx\n: Numpy array of test data,\n    or list of Numpy arrays if the model has multiple inputs.\n    If all inputs in the model are named,\n    you can also pass a dictionary\n    mapping input names to Numpy arrays.\n\n\ny\n: Numpy array of target data,\n    or list of Numpy arrays if the model has multiple outputs.\n    If all outputs in the model are named,\n    you can also pass a dictionary\n    mapping output names to Numpy arrays.\n\n\nsample_weight\n: optional array of the same length as x, containing\n    weights to apply to the model's loss for each sample.\n    In the case of temporal data, you can pass a 2D array\n    with shape (samples, sequence_length),\n    to apply a different weight to every timestep of every sample.\n    In this case you should make sure to specify\n    sample_weight_mode=\"temporal\" in compile().\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\npredict_on_batch\n\n\npredict_on_batch(self, x)\n\n\n\n\nReturns predictions for a single batch of samples.\n\n\n\n\nfit_generator\n\n\nfit_generator(self, generator, samples_per_epoch, nb_epoch, verbose=1, callbacks=None, validation_data=None, nb_val_samples=None, class_weight=None, max_q_size=10, nb_worker=1, pickle_safe=False, initial_epoch=0)\n\n\n\n\nFits the model on data generated batch-by-batch by\na Python generator.\nThe generator is run in parallel to the model, for efficiency.\nFor instance, this allows you to do real-time data augmentation\non images on CPU in parallel to training your model on GPU.\n\n\nArguments\n\n\n\n\ngenerator\n: a generator.\n    The output of the generator must be either\n\n\na tuple (inputs, targets)\n\n\na tuple (inputs, targets, sample_weights).\nAll arrays should contain the same number of samples.\nThe generator is expected to loop over its data\nindefinitely. An epoch finishes when \nsamples_per_epoch\n\nsamples have been seen by the model.\n\n\n\n\n\n\nsamples_per_epoch\n: integer, number of samples to process before\n    going to the next epoch.\n\n\nnb_epoch\n: integer, total number of iterations on the data.\n\n\nverbose\n: verbosity mode, 0, 1, or 2.\n\n\ncallbacks\n: list of callbacks to be called during training.\n\n\nvalidation_data\n: this can be either\n\n\na generator for the validation data\n\n\na tuple (inputs, targets)\n\n\na tuple (inputs, targets, sample_weights).\n\n\n\n\n\n\nnb_val_samples\n: only relevant if \nvalidation_data\n is a generator.\n    number of samples to use from validation generator\n    at the end of every epoch.\n\n\nclass_weight\n: dictionary mapping class indices to a weight\n    for the class.\n\n\nmax_q_size\n: maximum size for the generator queue\n\n\nnb_worker\n: maximum number of processes to spin up\n    when using process based threading\n\n\npickle_safe\n: if True, use process based threading.\n    Note that because\n    this implementation relies on multiprocessing,\n    you should not pass\n    non picklable arguments to the generator\n    as they can't be passed\n    easily to children processes.\n\n\ninitial_epoch\n: epoch at which to start training\n    (useful for resuming a previous training run)\n\n\n\n\nReturns\n\n\nA \nHistory\n object.\n\n\nExample\n\n\ndef generate_arrays_from_file(path):\n    while 1:\n    f = open(path)\n    for line in f:\n        # create numpy arrays of input data\n        # and labels, from each line in the file\n        x1, x2, y = process_line(line)\n        yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n    f.close()\n\nmodel.fit_generator(generate_arrays_from_file('/my_file.txt'),\n        samples_per_epoch=10000, nb_epoch=10)\n\n\n\n\n\n\nevaluate_generator\n\n\nevaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n\n\n\n\nEvaluates the model on a data generator. The generator should\nreturn the same kind of data as accepted by \ntest_on_batch\n.\n\n\n\n\nArguments\n:\n\n\ngenerator\n:\n    generator yielding tuples (inputs, targets)\n    or (inputs, targets, sample_weights)\n\n\nval_samples\n:\n    total number of samples to generate from \ngenerator\n\n    before returning.\n\n\nmax_q_size\n: maximum size for the generator queue\n\n\nnb_worker\n: maximum number of processes to spin up\n    when using process based threading\n\n\npickle_safe\n: if True, use process based threading.\n    Note that because\n    this implementation relies on multiprocessing,\n    you should not pass\n    non picklable arguments to the generator\n    as they can't be passed\n    easily to children processes.\n\n\n\n\nReturns\n\n\nScalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute \nmodel.metrics_names\n will give you\nthe display labels for the scalar outputs.\n\n\n\n\npredict_generator\n\n\npredict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)\n\n\n\n\nGenerates predictions for the input samples from a data generator.\nThe generator should return the same kind of data as accepted by\n\npredict_on_batch\n.\n\n\nArguments\n\n\n\n\ngenerator\n: generator yielding batches of input samples.\n\n\nval_samples\n: total number of samples to generate from \ngenerator\n\n    before returning.\n\n\nmax_q_size\n: maximum size for the generator queue\n\n\nnb_worker\n: maximum number of processes to spin up\n    when using process based threading\n\n\npickle_safe\n: if True, use process based threading.\n    Note that because\n    this implementation relies on multiprocessing,\n    you should not pass\n    non picklable arguments to the generator\n    as they can't be passed\n    easily to children processes.\n\n\n\n\nReturns\n\n\nNumpy array(s) of predictions.\n\n\n\n\nget_layer\n\n\nget_layer(self, name=None, index=None)\n\n\n\n\nReturns a layer based on either its name (unique)\nor its index in the graph. Indices are based on\norder of horizontal graph traversal (bottom-up).\n\n\nArguments\n\n\n\n\nname\n: String, name of layer.\n\n\nindex\n: Integer, index of layer.\n\n\n\n\nReturns\n\n\nA layer instance.", 
            "title": "Model (functional API)"
        }, 
        {
            "location": "/models/model/#model-class-api", 
            "text": "In the functional API, given an input tensor and output tensor, you can instantiate a  Model  via:  from keras.models import Model\nfrom keras.layers import Input, Dense\n\na = Input(shape=(32,))\nb = Dense(32)(a)\nmodel = Model(input=a, output=b)  This model will include all layers required in the computation of  b  given  a .  In the case of multi-input or multi-output models, you can use lists as well:  model = Model(input=[a1, a2], output=[b1, b3, b3])  For a detailed introduction of what  Model  can do, read  this guide to the Keras functional API .", 
            "title": "Model class API"
        }, 
        {
            "location": "/models/model/#useful-attributes-of-model", 
            "text": "model.layers  is a flattened list of the layers comprising the model graph.  model.inputs  is the list of input tensors.  model.outputs  is the list of output tensors.", 
            "title": "Useful attributes of Model"
        }, 
        {
            "location": "/models/model/#methods", 
            "text": "", 
            "title": "Methods"
        }, 
        {
            "location": "/models/model/#compile", 
            "text": "compile(self, optimizer, loss, metrics=None, loss_weights=None, sample_weight_mode=None)  Configures the model for training.  Arguments   optimizer : str (name of optimizer) or optimizer object.\n    See  optimizers .  loss : str (name of objective function) or objective function.\n    See  objectives .\n    If the model has multiple outputs, you can use a different loss\n    on each output by passing a dictionary or a list of objectives.  metrics : list of metrics to be evaluated by the model\n    during training and testing.\n    Typically you will use  metrics=['accuracy'] .\n    To specify different metrics for different outputs of a\n    multi-output model, you could also pass a dictionary,\n    such as  metrics={'output_a': 'accuracy'} .  sample_weight_mode : if you need to do timestep-wise\n    sample weighting (2D weights), set this to \"temporal\".\n    \"None\" defaults to sample-wise weights (1D).\n    If the model has multiple outputs, you can use a different\n     sample_weight_mode  on each output by passing a\n    dictionary or a list of modes.  kwargs : when using the Theano backend, these arguments\n    are passed into K.function. Ignored for Tensorflow backend.", 
            "title": "compile"
        }, 
        {
            "location": "/models/model/#fit", 
            "text": "fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0)  Trains the model for a fixed number of epochs (iterations on a dataset).  Arguments   x : Numpy array of training data,\n    or list of Numpy arrays if the model has multiple inputs.\n    If all inputs in the model are named,\n    you can also pass a dictionary\n    mapping input names to Numpy arrays.  y : Numpy array of target data,\n    or list of Numpy arrays if the model has multiple outputs.\n    If all outputs in the model are named,\n    you can also pass a dictionary\n    mapping output names to Numpy arrays.  batch_size : integer. Number of samples per gradient update.  nb_epoch : integer, the number of times to iterate\n    over the training data arrays.  verbose : 0, 1, or 2. Verbosity mode.\n0 = silent, 1 = verbose, 2 = one log line per epoch.    callbacks : list of callbacks to be called during training.\n    See  callbacks .  validation_split : float between 0 and 1:\n    fraction of the training data to be used as validation data.\n    The model will set apart this fraction of the training data,\n    will not train on it, and will evaluate\n    the loss and any model metrics\n    on this data at the end of each epoch.  validation_data : data on which to evaluate\n    the loss and any model metrics\n    at the end of each epoch. The model will not\n    be trained on this data.\n    This could be a tuple (x_val, y_val)\n    or a tuple (x_val, y_val, val_sample_weights).  shuffle : boolean, whether to shuffle the training data\n    before each epoch.  class_weight : optional dictionary mapping\n    class indices (integers) to\n    a weight (float) to apply to the model's loss for the samples\n    from this class during training.\n    This can be useful to tell the model to \"pay more attention\" to\n    samples from an under-represented class.  sample_weight : optional array of the same length as x, containing\n    weights to apply to the model's loss for each sample.\n    In the case of temporal data, you can pass a 2D array\n    with shape (samples, sequence_length),\n    to apply a different weight to every timestep of every sample.\n    In this case you should make sure to specify\n    sample_weight_mode=\"temporal\" in compile().  initial_epoch : epoch at which to start training\n    (useful for resuming a previous training run)   Returns  A  History  instance. Its  history  attribute contains\nall information collected during training.", 
            "title": "fit"
        }, 
        {
            "location": "/models/model/#evaluate", 
            "text": "evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None)  Returns the loss value and metrics values for the model\nin test mode. Computation is done in batches.  Arguments   x : Numpy array of test data,\n    or list of Numpy arrays if the model has multiple inputs.\n    If all inputs in the model are named,\n    you can also pass a dictionary\n    mapping input names to Numpy arrays.  y : Numpy array of target data,\n    or list of Numpy arrays if the model has multiple outputs.\n    If all outputs in the model are named,\n    you can also pass a dictionary\n    mapping output names to Numpy arrays.  batch_size : integer. Number of samples per gradient update.   Returns  Scalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.", 
            "title": "evaluate"
        }, 
        {
            "location": "/models/model/#predict", 
            "text": "predict(self, x, batch_size=32, verbose=0)  Generates output predictions for the input samples,\nprocessing the samples in a batched way.  Arguments   x : the input data, as a Numpy array\n    (or list of Numpy arrays if the model has multiple outputs).  batch_size : integer.  verbose : verbosity mode, 0 or 1.   Returns  A Numpy array of predictions.", 
            "title": "predict"
        }, 
        {
            "location": "/models/model/#train_on_batch", 
            "text": "train_on_batch(self, x, y, sample_weight=None, class_weight=None)  Runs a single gradient update on a single batch of data.  Arguments   x : Numpy array of training data,\n    or list of Numpy arrays if the model has multiple inputs.\n    If all inputs in the model are named,\n    you can also pass a dictionary\n    mapping input names to Numpy arrays.  y : Numpy array of target data,\n    or list of Numpy arrays if the model has multiple outputs.\n    If all outputs in the model are named,\n    you can also pass a dictionary\n    mapping output names to Numpy arrays.  sample_weight : optional array of the same length as x, containing\n    weights to apply to the model's loss for each sample.\n    In the case of temporal data, you can pass a 2D array\n    with shape (samples, sequence_length),\n    to apply a different weight to every timestep of every sample.\n    In this case you should make sure to specify\n    sample_weight_mode=\"temporal\" in compile().  class_weight : optional dictionary mapping\n    lass indices (integers) to\n    a weight (float) to apply to the model's loss for the samples\n    from this class during training.\n    This can be useful to tell the model to \"pay more attention\" to\n    samples from an under-represented class.   Returns  Scalar training loss\n(if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.", 
            "title": "train_on_batch"
        }, 
        {
            "location": "/models/model/#test_on_batch", 
            "text": "test_on_batch(self, x, y, sample_weight=None)  Test the model on a single batch of samples.  Arguments   x : Numpy array of test data,\n    or list of Numpy arrays if the model has multiple inputs.\n    If all inputs in the model are named,\n    you can also pass a dictionary\n    mapping input names to Numpy arrays.  y : Numpy array of target data,\n    or list of Numpy arrays if the model has multiple outputs.\n    If all outputs in the model are named,\n    you can also pass a dictionary\n    mapping output names to Numpy arrays.  sample_weight : optional array of the same length as x, containing\n    weights to apply to the model's loss for each sample.\n    In the case of temporal data, you can pass a 2D array\n    with shape (samples, sequence_length),\n    to apply a different weight to every timestep of every sample.\n    In this case you should make sure to specify\n    sample_weight_mode=\"temporal\" in compile().   Returns  Scalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.", 
            "title": "test_on_batch"
        }, 
        {
            "location": "/models/model/#predict_on_batch", 
            "text": "predict_on_batch(self, x)  Returns predictions for a single batch of samples.", 
            "title": "predict_on_batch"
        }, 
        {
            "location": "/models/model/#fit_generator", 
            "text": "fit_generator(self, generator, samples_per_epoch, nb_epoch, verbose=1, callbacks=None, validation_data=None, nb_val_samples=None, class_weight=None, max_q_size=10, nb_worker=1, pickle_safe=False, initial_epoch=0)  Fits the model on data generated batch-by-batch by\na Python generator.\nThe generator is run in parallel to the model, for efficiency.\nFor instance, this allows you to do real-time data augmentation\non images on CPU in parallel to training your model on GPU.  Arguments   generator : a generator.\n    The output of the generator must be either  a tuple (inputs, targets)  a tuple (inputs, targets, sample_weights).\nAll arrays should contain the same number of samples.\nThe generator is expected to loop over its data\nindefinitely. An epoch finishes when  samples_per_epoch \nsamples have been seen by the model.    samples_per_epoch : integer, number of samples to process before\n    going to the next epoch.  nb_epoch : integer, total number of iterations on the data.  verbose : verbosity mode, 0, 1, or 2.  callbacks : list of callbacks to be called during training.  validation_data : this can be either  a generator for the validation data  a tuple (inputs, targets)  a tuple (inputs, targets, sample_weights).    nb_val_samples : only relevant if  validation_data  is a generator.\n    number of samples to use from validation generator\n    at the end of every epoch.  class_weight : dictionary mapping class indices to a weight\n    for the class.  max_q_size : maximum size for the generator queue  nb_worker : maximum number of processes to spin up\n    when using process based threading  pickle_safe : if True, use process based threading.\n    Note that because\n    this implementation relies on multiprocessing,\n    you should not pass\n    non picklable arguments to the generator\n    as they can't be passed\n    easily to children processes.  initial_epoch : epoch at which to start training\n    (useful for resuming a previous training run)   Returns  A  History  object.  Example  def generate_arrays_from_file(path):\n    while 1:\n    f = open(path)\n    for line in f:\n        # create numpy arrays of input data\n        # and labels, from each line in the file\n        x1, x2, y = process_line(line)\n        yield ({'input_1': x1, 'input_2': x2}, {'output': y})\n    f.close()\n\nmodel.fit_generator(generate_arrays_from_file('/my_file.txt'),\n        samples_per_epoch=10000, nb_epoch=10)", 
            "title": "fit_generator"
        }, 
        {
            "location": "/models/model/#evaluate_generator", 
            "text": "evaluate_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)  Evaluates the model on a data generator. The generator should\nreturn the same kind of data as accepted by  test_on_batch .   Arguments :  generator :\n    generator yielding tuples (inputs, targets)\n    or (inputs, targets, sample_weights)  val_samples :\n    total number of samples to generate from  generator \n    before returning.  max_q_size : maximum size for the generator queue  nb_worker : maximum number of processes to spin up\n    when using process based threading  pickle_safe : if True, use process based threading.\n    Note that because\n    this implementation relies on multiprocessing,\n    you should not pass\n    non picklable arguments to the generator\n    as they can't be passed\n    easily to children processes.   Returns  Scalar test loss (if the model has a single output and no metrics)\nor list of scalars (if the model has multiple outputs\nand/or metrics). The attribute  model.metrics_names  will give you\nthe display labels for the scalar outputs.", 
            "title": "evaluate_generator"
        }, 
        {
            "location": "/models/model/#predict_generator", 
            "text": "predict_generator(self, generator, val_samples, max_q_size=10, nb_worker=1, pickle_safe=False)  Generates predictions for the input samples from a data generator.\nThe generator should return the same kind of data as accepted by predict_on_batch .  Arguments   generator : generator yielding batches of input samples.  val_samples : total number of samples to generate from  generator \n    before returning.  max_q_size : maximum size for the generator queue  nb_worker : maximum number of processes to spin up\n    when using process based threading  pickle_safe : if True, use process based threading.\n    Note that because\n    this implementation relies on multiprocessing,\n    you should not pass\n    non picklable arguments to the generator\n    as they can't be passed\n    easily to children processes.   Returns  Numpy array(s) of predictions.", 
            "title": "predict_generator"
        }, 
        {
            "location": "/models/model/#get_layer", 
            "text": "get_layer(self, name=None, index=None)  Returns a layer based on either its name (unique)\nor its index in the graph. Indices are based on\norder of horizontal graph traversal (bottom-up).  Arguments   name : String, name of layer.  index : Integer, index of layer.   Returns  A layer instance.", 
            "title": "get_layer"
        }, 
        {
            "location": "/layers/about-keras-layers/", 
            "text": "About Keras layers\n\n\nAll Keras layers have a number of methods in common:\n\n\n\n\nlayer.get_weights()\n: returns the weights of the layer as a list of Numpy arrays.\n\n\nlayer.set_weights(weights)\n: sets the weights of the layer from a list of Numpy arrays (with the same shapes as the output of \nget_weights\n).\n\n\nlayer.get_config()\n: returns a dictionary containing the configuration of the layer. The layer can be reinstantiated from its config via:\n\n\n\n\nfrom keras.utils.layer_utils import layer_from_config\n\nconfig = layer.get_config()\nlayer = layer_from_config(config)\n\n\n\n\nIf a layer has a single node (i.e. if it isn't a shared layer), you can get its input tensor, output tensor, input shape and output shape via:\n\n\n\n\nlayer.input\n\n\nlayer.output\n\n\nlayer.input_shape\n\n\nlayer.output_shape\n\n\n\n\nIf the layer has multiple nodes (see: \nthe concept of layer node and shared layers\n), you can use the following methods:\n\n\n\n\nlayer.get_input_at(node_index)\n\n\nlayer.get_output_at(node_index)\n\n\nlayer.get_input_shape_at(node_index)\n\n\nlayer.get_output_shape_at(node_index)", 
            "title": "About Keras layers"
        }, 
        {
            "location": "/layers/about-keras-layers/#about-keras-layers", 
            "text": "All Keras layers have a number of methods in common:   layer.get_weights() : returns the weights of the layer as a list of Numpy arrays.  layer.set_weights(weights) : sets the weights of the layer from a list of Numpy arrays (with the same shapes as the output of  get_weights ).  layer.get_config() : returns a dictionary containing the configuration of the layer. The layer can be reinstantiated from its config via:   from keras.utils.layer_utils import layer_from_config\n\nconfig = layer.get_config()\nlayer = layer_from_config(config)  If a layer has a single node (i.e. if it isn't a shared layer), you can get its input tensor, output tensor, input shape and output shape via:   layer.input  layer.output  layer.input_shape  layer.output_shape   If the layer has multiple nodes (see:  the concept of layer node and shared layers ), you can use the following methods:   layer.get_input_at(node_index)  layer.get_output_at(node_index)  layer.get_input_shape_at(node_index)  layer.get_output_shape_at(node_index)", 
            "title": "About Keras layers"
        }, 
        {
            "location": "/layers/core/", 
            "text": "[source]\n\n\nDense\n\n\nkeras.layers.core.Dense(output_dim, init='glorot_uniform', activation=None, weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)\n\n\n\n\nJust your regular densely-connected NN layer.\n\n\nExample\n\n\n# as first layer in a sequential model:\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=16))\n# now the model will take as input arrays of shape (*, 16)\n# and output arrays of shape (*, 32)\n\n# this is equivalent to the above:\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(16,)))\n\n# after the first layer, you don't need to specify\n# the size of the input anymore:\nmodel.add(Dense(32))\n\n\n\n\nArguments\n\n\n\n\noutput_dim\n: int \n 0.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n),\n    or alternatively, Theano function to use for weights\n    initialization. This parameter is only relevant\n    if you don't pass a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of Numpy arrays to set as initial weights.\n    The list should have 2 elements, of shape \n(input_dim, output_dim)\n\n    and (output_dim,) for weights and biases respectively.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\ninput_dim\n: dimensionality of the input (integer). This argument\n    (or alternatively, the keyword argument \ninput_shape\n)\n    is required when using this layer as the first layer in a model.\n\n\n\n\nInput shape\n\n\nnD tensor with shape: \n(nb_samples, ..., input_dim)\n.\nThe most common situation would be\na 2D input with shape \n(nb_samples, input_dim)\n.\n\n\nOutput shape\n\n\nnD tensor with shape: \n(nb_samples, ..., output_dim)\n.\nFor instance, for a 2D input with shape \n(nb_samples, input_dim)\n,\nthe output would have shape \n(nb_samples, output_dim)\n.\n\n\n\n\n[source]\n\n\nActivation\n\n\nkeras.layers.core.Activation(activation)\n\n\n\n\nApplies an activation function to an output.\n\n\nArguments\n\n\n\n\nactivation\n: name of activation function to use\n\n\n(see\n: \nactivations\n),\nor alternatively, a Theano or TensorFlow operation.\n\n\n\n\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\n\n\n[source]\n\n\nDropout\n\n\nkeras.layers.core.Dropout(p, noise_shape=None, seed=None)\n\n\n\n\nApplies Dropout to the input.\n\n\nDropout consists in randomly setting\na fraction \np\n of input units to 0 at each update during training time,\nwhich helps prevent overfitting.\n\n\nArguments\n\n\n\n\np\n: float between 0 and 1. Fraction of the input units to drop.\n\n\nnoise_shape\n: 1D integer tensor representing the shape of the\n    binary dropout mask that will be multiplied with the input.\n    For instance, if your inputs ahve shape\n    \n(batch_size, timesteps, features)\n and\n    you want the dropout mask to be the same for all timesteps,\n    you can use \nnoise_shape=(batch_size, 1, features)\n.\n\n\nseed\n: A Python integer to use as random seed.\n\n\n\n\nReferences\n\n\n\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting\n\n\n\n\n\n\n[source]\n\n\nSpatialDropout2D\n\n\nkeras.layers.core.SpatialDropout2D(p, dim_ordering='default')\n\n\n\n\nSpatial 2D version of Dropout.\n\n\nThis version performs the same function as Dropout, however it drops\nentire 2D feature maps instead of individual elements. If adjacent pixels\nwithin feature maps are strongly correlated (as is normally the case in\nearly convolution layers) then regular dropout will not regularize the\nactivations and will otherwise just result in an effective learning rate\ndecrease. In this case, SpatialDropout2D will help promote independence\nbetween feature maps and should be used instead.\n\n\nArguments\n\n\n\n\np\n: float between 0 and 1. Fraction of the input units to drop.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\nSame as input\n\n\nReferences\n\n\n\n\nEfficient Object Localization Using Convolutional Networks\n\n\n\n\n\n\n[source]\n\n\nSpatialDropout3D\n\n\nkeras.layers.core.SpatialDropout3D(p, dim_ordering='default')\n\n\n\n\nSpatial 3D version of Dropout.\n\n\nThis version performs the same function as Dropout, however it drops\nentire 3D feature maps instead of individual elements. If adjacent voxels\nwithin feature maps are strongly correlated (as is normally the case in\nearly convolution layers) then regular dropout will not regularize the\nactivations and will otherwise just result in an effective learning rate\ndecrease. In this case, SpatialDropout3D will help promote independence\nbetween feature maps and should be used instead.\n\n\nArguments\n\n\n\n\np\n: float between 0 and 1. Fraction of the input units to drop.\n\n\ndim_ordering\n: 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n\n(samples, channels, dim1, dim2, dim3)\n if dim_ordering='th'\nor 5D tensor with shape:\n\n(samples, dim1, dim2, dim3, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\nSame as input\n\n\nReferences\n\n\n\n\nEfficient Object Localization Using Convolutional Networks\n\n\n\n\n\n\n[source]\n\n\nFlatten\n\n\nkeras.layers.core.Flatten()\n\n\n\n\nFlattens the input. Does not affect the batch size.\n\n\nExample\n\n\nmodel = Sequential()\nmodel.add(Convolution2D(64, 3, 3,\n            border_mode='same',\n            input_shape=(3, 32, 32)))\n# now: model.output_shape == (None, 64, 32, 32)\n\nmodel.add(Flatten())\n# now: model.output_shape == (None, 65536)\n\n\n\n\n\n\n[source]\n\n\nReshape\n\n\nkeras.layers.core.Reshape(target_shape)\n\n\n\n\nReshapes an output to a certain shape.\n\n\nArguments\n\n\n\n\ntarget_shape\n: target shape. Tuple of integers,\n    does not include the samples dimension (batch size).\n\n\n\n\nInput shape\n\n\nArbitrary, although all dimensions in the input shaped must be fixed.\nUse the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\n(batch_size,) + target_shape\n\n\nExample\n\n\n# as first layer in a Sequential model\nmodel = Sequential()\nmodel.add(Reshape((3, 4), input_shape=(12,)))\n# now: model.output_shape == (None, 3, 4)\n# note: `None` is the batch dimension\n\n# as intermediate layer in a Sequential model\nmodel.add(Reshape((6, 2)))\n# now: model.output_shape == (None, 6, 2)\n\n# also supports shape inference using `-1` as dimension\nmodel.add(Reshape((-1, 2, 2)))\n# now: model.output_shape == (None, 3, 2, 2)\n\n\n\n\n\n\n[source]\n\n\nPermute\n\n\nkeras.layers.core.Permute(dims)\n\n\n\n\nPermutes the dimensions of the input according to a given pattern.\n\n\nUseful for e.g. connecting RNNs and convnets together.\n\n\nExample\n\n\nmodel = Sequential()\nmodel.add(Permute((2, 1), input_shape=(10, 64)))\n# now: model.output_shape == (None, 64, 10)\n# note: `None` is the batch dimension\n\n\n\n\nArguments\n\n\n\n\ndims\n: Tuple of integers. Permutation pattern, does not include the\n    samples dimension. Indexing starts at 1.\n    For instance, \n(2, 1)\n permutes the first and second dimension\n    of the input.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame as the input shape, but with the dimensions re-ordered according\nto the specified pattern.\n\n\n\n\n[source]\n\n\nRepeatVector\n\n\nkeras.layers.core.RepeatVector(n)\n\n\n\n\nRepeats the input n times.\n\n\nExample\n\n\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=32))\n# now: model.output_shape == (None, 32)\n# note: `None` is the batch dimension\n\nmodel.add(RepeatVector(3))\n# now: model.output_shape == (None, 3, 32)\n\n\n\n\nArguments\n\n\n\n\nn\n: integer, repetition factor.\n\n\n\n\nInput shape\n\n\n2D tensor of shape \n(nb_samples, features)\n.\n\n\nOutput shape\n\n\n3D tensor of shape \n(nb_samples, n, features)\n.\n\n\n\n\n[source]\n\n\nMerge\n\n\nkeras.engine.topology.Merge(layers=None, mode='sum', concat_axis=-1, dot_axes=-1, output_shape=None, output_mask=None, arguments=None, node_indices=None, tensor_indices=None, name=None)\n\n\n\n\nA \nMerge\n layer can be used to merge a list of tensors\ninto a single tensor, following some merge \nmode\n.\n\n\nExample\n\n\nmodel1 = Sequential()\nmodel1.add(Dense(32, input_dim=32))\n\nmodel2 = Sequential()\nmodel2.add(Dense(32, input_dim=32))\n\nmerged_model = Sequential()\nmerged_model.add(Merge([model1, model2], mode='concat', concat_axis=1))\n\n\n\n\nArguments\n\n\n\n\nlayers\n: Can be a list of Keras tensors or\n    a list of layer instances. Must be more\n    than one layer/tensor.\n\n\nmode\n: String or lambda/function. If string, must be one\n\n\nof\n: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'.\nIf lambda/function, it should take as input a list of tensors\nand return a single tensor.\n\n\n\n\n\n\nconcat_axis\n: Integer, axis to use in mode \nconcat\n.\n\n\ndot_axes\n: Integer or tuple of integers,\n    axes to use in mode \ndot\n or \ncos\n.\n\n\noutput_shape\n: Either a shape tuple (tuple of integers),\n    or a lambda/function\n    to compute \noutput_shape\n\n    (only if merge mode is a lambda/function).\n    If the argument is a tuple,\n    it should be expected output shape, \nnot\n including the batch size\n    (same convention as the \ninput_shape\n argument in layers).\n    If the argument is callable,\n    it should take as input a list of shape tuples\n\n\n(1\n:1 mapping to input tensors)\nand return a single shape tuple, including the\nbatch size (same convention as the\n\nget_output_shape_for\n method of layers).\n\n\n\n\n\n\nnode_indices\n: Optional list of integers containing\n    the output node index for each input layer\n    (in case some input layers have multiple output nodes).\n    will default to an array of 0s if not provided.\n\n\ntensor_indices\n: Optional list of indices of output tensors\n    to consider for merging\n    (in case some input layer node returns multiple tensors).\n\n\noutput_mask\n: Mask or lambda/function to compute the output mask (only\n    if merge mode is a lambda/function). If the latter case, it should\n    take as input a list of masks and return a single mask.\n\n\n\n\n\n\n[source]\n\n\nLambda\n\n\nkeras.layers.core.Lambda(function, output_shape=None, arguments=None)\n\n\n\n\nUsed for evaluating an arbitrary expressions on an input.\n\n\nExamples\n\n\n# add a x -\n x^2 layer\nmodel.add(Lambda(lambda x: x ** 2))\n\n\n\n\n# add a layer that returns the concatenation\n# of the positive part of the input and\n# the opposite of the negative part\n\ndef antirectifier(x):\n    x -= K.mean(x, axis=1, keepdims=True)\n    x = K.l2_normalize(x, axis=1)\n    pos = K.relu(x)\n    neg = K.relu(-x)\n    return K.concatenate([pos, neg], axis=1)\n\ndef antirectifier_output_shape(input_shape):\n    shape = list(input_shape)\n    assert len(shape) == 2  # only valid for 2D tensors\n    shape[-1] *= 2\n    return tuple(shape)\n\nmodel.add(Lambda(antirectifier,\n         output_shape=antirectifier_output_shape))\n\n\n\n\nArguments\n\n\n\n\nfunction\n: The function to be evaluated.\n    Takes input tensor as first argument.\n\n\noutput_shape\n: Expected output shape from function.\n    Can be a tuple or function.\n    If a tuple, it only specifies the first dimension onward;\n     sample dimension is assumed either the same as the input:\n     \noutput_shape = (input_shape[0], ) + output_shape\n\n     or, the input is \nNone\n and\n     the sample dimension is also \nNone\n:\n     \noutput_shape = (None, ) + output_shape\n\n    If a function, it specifies the entire shape as a function of the\n    input shape: \noutput_shape = f(input_shape)\n\n\narguments\n: optional dictionary of keyword arguments to be passed\n    to the function.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument input_shape\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSpecified by \noutput_shape\n argument.\n\n\n\n\n[source]\n\n\nActivityRegularization\n\n\nkeras.layers.core.ActivityRegularization(l1=0.0, l2=0.0)\n\n\n\n\nLayer that applies an update to the cost function based input activity.\n\n\nArguments\n\n\n\n\nl1\n: L1 regularization factor (positive float).\n\n\nl2\n: L2 regularization factor (positive float).\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\n\n\n[source]\n\n\nMasking\n\n\nkeras.layers.core.Masking(mask_value=0.0)\n\n\n\n\nMasks a sequence by using a mask value to skip timesteps.\n\n\nFor each timestep in the input tensor (dimension #1 in the tensor),\nif all values in the input tensor at that timestep\nare equal to \nmask_value\n, then the timestep will masked (skipped)\nin all downstream layers (as long as they support masking).\n\n\nIf any downstream layer does not support masking yet receives such\nan input mask, an exception will be raised.\n\n\nExample\n\n\nConsider a Numpy data array \nx\n of shape \n(samples, timesteps, features)\n,\nto be fed to a LSTM layer.\nYou want to mask timestep #3 and #5 because you lack data for\nthese timesteps. You can:\n\n\n\n\nset \nx[:, 3, :] = 0.\n and \nx[:, 5, :] = 0.\n\n\ninsert a \nMasking\n layer with \nmask_value=0.\n before the LSTM layer:\n\n\n\n\nmodel = Sequential()\nmodel.add(Masking(mask_value=0., input_shape=(timesteps, features)))\nmodel.add(LSTM(32))\n\n\n\n\n\n\n[source]\n\n\nHighway\n\n\nkeras.layers.core.Highway(init='glorot_uniform', activation=None, weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)\n\n\n\n\nDensely connected highway network.\n\n\nHighway layers are a natural extension of LSTMs to feedforward networks.\n\n\nArguments\n\n\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n),\n    or alternatively, Theano function to use for weights\n    initialization. This parameter is only relevant\n    if you don't pass a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of Numpy arrays to set as initial weights.\n    The list should have 2 elements, of shape \n(input_dim, output_dim)\n\n    and (output_dim,) for weights and biases respectively.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\ninput_dim\n: dimensionality of the input (integer). This argument\n    (or alternatively, the keyword argument \ninput_shape\n)\n    is required when using this layer as the first layer in a model.\n\n\n\n\nInput shape\n\n\n2D tensor with shape: \n(nb_samples, input_dim)\n.\n\n\nOutput shape\n\n\n2D tensor with shape: \n(nb_samples, input_dim)\n.\n\n\nReferences\n\n\n\n\nHighway Networks\n\n\n\n\n\n\n[source]\n\n\nMaxoutDense\n\n\nkeras.layers.core.MaxoutDense(output_dim, nb_feature=4, init='glorot_uniform', weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)\n\n\n\n\nA dense maxout layer.\n\n\nA \nMaxoutDense\n layer takes the element-wise maximum of\n\nnb_feature\n \nDense(input_dim, output_dim)\n linear layers.\nThis allows the layer to learn a convex,\npiecewise linear activation function over the inputs.\n\n\nNote that this is a \nlinear\n layer;\nif you wish to apply activation function\n(you shouldn't need to --they are universal function approximators),\nan \nActivation\n layer must be added after.\n\n\nArguments\n\n\n\n\noutput_dim\n: int \n 0.\n\n\nnb_feature\n: number of Dense layers to use internally.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n),\n    or alternatively, Theano function to use for weights\n    initialization. This parameter is only relevant\n    if you don't pass a \nweights\n argument.\n\n\nweights\n: list of Numpy arrays to set as initial weights.\n    The list should have 2 elements, of shape \n(input_dim, output_dim)\n\n    and (output_dim,) for weights and biases respectively.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\ninput_dim\n: dimensionality of the input (integer). This argument\n    (or alternatively, the keyword argument \ninput_shape\n)\n    is required when using this layer as the first layer in a model.\n\n\n\n\nInput shape\n\n\n2D tensor with shape: \n(nb_samples, input_dim)\n.\n\n\nOutput shape\n\n\n2D tensor with shape: \n(nb_samples, output_dim)\n.\n\n\nReferences\n\n\n\n\nMaxout Networks\n\n\n\n\n\n\n[source]\n\n\nTimeDistributedDense\n\n\nkeras.layers.core.TimeDistributedDense(output_dim, init='glorot_uniform', activation=None, weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, input_length=None)\n\n\n\n\nApply a same Dense layer for each dimension[1] (time_dimension) input.\n\n\nEspecially useful after a recurrent network with 'return_sequence=True'.\n\n\n\n\nNote\n: this layer is deprecated, prefer using the \nTimeDistributed\n wrapper:\n\n\n\n\nmodel.add(TimeDistributed(Dense(32)))\n\n\n\n\nInput shape\n\n\n3D tensor with shape \n(nb_sample, time_dimension, input_dim)\n.\n\n\nOutput shape\n\n\n3D tensor with shape \n(nb_sample, time_dimension, output_dim)\n.\n\n\nArguments\n\n\n\n\noutput_dim\n: int \n 0.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n),\n    or alternatively, Theano function to use for weights\n    initialization. This parameter is only relevant\n    if you don't pass a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of Numpy arrays to set as initial weights.\n    The list should have 2 elements, of shape \n(input_dim, output_dim)\n\n    and (output_dim,) for weights and biases respectively.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\ninput_dim\n: dimensionality of the input (integer). This argument\n    (or alternatively, the keyword argument \ninput_shape\n)\n    is required when using this layer as the first layer in a model.\n\n\ninput_length\n: length of inputs sequences\n    (integer, or None for variable-length sequences).", 
            "title": "Core Layers"
        }, 
        {
            "location": "/layers/core/#dense", 
            "text": "keras.layers.core.Dense(output_dim, init='glorot_uniform', activation=None, weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)  Just your regular densely-connected NN layer.  Example  # as first layer in a sequential model:\nmodel = Sequential()\nmodel.add(Dense(32, input_dim=16))\n# now the model will take as input arrays of shape (*, 16)\n# and output arrays of shape (*, 32)\n\n# this is equivalent to the above:\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(16,)))\n\n# after the first layer, you don't need to specify\n# the size of the input anymore:\nmodel.add(Dense(32))  Arguments   output_dim : int   0.  init : name of initialization function for the weights of the layer\n    (see  initializations ),\n    or alternatively, Theano function to use for weights\n    initialization. This parameter is only relevant\n    if you don't pass a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of Numpy arrays to set as initial weights.\n    The list should have 2 elements, of shape  (input_dim, output_dim) \n    and (output_dim,) for weights and biases respectively.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).  input_dim : dimensionality of the input (integer). This argument\n    (or alternatively, the keyword argument  input_shape )\n    is required when using this layer as the first layer in a model.   Input shape  nD tensor with shape:  (nb_samples, ..., input_dim) .\nThe most common situation would be\na 2D input with shape  (nb_samples, input_dim) .  Output shape  nD tensor with shape:  (nb_samples, ..., output_dim) .\nFor instance, for a 2D input with shape  (nb_samples, input_dim) ,\nthe output would have shape  (nb_samples, output_dim) .   [source]", 
            "title": "Dense"
        }, 
        {
            "location": "/layers/core/#activation", 
            "text": "keras.layers.core.Activation(activation)  Applies an activation function to an output.  Arguments   activation : name of activation function to use  (see :  activations ),\nor alternatively, a Theano or TensorFlow operation.     Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.   [source]", 
            "title": "Activation"
        }, 
        {
            "location": "/layers/core/#dropout", 
            "text": "keras.layers.core.Dropout(p, noise_shape=None, seed=None)  Applies Dropout to the input.  Dropout consists in randomly setting\na fraction  p  of input units to 0 at each update during training time,\nwhich helps prevent overfitting.  Arguments   p : float between 0 and 1. Fraction of the input units to drop.  noise_shape : 1D integer tensor representing the shape of the\n    binary dropout mask that will be multiplied with the input.\n    For instance, if your inputs ahve shape\n     (batch_size, timesteps, features)  and\n    you want the dropout mask to be the same for all timesteps,\n    you can use  noise_shape=(batch_size, 1, features) .  seed : A Python integer to use as random seed.   References   Dropout: A Simple Way to Prevent Neural Networks from Overfitting    [source]", 
            "title": "Dropout"
        }, 
        {
            "location": "/layers/core/#spatialdropout2d", 
            "text": "keras.layers.core.SpatialDropout2D(p, dim_ordering='default')  Spatial 2D version of Dropout.  This version performs the same function as Dropout, however it drops\nentire 2D feature maps instead of individual elements. If adjacent pixels\nwithin feature maps are strongly correlated (as is normally the case in\nearly convolution layers) then regular dropout will not regularize the\nactivations and will otherwise just result in an effective learning rate\ndecrease. In this case, SpatialDropout2D will help promote independence\nbetween feature maps and should be used instead.  Arguments   p : float between 0 and 1. Fraction of the input units to drop.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  Same as input  References   Efficient Object Localization Using Convolutional Networks    [source]", 
            "title": "SpatialDropout2D"
        }, 
        {
            "location": "/layers/core/#spatialdropout3d", 
            "text": "keras.layers.core.SpatialDropout3D(p, dim_ordering='default')  Spatial 3D version of Dropout.  This version performs the same function as Dropout, however it drops\nentire 3D feature maps instead of individual elements. If adjacent voxels\nwithin feature maps are strongly correlated (as is normally the case in\nearly convolution layers) then regular dropout will not regularize the\nactivations and will otherwise just result in an effective learning rate\ndecrease. In this case, SpatialDropout3D will help promote independence\nbetween feature maps and should be used instead.  Arguments   p : float between 0 and 1. Fraction of the input units to drop.  dim_ordering : 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  5D tensor with shape: (samples, channels, dim1, dim2, dim3)  if dim_ordering='th'\nor 5D tensor with shape: (samples, dim1, dim2, dim3, channels)  if dim_ordering='tf'.  Output shape  Same as input  References   Efficient Object Localization Using Convolutional Networks    [source]", 
            "title": "SpatialDropout3D"
        }, 
        {
            "location": "/layers/core/#flatten", 
            "text": "keras.layers.core.Flatten()  Flattens the input. Does not affect the batch size.  Example  model = Sequential()\nmodel.add(Convolution2D(64, 3, 3,\n            border_mode='same',\n            input_shape=(3, 32, 32)))\n# now: model.output_shape == (None, 64, 32, 32)\n\nmodel.add(Flatten())\n# now: model.output_shape == (None, 65536)   [source]", 
            "title": "Flatten"
        }, 
        {
            "location": "/layers/core/#reshape", 
            "text": "keras.layers.core.Reshape(target_shape)  Reshapes an output to a certain shape.  Arguments   target_shape : target shape. Tuple of integers,\n    does not include the samples dimension (batch size).   Input shape  Arbitrary, although all dimensions in the input shaped must be fixed.\nUse the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  (batch_size,) + target_shape  Example  # as first layer in a Sequential model\nmodel = Sequential()\nmodel.add(Reshape((3, 4), input_shape=(12,)))\n# now: model.output_shape == (None, 3, 4)\n# note: `None` is the batch dimension\n\n# as intermediate layer in a Sequential model\nmodel.add(Reshape((6, 2)))\n# now: model.output_shape == (None, 6, 2)\n\n# also supports shape inference using `-1` as dimension\nmodel.add(Reshape((-1, 2, 2)))\n# now: model.output_shape == (None, 3, 2, 2)   [source]", 
            "title": "Reshape"
        }, 
        {
            "location": "/layers/core/#permute", 
            "text": "keras.layers.core.Permute(dims)  Permutes the dimensions of the input according to a given pattern.  Useful for e.g. connecting RNNs and convnets together.  Example  model = Sequential()\nmodel.add(Permute((2, 1), input_shape=(10, 64)))\n# now: model.output_shape == (None, 64, 10)\n# note: `None` is the batch dimension  Arguments   dims : Tuple of integers. Permutation pattern, does not include the\n    samples dimension. Indexing starts at 1.\n    For instance,  (2, 1)  permutes the first and second dimension\n    of the input.   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same as the input shape, but with the dimensions re-ordered according\nto the specified pattern.   [source]", 
            "title": "Permute"
        }, 
        {
            "location": "/layers/core/#repeatvector", 
            "text": "keras.layers.core.RepeatVector(n)  Repeats the input n times.  Example  model = Sequential()\nmodel.add(Dense(32, input_dim=32))\n# now: model.output_shape == (None, 32)\n# note: `None` is the batch dimension\n\nmodel.add(RepeatVector(3))\n# now: model.output_shape == (None, 3, 32)  Arguments   n : integer, repetition factor.   Input shape  2D tensor of shape  (nb_samples, features) .  Output shape  3D tensor of shape  (nb_samples, n, features) .   [source]", 
            "title": "RepeatVector"
        }, 
        {
            "location": "/layers/core/#merge", 
            "text": "keras.engine.topology.Merge(layers=None, mode='sum', concat_axis=-1, dot_axes=-1, output_shape=None, output_mask=None, arguments=None, node_indices=None, tensor_indices=None, name=None)  A  Merge  layer can be used to merge a list of tensors\ninto a single tensor, following some merge  mode .  Example  model1 = Sequential()\nmodel1.add(Dense(32, input_dim=32))\n\nmodel2 = Sequential()\nmodel2.add(Dense(32, input_dim=32))\n\nmerged_model = Sequential()\nmerged_model.add(Merge([model1, model2], mode='concat', concat_axis=1))  Arguments   layers : Can be a list of Keras tensors or\n    a list of layer instances. Must be more\n    than one layer/tensor.  mode : String or lambda/function. If string, must be one  of : 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'.\nIf lambda/function, it should take as input a list of tensors\nand return a single tensor.    concat_axis : Integer, axis to use in mode  concat .  dot_axes : Integer or tuple of integers,\n    axes to use in mode  dot  or  cos .  output_shape : Either a shape tuple (tuple of integers),\n    or a lambda/function\n    to compute  output_shape \n    (only if merge mode is a lambda/function).\n    If the argument is a tuple,\n    it should be expected output shape,  not  including the batch size\n    (same convention as the  input_shape  argument in layers).\n    If the argument is callable,\n    it should take as input a list of shape tuples  (1 :1 mapping to input tensors)\nand return a single shape tuple, including the\nbatch size (same convention as the get_output_shape_for  method of layers).    node_indices : Optional list of integers containing\n    the output node index for each input layer\n    (in case some input layers have multiple output nodes).\n    will default to an array of 0s if not provided.  tensor_indices : Optional list of indices of output tensors\n    to consider for merging\n    (in case some input layer node returns multiple tensors).  output_mask : Mask or lambda/function to compute the output mask (only\n    if merge mode is a lambda/function). If the latter case, it should\n    take as input a list of masks and return a single mask.    [source]", 
            "title": "Merge"
        }, 
        {
            "location": "/layers/core/#lambda", 
            "text": "keras.layers.core.Lambda(function, output_shape=None, arguments=None)  Used for evaluating an arbitrary expressions on an input.  Examples  # add a x -  x^2 layer\nmodel.add(Lambda(lambda x: x ** 2))  # add a layer that returns the concatenation\n# of the positive part of the input and\n# the opposite of the negative part\n\ndef antirectifier(x):\n    x -= K.mean(x, axis=1, keepdims=True)\n    x = K.l2_normalize(x, axis=1)\n    pos = K.relu(x)\n    neg = K.relu(-x)\n    return K.concatenate([pos, neg], axis=1)\n\ndef antirectifier_output_shape(input_shape):\n    shape = list(input_shape)\n    assert len(shape) == 2  # only valid for 2D tensors\n    shape[-1] *= 2\n    return tuple(shape)\n\nmodel.add(Lambda(antirectifier,\n         output_shape=antirectifier_output_shape))  Arguments   function : The function to be evaluated.\n    Takes input tensor as first argument.  output_shape : Expected output shape from function.\n    Can be a tuple or function.\n    If a tuple, it only specifies the first dimension onward;\n     sample dimension is assumed either the same as the input:\n      output_shape = (input_shape[0], ) + output_shape \n     or, the input is  None  and\n     the sample dimension is also  None :\n      output_shape = (None, ) + output_shape \n    If a function, it specifies the entire shape as a function of the\n    input shape:  output_shape = f(input_shape)  arguments : optional dictionary of keyword arguments to be passed\n    to the function.   Input shape  Arbitrary. Use the keyword argument input_shape\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Specified by  output_shape  argument.   [source]", 
            "title": "Lambda"
        }, 
        {
            "location": "/layers/core/#activityregularization", 
            "text": "keras.layers.core.ActivityRegularization(l1=0.0, l2=0.0)  Layer that applies an update to the cost function based input activity.  Arguments   l1 : L1 regularization factor (positive float).  l2 : L2 regularization factor (positive float).   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.   [source]", 
            "title": "ActivityRegularization"
        }, 
        {
            "location": "/layers/core/#masking", 
            "text": "keras.layers.core.Masking(mask_value=0.0)  Masks a sequence by using a mask value to skip timesteps.  For each timestep in the input tensor (dimension #1 in the tensor),\nif all values in the input tensor at that timestep\nare equal to  mask_value , then the timestep will masked (skipped)\nin all downstream layers (as long as they support masking).  If any downstream layer does not support masking yet receives such\nan input mask, an exception will be raised.  Example  Consider a Numpy data array  x  of shape  (samples, timesteps, features) ,\nto be fed to a LSTM layer.\nYou want to mask timestep #3 and #5 because you lack data for\nthese timesteps. You can:   set  x[:, 3, :] = 0.  and  x[:, 5, :] = 0.  insert a  Masking  layer with  mask_value=0.  before the LSTM layer:   model = Sequential()\nmodel.add(Masking(mask_value=0., input_shape=(timesteps, features)))\nmodel.add(LSTM(32))   [source]", 
            "title": "Masking"
        }, 
        {
            "location": "/layers/core/#highway", 
            "text": "keras.layers.core.Highway(init='glorot_uniform', activation=None, weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)  Densely connected highway network.  Highway layers are a natural extension of LSTMs to feedforward networks.  Arguments   init : name of initialization function for the weights of the layer\n    (see  initializations ),\n    or alternatively, Theano function to use for weights\n    initialization. This parameter is only relevant\n    if you don't pass a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of Numpy arrays to set as initial weights.\n    The list should have 2 elements, of shape  (input_dim, output_dim) \n    and (output_dim,) for weights and biases respectively.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).  input_dim : dimensionality of the input (integer). This argument\n    (or alternatively, the keyword argument  input_shape )\n    is required when using this layer as the first layer in a model.   Input shape  2D tensor with shape:  (nb_samples, input_dim) .  Output shape  2D tensor with shape:  (nb_samples, input_dim) .  References   Highway Networks    [source]", 
            "title": "Highway"
        }, 
        {
            "location": "/layers/core/#maxoutdense", 
            "text": "keras.layers.core.MaxoutDense(output_dim, nb_feature=4, init='glorot_uniform', weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None)  A dense maxout layer.  A  MaxoutDense  layer takes the element-wise maximum of nb_feature   Dense(input_dim, output_dim)  linear layers.\nThis allows the layer to learn a convex,\npiecewise linear activation function over the inputs.  Note that this is a  linear  layer;\nif you wish to apply activation function\n(you shouldn't need to --they are universal function approximators),\nan  Activation  layer must be added after.  Arguments   output_dim : int   0.  nb_feature : number of Dense layers to use internally.  init : name of initialization function for the weights of the layer\n    (see  initializations ),\n    or alternatively, Theano function to use for weights\n    initialization. This parameter is only relevant\n    if you don't pass a  weights  argument.  weights : list of Numpy arrays to set as initial weights.\n    The list should have 2 elements, of shape  (input_dim, output_dim) \n    and (output_dim,) for weights and biases respectively.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).  input_dim : dimensionality of the input (integer). This argument\n    (or alternatively, the keyword argument  input_shape )\n    is required when using this layer as the first layer in a model.   Input shape  2D tensor with shape:  (nb_samples, input_dim) .  Output shape  2D tensor with shape:  (nb_samples, output_dim) .  References   Maxout Networks    [source]", 
            "title": "MaxoutDense"
        }, 
        {
            "location": "/layers/core/#timedistributeddense", 
            "text": "keras.layers.core.TimeDistributedDense(output_dim, init='glorot_uniform', activation=None, weights=None, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, input_length=None)  Apply a same Dense layer for each dimension[1] (time_dimension) input.  Especially useful after a recurrent network with 'return_sequence=True'.   Note : this layer is deprecated, prefer using the  TimeDistributed  wrapper:   model.add(TimeDistributed(Dense(32)))  Input shape  3D tensor with shape  (nb_sample, time_dimension, input_dim) .  Output shape  3D tensor with shape  (nb_sample, time_dimension, output_dim) .  Arguments   output_dim : int   0.  init : name of initialization function for the weights of the layer\n    (see  initializations ),\n    or alternatively, Theano function to use for weights\n    initialization. This parameter is only relevant\n    if you don't pass a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of Numpy arrays to set as initial weights.\n    The list should have 2 elements, of shape  (input_dim, output_dim) \n    and (output_dim,) for weights and biases respectively.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).  input_dim : dimensionality of the input (integer). This argument\n    (or alternatively, the keyword argument  input_shape )\n    is required when using this layer as the first layer in a model.  input_length : length of inputs sequences\n    (integer, or None for variable-length sequences).", 
            "title": "TimeDistributedDense"
        }, 
        {
            "location": "/layers/convolutional/", 
            "text": "[source]\n\n\nConvolution1D\n\n\nkeras.layers.convolutional.Convolution1D(nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample_length=1, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, input_length=None)\n\n\n\n\nConvolution operator for filtering neighborhoods of 1-D inputs.\n\n\nWhen using this layer as the first layer in a model,\neither provide the keyword argument \ninput_dim\n\n(int, e.g. 128 for sequences of 128-dimensional vectors),\nor \ninput_shape\n (tuple of integers, e.g. (10, 128) for sequences\nof 10 vectors of 128-dimensional vectors).\n\n\nExample\n\n\n# apply a convolution 1d of length 3 to a sequence with 10 timesteps,\n# with 64 output filters\nmodel = Sequential()\nmodel.add(Convolution1D(64, 3, border_mode='same', input_shape=(10, 32)))\n# now model.output_shape == (None, 10, 64)\n\n# add a new conv1d on top\nmodel.add(Convolution1D(32, 3, border_mode='same'))\n# now model.output_shape == (None, 10, 32)\n\n\n\n\nArguments\n\n\n\n\nnb_filter\n: Number of convolution kernels to use\n    (dimensionality of the output).\n\n\nfilter_length\n: The extension (spatial or temporal) of each filter.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant\n    if you don't pass a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of numpy arrays to set as initial weights.\n\n\nborder_mode\n: 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).\n\n\nsubsample_length\n: factor by which to subsample output.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\ninput_dim\n: Number of channels/dimensions in the input.\n    Either this argument or the keyword argument \ninput_shape\nmust be\n    provided when using this layer as the first layer in a model.\n\n\ninput_length\n: Length of input sequences, when it is constant.\n    This argument is required if you are going to connect\n    \nFlatten\n then \nDense\n layers upstream\n    (without it, the shape of the dense outputs cannot be computed).\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(samples, steps, input_dim)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(samples, new_steps, nb_filter)\n.\n\nsteps\n value might have changed due to padding.\n\n\n\n\n[source]\n\n\nAtrousConvolution1D\n\n\nkeras.layers.convolutional.AtrousConvolution1D(nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)\n\n\n\n\nAtrous Convolution operator for filtering neighborhoods of 1-D inputs.\n\n\nA.k.a dilated convolution or convolution with holes.\nWhen using this layer as the first layer in a model,\neither provide the keyword argument \ninput_dim\n\n(int, e.g. 128 for sequences of 128-dimensional vectors),\nor \ninput_shape\n (tuples of integers, e.g. (10, 128) for sequences\nof 10 vectors of 128-dimensional vectors).\n\n\nExample\n\n\n# apply an atrous convolution 1d\n# with atrous rate 2 of length 3 to a sequence with 10 timesteps,\n# with 64 output filters\nmodel = Sequential()\nmodel.add(AtrousConvolution1D(64, 3, atrous_rate=2,\n              border_mode='same',\n              input_shape=(10, 32)))\n# now model.output_shape == (None, 10, 64)\n\n# add a new atrous conv1d on top\nmodel.add(AtrousConvolution1D(32, 3, atrous_rate=2,\n              border_mode='same'))\n# now model.output_shape == (None, 10, 32)\n\n\n\n\nArguments\n\n\n\n\nnb_filter\n: Number of convolution kernels to use\n    (dimensionality of the output).\n\n\nfilter_length\n: The extension (spatial or temporal) of each filter.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant\n    if you don't pass a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of numpy arrays to set as initial weights.\n\n\nborder_mode\n: 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).\n\n\nsubsample_length\n: factor by which to subsample output.\n\n\natrous_rate\n: Factor for kernel dilation. Also called filter_dilation\n    elsewhere.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\ninput_dim\n: Number of channels/dimensions in the input.\n    Either this argument or the keyword argument \ninput_shape\nmust be\n    provided when using this layer as the first layer in a model.\n\n\ninput_length\n: Length of input sequences, when it is constant.\n    This argument is required if you are going to connect\n    \nFlatten\n then \nDense\n layers upstream\n    (without it, the shape of the dense outputs cannot be computed).\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(samples, steps, input_dim)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(samples, new_steps, nb_filter)\n.\n\nsteps\n value might have changed due to padding.\n\n\n\n\n[source]\n\n\nConvolution2D\n\n\nkeras.layers.convolutional.Convolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)\n\n\n\n\nConvolution operator for filtering windows of two-dimensional inputs.\n\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument \ninput_shape\n\n(tuple of integers, does not include the sample axis),\ne.g. \ninput_shape=(3, 128, 128)\n for 128x128 RGB pictures.\n\n\nExamples\n\n\n# apply a 3x3 convolution with 64 output filters on a 256x256 image:\nmodel = Sequential()\nmodel.add(Convolution2D(64, 3, 3,\n            border_mode='same',\n            input_shape=(3, 256, 256)))\n# now model.output_shape == (None, 64, 256, 256)\n\n# add a 3x3 convolution on top, with 32 output filters:\nmodel.add(Convolution2D(32, 3, 3, border_mode='same'))\n# now model.output_shape == (None, 32, 256, 256)\n\n\n\n\nArguments\n\n\n\n\nnb_filter\n: Number of convolution filters to use.\n\n\nnb_row\n: Number of rows in the convolution kernel.\n\n\nnb_col\n: Number of columns in the convolution kernel.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of numpy arrays to set as initial weights.\n\n\nborder_mode\n: 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).\n\n\nsubsample\n: tuple of length 2. Factor by which to subsample output.\n    Also called strides elsewhere.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, nb_filter, new_rows, new_cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, new_rows, new_cols, nb_filter)\n if dim_ordering='tf'.\n\nrows\n and \ncols\n values might have changed due to padding.\n\n\n\n\n[source]\n\n\nAtrousConvolution2D\n\n\nkeras.layers.convolutional.AtrousConvolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), atrous_rate=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)\n\n\n\n\nAtrous Convolution operator for filtering windows of 2-D inputs.\n\n\nA.k.a dilated convolution or convolution with holes.\nWhen using this layer as the first layer in a model,\nprovide the keyword argument \ninput_shape\n\n(tuple of integers, does not include the sample axis),\ne.g. \ninput_shape=(3, 128, 128)\n for 128x128 RGB pictures.\n\n\nExamples\n\n\n# apply a 3x3 convolution with atrous rate 2x2\n# and 64 output filters on a 256x256 image:\nmodel = Sequential()\nmodel.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2),\n              border_mode='valid',\n              input_shape=(3, 256, 256)))\n# now the actual kernel size is dilated\n# from 3x3 to 5x5 (3+(3-1)*(2-1)=5)\n# thus model.output_shape == (None, 64, 252, 252)\n\n\n\n\nArguments\n\n\n\n\nnb_filter\n: Number of convolution filters to use.\n\n\nnb_row\n: Number of rows in the convolution kernel.\n\n\nnb_col\n: Number of columns in the convolution kernel.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of numpy arrays to set as initial weights.\n\n\nborder_mode\n: 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).\n\n\nsubsample\n: tuple of length 2. Factor by which to subsample output.\n    Also called strides elsewhere.\n\n\natrous_rate\n: tuple of length 2. Factor for kernel dilation.\n    Also called filter_dilation elsewhere.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, nb_filter, new_rows, new_cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, new_rows, new_cols, nb_filter)\n if dim_ordering='tf'.\n\nrows\n and \ncols\n values might have changed due to padding.\n\n\nReferences\n\n\n\n\nMulti-Scale Context Aggregation by Dilated Convolutions\n\n\n\n\n\n\n[source]\n\n\nSeparableConvolution2D\n\n\nkeras.layers.convolutional.SeparableConvolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), depth_multiplier=1, dim_ordering='default', depthwise_regularizer=None, pointwise_regularizer=None, b_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, b_constraint=None, bias=True)\n\n\n\n\nSeparable convolution operator for 2D inputs.\n\n\nSeparable convolutions consist in first performing\na depthwise spatial convolution\n(which acts on each input channel separately)\nfollowed by a pointwise convolution which mixes together the resulting\noutput channels. The \ndepth_multiplier\n argument controls how many\noutput channels are generated per input channel in the depthwise step.\n\n\nIntuitively, separable convolutions can be understood as\na way to factorize a convolution kernel into two smaller kernels,\nor as an extreme version of an Inception block.\n\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument \ninput_shape\n\n(tuple of integers, does not include the sample axis),\ne.g. \ninput_shape=(3, 128, 128)\n for 128x128 RGB pictures.\n\n\nTheano warning\n\n\nThis layer is only available with the\nTensorFlow backend for the time being.\n\n\nArguments\n\n\n\n\nnb_filter\n: Number of convolution filters to use.\n\n\nnb_row\n: Number of rows in the convolution kernel.\n\n\nnb_col\n: Number of columns in the convolution kernel.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of numpy arrays to set as initial weights.\n\n\nborder_mode\n: 'valid' or 'same'.\n\n\nsubsample\n: tuple of length 2. Factor by which to subsample output.\n    Also called strides elsewhere.\n\n\ndepth_multiplier\n: how many output channel to use per input channel\n    for the depthwise convolution step.\n\n\ndepthwise_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the depthwise weights matrix.\n\n\npointwise_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the pointwise weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\ndepthwise_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the depthwise weights matrix.\n\n\npointwise_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the pointwise weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, nb_filter, new_rows, new_cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, new_rows, new_cols, nb_filter)\n if dim_ordering='tf'.\n\nrows\n and \ncols\n values might have changed due to padding.\n\n\n\n\n[source]\n\n\nDeconvolution2D\n\n\nkeras.layers.convolutional.Deconvolution2D(nb_filter, nb_row, nb_col, output_shape, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)\n\n\n\n\nTransposed convolution operator for filtering windows of 2-D inputs.\n\n\nThe need for transposed convolutions generally arises from the desire to\nuse a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape\nof the output of some convolution to something that has the shape\nof its input while maintaining a connectivity pattern\nthat is compatible with said convolution.\n\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument \ninput_shape\n\n(tuple of integers, does not include the sample axis),\ne.g. \ninput_shape=(3, 128, 128)\n for 128x128 RGB pictures.\n\n\nTo pass the correct \noutput_shape\n to this layer,\none could use a test model to predict and observe the actual output shape.\n\n\nExamples\n\n\n# apply a 3x3 transposed convolution\n# with stride 1x1 and 3 output filters on a 12x12 image:\nmodel = Sequential()\nmodel.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 14, 14),\n              border_mode='valid',\n              input_shape=(3, 12, 12)))\n# Note that you will have to change\n# the output_shape depending on the backend used.\n\n# we can predict with the model and print the shape of the array.\ndummy_input = np.ones((32, 3, 12, 12))\n# For TensorFlow dummy_input = np.ones((32, 12, 12, 3))\npreds = model.predict(dummy_input)\nprint(preds.shape)\n# Theano GPU: (None, 3, 13, 13)\n# Theano CPU: (None, 3, 14, 14)\n# TensorFlow: (None, 14, 14, 3)\n\n# apply a 3x3 transposed convolution\n# with stride 2x2 and 3 output filters on a 12x12 image:\nmodel = Sequential()\nmodel.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 25, 25),\n              subsample=(2, 2),\n              border_mode='valid',\n              input_shape=(3, 12, 12)))\nmodel.summary()\n\n# we can predict with the model and print the shape of the array.\ndummy_input = np.ones((32, 3, 12, 12))\n# For TensorFlow dummy_input = np.ones((32, 12, 12, 3))\npreds = model.predict(dummy_input)\nprint(preds.shape)\n# Theano GPU: (None, 3, 25, 25)\n# Theano CPU: (None, 3, 25, 25)\n# TensorFlow: (None, 25, 25, 3)\n\n\n\n\nArguments\n\n\n\n\nnb_filter\n: Number of transposed convolution filters to use.\n\n\nnb_row\n: Number of rows in the transposed convolution kernel.\n\n\nnb_col\n: Number of columns in the transposed convolution kernel.\n\n\noutput_shape\n: Output shape of the transposed convolution operation.\n    tuple of integers\n    \n(nb_samples, nb_filter, nb_output_rows, nb_output_cols)\n.\n     It is better to use\n     a dummy input and observe the actual output shape of\n     a layer, as specified in the examples.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano/TensorFlow function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of numpy arrays to set as initial weights.\n\n\nborder_mode\n: 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).\n\n\nsubsample\n: tuple of length 2. Factor by which to oversample output.\n    Also called strides elsewhere.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, nb_filter, new_rows, new_cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, new_rows, new_cols, nb_filter)\n if dim_ordering='tf'.\n\nrows\n and \ncols\n values might have changed due to padding.\n\n\nReferences\n\n\n\n\nA guide to convolution arithmetic for deep learning\n\n\nTransposed convolution arithmetic\n\n\nDeconvolutional Networks\n\n\n\n\n\n\n[source]\n\n\nConvolution3D\n\n\nkeras.layers.convolutional.Convolution3D(nb_filter, kernel_dim1, kernel_dim2, kernel_dim3, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)\n\n\n\n\nConvolution operator for filtering windows of three-dimensional inputs.\n\n\nWhen using this layer as the first layer in a model,\nprovide the keyword argument \ninput_shape\n\n(tuple of integers, does not include the sample axis),\ne.g. \ninput_shape=(3, 10, 128, 128)\n for 10 frames of 128x128 RGB pictures.\n\n\nArguments\n\n\n\n\nnb_filter\n: Number of convolution filters to use.\n\n\nkernel_dim1\n: Length of the first dimension in the convolution kernel.\n\n\nkernel_dim2\n: Length of the second dimension in the convolution kernel.\n\n\nkernel_dim3\n: Length of the third dimension in the convolution kernel.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of Numpy arrays to set as initial weights.\n\n\nborder_mode\n: 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).\n\n\nsubsample\n: tuple of length 3. Factor by which to subsample output.\n    Also called strides elsewhere.\n\n\nNote\n: 'subsample' is implemented by slicing\nthe output of conv3d with strides=(1,1,1).\n\n\n\n\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\nbias\n: whether to include a bias\n    (i.e. make the layer affine rather than linear).\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n\n(samples, channels, conv_dim1, conv_dim2, conv_dim3)\n if dim_ordering='th'\nor 5D tensor with shape:\n\n(samples, conv_dim1, conv_dim2, conv_dim3, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n5D tensor with shape:\n\n(samples, nb_filter, new_conv_dim1, new_conv_dim2, new_conv_dim3)\n if dim_ordering='th'\nor 5D tensor with shape:\n\n(samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, nb_filter)\n if dim_ordering='tf'.\n\nnew_conv_dim1\n, \nnew_conv_dim2\n and \nnew_conv_dim3\n values might have changed due to padding.\n\n\n\n\n[source]\n\n\nCropping1D\n\n\nkeras.layers.convolutional.Cropping1D(cropping=(1, 1))\n\n\n\n\nCropping layer for 1D input (e.g. temporal sequence).\n\n\nIt crops along the time dimension (axis 1).\n\n\nArguments\n\n\n\n\ncropping\n: tuple of int (length 2)\n    How many units should be trimmed off at the beginning and end of\n    the cropping dimension (axis 1).\n\n\n\n\nInput shape\n\n\n3D tensor with shape \n(samples, axis_to_crop, features)\n\n\nOutput shape\n\n\n3D tensor with shape \n(samples, cropped_axis, features)\n\n\n\n\n[source]\n\n\nCropping2D\n\n\nkeras.layers.convolutional.Cropping2D(cropping=((0, 0), (0, 0)), dim_ordering='default')\n\n\n\n\nCropping layer for 2D input (e.g. picture).\n\n\nIt crops along spatial dimensions, i.e. width and height.\n\n\nArguments\n\n\n\n\ncropping\n: tuple of tuple of int (length 2)\n    How many units should be trimmed off at the beginning and end of\n    the 2 cropping dimensions (width, height).\n\n\ndim_ordering\n: 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, depth, first_axis_to_crop, second_axis_to_crop)\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, depth, first_cropped_axis, second_cropped_axis)\n\n\nExamples\n\n\n# Crop the input 2D images or feature maps\nmodel = Sequential()\nmodel.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(3, 28, 28)))\n# now model.output_shape == (None, 3, 24, 20)\nmodel.add(Convolution2D(64, 3, 3, border_mode='same))\nmodel.add(Cropping2D(cropping=((2, 2), (2, 2))))\n# now model.output_shape == (None, 64, 20, 16)\n\n\n\n\n\n\n\n[source]\n\n\nCropping3D\n\n\nkeras.layers.convolutional.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), dim_ordering='default')\n\n\n\n\nCropping layer for 3D data (e.g. spatial or spatio-temporal).\n\n\nArguments\n\n\n\n\ncropping\n: tuple of tuple of int (length 3)\n    How many units should be trimmed off at the beginning and end of\n    the 3 cropping dimensions (kernel_dim1, kernel_dim2, kernerl_dim3).\n\n\ndim_ordering\n: 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n\n(samples, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)\n\n\nOutput shape\n\n\n5D tensor with shape:\n\n(samples, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)\n\n\n\n\n[source]\n\n\nUpSampling1D\n\n\nkeras.layers.convolutional.UpSampling1D(length=2)\n\n\n\n\nUpsampling layer for 1D inputs.\n\n\nRepeats each temporal step \nlength\n times along the time axis.\n\n\nArguments\n\n\n\n\nlength\n: integer. Upsampling factor.\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(samples, steps, features)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(samples, upsampled_steps, features)\n.\n\n\n\n\n[source]\n\n\nUpSampling2D\n\n\nkeras.layers.convolutional.UpSampling2D(size=(2, 2), dim_ordering='default')\n\n\n\n\nUpsampling layer for 2D inputs.\n\n\nRepeats the rows and columns of the data\nby size[0] and size[1] respectively.\n\n\nArguments\n\n\n\n\nsize\n: tuple of 2 integers. The upsampling factors for rows and columns.\n\n\ndim_ordering\n: 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, channels, upsampled_rows, upsampled_cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, upsampled_rows, upsampled_cols, channels)\n if dim_ordering='tf'.\n\n\n\n\n[source]\n\n\nUpSampling3D\n\n\nkeras.layers.convolutional.UpSampling3D(size=(2, 2, 2), dim_ordering='default')\n\n\n\n\nUpsampling layer for 3D inputs.\n\n\nRepeats the 1st, 2nd and 3rd dimensions\nof the data by size[0], size[1] and size[2] respectively.\n\n\nArguments\n\n\n\n\nsize\n: tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.\n\n\ndim_ordering\n: 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n\n(samples, channels, dim1, dim2, dim3)\n if dim_ordering='th'\nor 5D tensor with shape:\n\n(samples, dim1, dim2, dim3, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n5D tensor with shape:\n\n(samples, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)\n if dim_ordering='th'\nor 5D tensor with shape:\n\n(samples, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)\n if dim_ordering='tf'.\n\n\n\n\n[source]\n\n\nZeroPadding1D\n\n\nkeras.layers.convolutional.ZeroPadding1D(padding=1)\n\n\n\n\nZero-padding layer for 1D input (e.g. temporal sequence).\n\n\nArguments\n\n\n\n\npadding\n: int, or tuple of int (length 2), or dictionary.\n\n\nIf int:\nHow many zeros to add at the beginning and end of\nthe padding dimension (axis 1).\n\n\nIf tuple of int (length 2)\nHow many zeros to add at the beginning and at the end of\nthe padding dimension, in order '(left_pad, right_pad)'.\n\n\nIf dictionary: should contain the keys\n{'left_pad', 'right_pad'}.\nIf any key is missing, default value of 0 will be used for the missing key.\n\n\n\n\n\n\n\n\nInput shape\n\n\n3D tensor with shape \n(samples, axis_to_pad, features)\n\n\nOutput shape\n\n\n3D tensor with shape \n(samples, padded_axis, features)\n\n\n\n\n[source]\n\n\nZeroPadding2D\n\n\nkeras.layers.convolutional.ZeroPadding2D(padding=(1, 1), dim_ordering='default')\n\n\n\n\nZero-padding layer for 2D input (e.g. picture).\n\n\nArguments\n\n\n\n\npadding\n: tuple of int (length 2), or tuple of int (length 4), or dictionary.\n\n\nIf tuple of int (length 2):\nHow many zeros to add at the beginning and end of\nthe 2 padding dimensions (rows and cols).\n\n\nIf tuple of int (length 4):\nHow many zeros to add at the beginning and at the end of\nthe 2 padding dimensions (rows and cols), in the order\n'(top_pad, bottom_pad, left_pad, right_pad)'.\n\n\nIf dictionary: should contain the keys\n{'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}.\nIf any key is missing, default value of 0 will be used for the missing key.\n\n\n\n\n\n\ndim_ordering\n: 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, channels, padded_rows, padded_cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, padded_rows, padded_cols, channels)\n if dim_ordering='tf'.\n\n\n\n\n[source]\n\n\nZeroPadding3D\n\n\nkeras.layers.convolutional.ZeroPadding3D(padding=(1, 1, 1), dim_ordering='default')\n\n\n\n\nZero-padding layer for 3D data (spatial or spatio-temporal).\n\n\nArguments\n\n\n\n\npadding\n: tuple of int (length 3)\n    How many zeros to add at the beginning and end of\n    the 3 padding dimensions (axis 3, 4 and 5).\n    Currently only symmetric padding is supported.\n\n\ndim_ordering\n: 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n\n(samples, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)\n\n\nOutput shape\n\n\n5D tensor with shape:\n\n(samples, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)", 
            "title": "Convolutional Layers"
        }, 
        {
            "location": "/layers/convolutional/#convolution1d", 
            "text": "keras.layers.convolutional.Convolution1D(nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample_length=1, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, input_length=None)  Convolution operator for filtering neighborhoods of 1-D inputs.  When using this layer as the first layer in a model,\neither provide the keyword argument  input_dim \n(int, e.g. 128 for sequences of 128-dimensional vectors),\nor  input_shape  (tuple of integers, e.g. (10, 128) for sequences\nof 10 vectors of 128-dimensional vectors).  Example  # apply a convolution 1d of length 3 to a sequence with 10 timesteps,\n# with 64 output filters\nmodel = Sequential()\nmodel.add(Convolution1D(64, 3, border_mode='same', input_shape=(10, 32)))\n# now model.output_shape == (None, 10, 64)\n\n# add a new conv1d on top\nmodel.add(Convolution1D(32, 3, border_mode='same'))\n# now model.output_shape == (None, 10, 32)  Arguments   nb_filter : Number of convolution kernels to use\n    (dimensionality of the output).  filter_length : The extension (spatial or temporal) of each filter.  init : name of initialization function for the weights of the layer\n    (see  initializations ), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant\n    if you don't pass a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of numpy arrays to set as initial weights.  border_mode : 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).  subsample_length : factor by which to subsample output.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).  input_dim : Number of channels/dimensions in the input.\n    Either this argument or the keyword argument  input_shape must be\n    provided when using this layer as the first layer in a model.  input_length : Length of input sequences, when it is constant.\n    This argument is required if you are going to connect\n     Flatten  then  Dense  layers upstream\n    (without it, the shape of the dense outputs cannot be computed).   Input shape  3D tensor with shape:  (samples, steps, input_dim) .  Output shape  3D tensor with shape:  (samples, new_steps, nb_filter) . steps  value might have changed due to padding.   [source]", 
            "title": "Convolution1D"
        }, 
        {
            "location": "/layers/convolutional/#atrousconvolution1d", 
            "text": "keras.layers.convolutional.AtrousConvolution1D(nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample_length=1, atrous_rate=1, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)  Atrous Convolution operator for filtering neighborhoods of 1-D inputs.  A.k.a dilated convolution or convolution with holes.\nWhen using this layer as the first layer in a model,\neither provide the keyword argument  input_dim \n(int, e.g. 128 for sequences of 128-dimensional vectors),\nor  input_shape  (tuples of integers, e.g. (10, 128) for sequences\nof 10 vectors of 128-dimensional vectors).  Example  # apply an atrous convolution 1d\n# with atrous rate 2 of length 3 to a sequence with 10 timesteps,\n# with 64 output filters\nmodel = Sequential()\nmodel.add(AtrousConvolution1D(64, 3, atrous_rate=2,\n              border_mode='same',\n              input_shape=(10, 32)))\n# now model.output_shape == (None, 10, 64)\n\n# add a new atrous conv1d on top\nmodel.add(AtrousConvolution1D(32, 3, atrous_rate=2,\n              border_mode='same'))\n# now model.output_shape == (None, 10, 32)  Arguments   nb_filter : Number of convolution kernels to use\n    (dimensionality of the output).  filter_length : The extension (spatial or temporal) of each filter.  init : name of initialization function for the weights of the layer\n    (see  initializations ), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant\n    if you don't pass a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of numpy arrays to set as initial weights.  border_mode : 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).  subsample_length : factor by which to subsample output.  atrous_rate : Factor for kernel dilation. Also called filter_dilation\n    elsewhere.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).  input_dim : Number of channels/dimensions in the input.\n    Either this argument or the keyword argument  input_shape must be\n    provided when using this layer as the first layer in a model.  input_length : Length of input sequences, when it is constant.\n    This argument is required if you are going to connect\n     Flatten  then  Dense  layers upstream\n    (without it, the shape of the dense outputs cannot be computed).   Input shape  3D tensor with shape:  (samples, steps, input_dim) .  Output shape  3D tensor with shape:  (samples, new_steps, nb_filter) . steps  value might have changed due to padding.   [source]", 
            "title": "AtrousConvolution1D"
        }, 
        {
            "location": "/layers/convolutional/#convolution2d", 
            "text": "keras.layers.convolutional.Convolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)  Convolution operator for filtering windows of two-dimensional inputs.  When using this layer as the first layer in a model,\nprovide the keyword argument  input_shape \n(tuple of integers, does not include the sample axis),\ne.g.  input_shape=(3, 128, 128)  for 128x128 RGB pictures.  Examples  # apply a 3x3 convolution with 64 output filters on a 256x256 image:\nmodel = Sequential()\nmodel.add(Convolution2D(64, 3, 3,\n            border_mode='same',\n            input_shape=(3, 256, 256)))\n# now model.output_shape == (None, 64, 256, 256)\n\n# add a 3x3 convolution on top, with 32 output filters:\nmodel.add(Convolution2D(32, 3, 3, border_mode='same'))\n# now model.output_shape == (None, 32, 256, 256)  Arguments   nb_filter : Number of convolution filters to use.  nb_row : Number of rows in the convolution kernel.  nb_col : Number of columns in the convolution kernel.  init : name of initialization function for the weights of the layer\n    (see  initializations ), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of numpy arrays to set as initial weights.  border_mode : 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).  subsample : tuple of length 2. Factor by which to subsample output.\n    Also called strides elsewhere.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  4D tensor with shape: (samples, nb_filter, new_rows, new_cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, new_rows, new_cols, nb_filter)  if dim_ordering='tf'. rows  and  cols  values might have changed due to padding.   [source]", 
            "title": "Convolution2D"
        }, 
        {
            "location": "/layers/convolutional/#atrousconvolution2d", 
            "text": "keras.layers.convolutional.AtrousConvolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), atrous_rate=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)  Atrous Convolution operator for filtering windows of 2-D inputs.  A.k.a dilated convolution or convolution with holes.\nWhen using this layer as the first layer in a model,\nprovide the keyword argument  input_shape \n(tuple of integers, does not include the sample axis),\ne.g.  input_shape=(3, 128, 128)  for 128x128 RGB pictures.  Examples  # apply a 3x3 convolution with atrous rate 2x2\n# and 64 output filters on a 256x256 image:\nmodel = Sequential()\nmodel.add(AtrousConvolution2D(64, 3, 3, atrous_rate=(2,2),\n              border_mode='valid',\n              input_shape=(3, 256, 256)))\n# now the actual kernel size is dilated\n# from 3x3 to 5x5 (3+(3-1)*(2-1)=5)\n# thus model.output_shape == (None, 64, 252, 252)  Arguments   nb_filter : Number of convolution filters to use.  nb_row : Number of rows in the convolution kernel.  nb_col : Number of columns in the convolution kernel.  init : name of initialization function for the weights of the layer\n    (see  initializations ), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of numpy arrays to set as initial weights.  border_mode : 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).  subsample : tuple of length 2. Factor by which to subsample output.\n    Also called strides elsewhere.  atrous_rate : tuple of length 2. Factor for kernel dilation.\n    Also called filter_dilation elsewhere.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  4D tensor with shape: (samples, nb_filter, new_rows, new_cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, new_rows, new_cols, nb_filter)  if dim_ordering='tf'. rows  and  cols  values might have changed due to padding.  References   Multi-Scale Context Aggregation by Dilated Convolutions    [source]", 
            "title": "AtrousConvolution2D"
        }, 
        {
            "location": "/layers/convolutional/#separableconvolution2d", 
            "text": "keras.layers.convolutional.SeparableConvolution2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), depth_multiplier=1, dim_ordering='default', depthwise_regularizer=None, pointwise_regularizer=None, b_regularizer=None, activity_regularizer=None, depthwise_constraint=None, pointwise_constraint=None, b_constraint=None, bias=True)  Separable convolution operator for 2D inputs.  Separable convolutions consist in first performing\na depthwise spatial convolution\n(which acts on each input channel separately)\nfollowed by a pointwise convolution which mixes together the resulting\noutput channels. The  depth_multiplier  argument controls how many\noutput channels are generated per input channel in the depthwise step.  Intuitively, separable convolutions can be understood as\na way to factorize a convolution kernel into two smaller kernels,\nor as an extreme version of an Inception block.  When using this layer as the first layer in a model,\nprovide the keyword argument  input_shape \n(tuple of integers, does not include the sample axis),\ne.g.  input_shape=(3, 128, 128)  for 128x128 RGB pictures.  Theano warning  This layer is only available with the\nTensorFlow backend for the time being.  Arguments   nb_filter : Number of convolution filters to use.  nb_row : Number of rows in the convolution kernel.  nb_col : Number of columns in the convolution kernel.  init : name of initialization function for the weights of the layer\n    (see  initializations ), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of numpy arrays to set as initial weights.  border_mode : 'valid' or 'same'.  subsample : tuple of length 2. Factor by which to subsample output.\n    Also called strides elsewhere.  depth_multiplier : how many output channel to use per input channel\n    for the depthwise convolution step.  depthwise_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the depthwise weights matrix.  pointwise_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the pointwise weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  depthwise_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the depthwise weights matrix.  pointwise_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the pointwise weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  4D tensor with shape: (samples, nb_filter, new_rows, new_cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, new_rows, new_cols, nb_filter)  if dim_ordering='tf'. rows  and  cols  values might have changed due to padding.   [source]", 
            "title": "SeparableConvolution2D"
        }, 
        {
            "location": "/layers/convolutional/#deconvolution2d", 
            "text": "keras.layers.convolutional.Deconvolution2D(nb_filter, nb_row, nb_col, output_shape, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)  Transposed convolution operator for filtering windows of 2-D inputs.  The need for transposed convolutions generally arises from the desire to\nuse a transformation going in the opposite direction\nof a normal convolution, i.e., from something that has the shape\nof the output of some convolution to something that has the shape\nof its input while maintaining a connectivity pattern\nthat is compatible with said convolution.  When using this layer as the first layer in a model,\nprovide the keyword argument  input_shape \n(tuple of integers, does not include the sample axis),\ne.g.  input_shape=(3, 128, 128)  for 128x128 RGB pictures.  To pass the correct  output_shape  to this layer,\none could use a test model to predict and observe the actual output shape.  Examples  # apply a 3x3 transposed convolution\n# with stride 1x1 and 3 output filters on a 12x12 image:\nmodel = Sequential()\nmodel.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 14, 14),\n              border_mode='valid',\n              input_shape=(3, 12, 12)))\n# Note that you will have to change\n# the output_shape depending on the backend used.\n\n# we can predict with the model and print the shape of the array.\ndummy_input = np.ones((32, 3, 12, 12))\n# For TensorFlow dummy_input = np.ones((32, 12, 12, 3))\npreds = model.predict(dummy_input)\nprint(preds.shape)\n# Theano GPU: (None, 3, 13, 13)\n# Theano CPU: (None, 3, 14, 14)\n# TensorFlow: (None, 14, 14, 3)\n\n# apply a 3x3 transposed convolution\n# with stride 2x2 and 3 output filters on a 12x12 image:\nmodel = Sequential()\nmodel.add(Deconvolution2D(3, 3, 3, output_shape=(None, 3, 25, 25),\n              subsample=(2, 2),\n              border_mode='valid',\n              input_shape=(3, 12, 12)))\nmodel.summary()\n\n# we can predict with the model and print the shape of the array.\ndummy_input = np.ones((32, 3, 12, 12))\n# For TensorFlow dummy_input = np.ones((32, 12, 12, 3))\npreds = model.predict(dummy_input)\nprint(preds.shape)\n# Theano GPU: (None, 3, 25, 25)\n# Theano CPU: (None, 3, 25, 25)\n# TensorFlow: (None, 25, 25, 3)  Arguments   nb_filter : Number of transposed convolution filters to use.  nb_row : Number of rows in the transposed convolution kernel.  nb_col : Number of columns in the transposed convolution kernel.  output_shape : Output shape of the transposed convolution operation.\n    tuple of integers\n     (nb_samples, nb_filter, nb_output_rows, nb_output_cols) .\n     It is better to use\n     a dummy input and observe the actual output shape of\n     a layer, as specified in the examples.  init : name of initialization function for the weights of the layer\n    (see  initializations ), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano/TensorFlow function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of numpy arrays to set as initial weights.  border_mode : 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).  subsample : tuple of length 2. Factor by which to oversample output.\n    Also called strides elsewhere.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  4D tensor with shape: (samples, nb_filter, new_rows, new_cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, new_rows, new_cols, nb_filter)  if dim_ordering='tf'. rows  and  cols  values might have changed due to padding.  References   A guide to convolution arithmetic for deep learning  Transposed convolution arithmetic  Deconvolutional Networks    [source]", 
            "title": "Deconvolution2D"
        }, 
        {
            "location": "/layers/convolutional/#convolution3d", 
            "text": "keras.layers.convolutional.Convolution3D(nb_filter, kernel_dim1, kernel_dim2, kernel_dim3, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)  Convolution operator for filtering windows of three-dimensional inputs.  When using this layer as the first layer in a model,\nprovide the keyword argument  input_shape \n(tuple of integers, does not include the sample axis),\ne.g.  input_shape=(3, 10, 128, 128)  for 10 frames of 128x128 RGB pictures.  Arguments   nb_filter : Number of convolution filters to use.  kernel_dim1 : Length of the first dimension in the convolution kernel.  kernel_dim2 : Length of the second dimension in the convolution kernel.  kernel_dim3 : Length of the third dimension in the convolution kernel.  init : name of initialization function for the weights of the layer\n    (see  initializations ), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of Numpy arrays to set as initial weights.  border_mode : 'valid', 'same' or 'full'\n    ('full' requires the Theano backend).  subsample : tuple of length 3. Factor by which to subsample output.\n    Also called strides elsewhere.  Note : 'subsample' is implemented by slicing\nthe output of conv3d with strides=(1,1,1).    W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".  bias : whether to include a bias\n    (i.e. make the layer affine rather than linear).   Input shape  5D tensor with shape: (samples, channels, conv_dim1, conv_dim2, conv_dim3)  if dim_ordering='th'\nor 5D tensor with shape: (samples, conv_dim1, conv_dim2, conv_dim3, channels)  if dim_ordering='tf'.  Output shape  5D tensor with shape: (samples, nb_filter, new_conv_dim1, new_conv_dim2, new_conv_dim3)  if dim_ordering='th'\nor 5D tensor with shape: (samples, new_conv_dim1, new_conv_dim2, new_conv_dim3, nb_filter)  if dim_ordering='tf'. new_conv_dim1 ,  new_conv_dim2  and  new_conv_dim3  values might have changed due to padding.   [source]", 
            "title": "Convolution3D"
        }, 
        {
            "location": "/layers/convolutional/#cropping1d", 
            "text": "keras.layers.convolutional.Cropping1D(cropping=(1, 1))  Cropping layer for 1D input (e.g. temporal sequence).  It crops along the time dimension (axis 1).  Arguments   cropping : tuple of int (length 2)\n    How many units should be trimmed off at the beginning and end of\n    the cropping dimension (axis 1).   Input shape  3D tensor with shape  (samples, axis_to_crop, features)  Output shape  3D tensor with shape  (samples, cropped_axis, features)   [source]", 
            "title": "Cropping1D"
        }, 
        {
            "location": "/layers/convolutional/#cropping2d", 
            "text": "keras.layers.convolutional.Cropping2D(cropping=((0, 0), (0, 0)), dim_ordering='default')  Cropping layer for 2D input (e.g. picture).  It crops along spatial dimensions, i.e. width and height.  Arguments   cropping : tuple of tuple of int (length 2)\n    How many units should be trimmed off at the beginning and end of\n    the 2 cropping dimensions (width, height).  dim_ordering : 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  4D tensor with shape: (samples, depth, first_axis_to_crop, second_axis_to_crop)  Output shape  4D tensor with shape: (samples, depth, first_cropped_axis, second_cropped_axis)  Examples  # Crop the input 2D images or feature maps\nmodel = Sequential()\nmodel.add(Cropping2D(cropping=((2, 2), (4, 4)), input_shape=(3, 28, 28)))\n# now model.output_shape == (None, 3, 24, 20)\nmodel.add(Convolution2D(64, 3, 3, border_mode='same))\nmodel.add(Cropping2D(cropping=((2, 2), (2, 2))))\n# now model.output_shape == (None, 64, 20, 16)   [source]", 
            "title": "Cropping2D"
        }, 
        {
            "location": "/layers/convolutional/#cropping3d", 
            "text": "keras.layers.convolutional.Cropping3D(cropping=((1, 1), (1, 1), (1, 1)), dim_ordering='default')  Cropping layer for 3D data (e.g. spatial or spatio-temporal).  Arguments   cropping : tuple of tuple of int (length 3)\n    How many units should be trimmed off at the beginning and end of\n    the 3 cropping dimensions (kernel_dim1, kernel_dim2, kernerl_dim3).  dim_ordering : 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  5D tensor with shape: (samples, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)  Output shape  5D tensor with shape: (samples, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)   [source]", 
            "title": "Cropping3D"
        }, 
        {
            "location": "/layers/convolutional/#upsampling1d", 
            "text": "keras.layers.convolutional.UpSampling1D(length=2)  Upsampling layer for 1D inputs.  Repeats each temporal step  length  times along the time axis.  Arguments   length : integer. Upsampling factor.   Input shape  3D tensor with shape:  (samples, steps, features) .  Output shape  3D tensor with shape:  (samples, upsampled_steps, features) .   [source]", 
            "title": "UpSampling1D"
        }, 
        {
            "location": "/layers/convolutional/#upsampling2d", 
            "text": "keras.layers.convolutional.UpSampling2D(size=(2, 2), dim_ordering='default')  Upsampling layer for 2D inputs.  Repeats the rows and columns of the data\nby size[0] and size[1] respectively.  Arguments   size : tuple of 2 integers. The upsampling factors for rows and columns.  dim_ordering : 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  4D tensor with shape: (samples, channels, upsampled_rows, upsampled_cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, upsampled_rows, upsampled_cols, channels)  if dim_ordering='tf'.   [source]", 
            "title": "UpSampling2D"
        }, 
        {
            "location": "/layers/convolutional/#upsampling3d", 
            "text": "keras.layers.convolutional.UpSampling3D(size=(2, 2, 2), dim_ordering='default')  Upsampling layer for 3D inputs.  Repeats the 1st, 2nd and 3rd dimensions\nof the data by size[0], size[1] and size[2] respectively.  Arguments   size : tuple of 3 integers. The upsampling factors for dim1, dim2 and dim3.  dim_ordering : 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  5D tensor with shape: (samples, channels, dim1, dim2, dim3)  if dim_ordering='th'\nor 5D tensor with shape: (samples, dim1, dim2, dim3, channels)  if dim_ordering='tf'.  Output shape  5D tensor with shape: (samples, channels, upsampled_dim1, upsampled_dim2, upsampled_dim3)  if dim_ordering='th'\nor 5D tensor with shape: (samples, upsampled_dim1, upsampled_dim2, upsampled_dim3, channels)  if dim_ordering='tf'.   [source]", 
            "title": "UpSampling3D"
        }, 
        {
            "location": "/layers/convolutional/#zeropadding1d", 
            "text": "keras.layers.convolutional.ZeroPadding1D(padding=1)  Zero-padding layer for 1D input (e.g. temporal sequence).  Arguments   padding : int, or tuple of int (length 2), or dictionary.  If int:\nHow many zeros to add at the beginning and end of\nthe padding dimension (axis 1).  If tuple of int (length 2)\nHow many zeros to add at the beginning and at the end of\nthe padding dimension, in order '(left_pad, right_pad)'.  If dictionary: should contain the keys\n{'left_pad', 'right_pad'}.\nIf any key is missing, default value of 0 will be used for the missing key.     Input shape  3D tensor with shape  (samples, axis_to_pad, features)  Output shape  3D tensor with shape  (samples, padded_axis, features)   [source]", 
            "title": "ZeroPadding1D"
        }, 
        {
            "location": "/layers/convolutional/#zeropadding2d", 
            "text": "keras.layers.convolutional.ZeroPadding2D(padding=(1, 1), dim_ordering='default')  Zero-padding layer for 2D input (e.g. picture).  Arguments   padding : tuple of int (length 2), or tuple of int (length 4), or dictionary.  If tuple of int (length 2):\nHow many zeros to add at the beginning and end of\nthe 2 padding dimensions (rows and cols).  If tuple of int (length 4):\nHow many zeros to add at the beginning and at the end of\nthe 2 padding dimensions (rows and cols), in the order\n'(top_pad, bottom_pad, left_pad, right_pad)'.  If dictionary: should contain the keys\n{'top_pad', 'bottom_pad', 'left_pad', 'right_pad'}.\nIf any key is missing, default value of 0 will be used for the missing key.    dim_ordering : 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  4D tensor with shape: (samples, channels, padded_rows, padded_cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, padded_rows, padded_cols, channels)  if dim_ordering='tf'.   [source]", 
            "title": "ZeroPadding2D"
        }, 
        {
            "location": "/layers/convolutional/#zeropadding3d", 
            "text": "keras.layers.convolutional.ZeroPadding3D(padding=(1, 1, 1), dim_ordering='default')  Zero-padding layer for 3D data (spatial or spatio-temporal).  Arguments   padding : tuple of int (length 3)\n    How many zeros to add at the beginning and end of\n    the 3 padding dimensions (axis 3, 4 and 5).\n    Currently only symmetric padding is supported.  dim_ordering : 'th' or 'tf'.\n    In 'th' mode, the channels dimension (the depth)\n    is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  5D tensor with shape: (samples, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)  Output shape  5D tensor with shape: (samples, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)", 
            "title": "ZeroPadding3D"
        }, 
        {
            "location": "/layers/pooling/", 
            "text": "[source]\n\n\nMaxPooling1D\n\n\nkeras.layers.pooling.MaxPooling1D(pool_length=2, stride=None, border_mode='valid')\n\n\n\n\nMax pooling operation for temporal data.\n\n\nInput shape\n\n\n3D tensor with shape: \n(samples, steps, features)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(samples, downsampled_steps, features)\n.\n\n\nArguments\n\n\n\n\npool_length\n: size of the region to which max pooling is applied\n\n\nstride\n: integer, or None. factor by which to downscale.\n    2 will halve the input.\n    If None, it will default to \npool_length\n.\n\n\nborder_mode\n: 'valid' or 'same'.\n\n\n\n\n\n\n[source]\n\n\nMaxPooling2D\n\n\nkeras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='default')\n\n\n\n\nMax pooling operation for spatial data.\n\n\nArguments\n\n\n\n\npool_size\n: tuple of 2 integers,\n    factors by which to downscale (vertical, horizontal).\n    (2, 2) will halve the image in each dimension.\n\n\nstrides\n: tuple of 2 integers, or None. Strides values.\n    If None, it will default to \npool_size\n.\n\n\nborder_mode\n: 'valid' or 'same'.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(nb_samples, channels, pooled_rows, pooled_cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, pooled_rows, pooled_cols, channels)\n if dim_ordering='tf'.\n\n\n\n\n[source]\n\n\nMaxPooling3D\n\n\nkeras.layers.pooling.MaxPooling3D(pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default')\n\n\n\n\nMax pooling operation for 3D data (spatial or spatio-temporal).\n\n\nArguments\n\n\n\n\npool_size\n: tuple of 3 integers,\n    factors by which to downscale (dim1, dim2, dim3).\n    (2, 2, 2) will halve the size of the 3D input in each dimension.\n\n\nstrides\n: tuple of 3 integers, or None. Strides values.\n\n\nborder_mode\n: 'valid' or 'same'.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n\n(samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)\n if dim_ordering='th'\nor 5D tensor with shape:\n\n(samples, len_pool_dim1, len_pool_dim2, len_pool_dim3, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n5D tensor with shape:\n\n(nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)\n if dim_ordering='th'\nor 5D tensor with shape:\n\n(samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)\n if dim_ordering='tf'.\n\n\n\n\n[source]\n\n\nAveragePooling1D\n\n\nkeras.layers.pooling.AveragePooling1D(pool_length=2, stride=None, border_mode='valid')\n\n\n\n\nAverage pooling for temporal data.\n\n\nArguments\n\n\n\n\npool_length\n: factor by which to downscale. 2 will halve the input.\n\n\nstride\n: integer, or None. Stride value.\n    If None, it will default to \npool_length\n.\n\n\nborder_mode\n: 'valid' or 'same'.\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(samples, steps, features)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(samples, downsampled_steps, features)\n.\n\n\n\n\n[source]\n\n\nAveragePooling2D\n\n\nkeras.layers.pooling.AveragePooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='default')\n\n\n\n\nAverage pooling operation for spatial data.\n\n\nArguments\n\n\n\n\npool_size\n: tuple of 2 integers,\n    factors by which to downscale (vertical, horizontal).\n    (2, 2) will halve the image in each dimension.\n\n\nstrides\n: tuple of 2 integers, or None. Strides values.\n    If None, it will default to \npool_size\n.\n\n\nborder_mode\n: 'valid' or 'same'.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(nb_samples, channels, pooled_rows, pooled_cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, pooled_rows, pooled_cols, channels)\n if dim_ordering='tf'.\n\n\n\n\n[source]\n\n\nAveragePooling3D\n\n\nkeras.layers.pooling.AveragePooling3D(pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default')\n\n\n\n\nAverage pooling operation for 3D data (spatial or spatio-temporal).\n\n\nArguments\n\n\n\n\npool_size\n: tuple of 3 integers,\n    factors by which to downscale (dim1, dim2, dim3).\n    (2, 2, 2) will halve the size of the 3D input in each dimension.\n\n\nstrides\n: tuple of 3 integers, or None. Strides values.\n\n\nborder_mode\n: 'valid' or 'same'.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n5D tensor with shape:\n\n(samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)\n if dim_ordering='th'\nor 5D tensor with shape:\n\n(samples, len_pool_dim1, len_pool_dim2, len_pool_dim3, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n5D tensor with shape:\n\n(nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)\n if dim_ordering='th'\nor 5D tensor with shape:\n\n(samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)\n if dim_ordering='tf'.\n\n\n\n\n[source]\n\n\nGlobalMaxPooling1D\n\n\nkeras.layers.pooling.GlobalMaxPooling1D()\n\n\n\n\nGlobal max pooling operation for temporal data.\n\n\nInput shape\n\n\n3D tensor with shape: \n(samples, steps, features)\n.\n\n\nOutput shape\n\n\n2D tensor with shape: \n(samples, features)\n.\n\n\n\n\n[source]\n\n\nGlobalAveragePooling1D\n\n\nkeras.layers.pooling.GlobalAveragePooling1D()\n\n\n\n\nGlobal average pooling operation for temporal data.\n\n\nInput shape\n\n\n3D tensor with shape: \n(samples, steps, features)\n.\n\n\nOutput shape\n\n\n2D tensor with shape: \n(samples, features)\n.\n\n\n\n\n[source]\n\n\nGlobalMaxPooling2D\n\n\nkeras.layers.pooling.GlobalMaxPooling2D(dim_ordering='default')\n\n\n\n\nGlobal max pooling operation for spatial data.\n\n\nArguments\n\n\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n2D tensor with shape:\n\n(nb_samples, channels)\n\n\n\n\n[source]\n\n\nGlobalAveragePooling2D\n\n\nkeras.layers.pooling.GlobalAveragePooling2D(dim_ordering='default')\n\n\n\n\nGlobal average pooling operation for spatial data.\n\n\nArguments\n\n\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n2D tensor with shape:\n\n(nb_samples, channels)", 
            "title": "Pooling Layers"
        }, 
        {
            "location": "/layers/pooling/#maxpooling1d", 
            "text": "keras.layers.pooling.MaxPooling1D(pool_length=2, stride=None, border_mode='valid')  Max pooling operation for temporal data.  Input shape  3D tensor with shape:  (samples, steps, features) .  Output shape  3D tensor with shape:  (samples, downsampled_steps, features) .  Arguments   pool_length : size of the region to which max pooling is applied  stride : integer, or None. factor by which to downscale.\n    2 will halve the input.\n    If None, it will default to  pool_length .  border_mode : 'valid' or 'same'.    [source]", 
            "title": "MaxPooling1D"
        }, 
        {
            "location": "/layers/pooling/#maxpooling2d", 
            "text": "keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='default')  Max pooling operation for spatial data.  Arguments   pool_size : tuple of 2 integers,\n    factors by which to downscale (vertical, horizontal).\n    (2, 2) will halve the image in each dimension.  strides : tuple of 2 integers, or None. Strides values.\n    If None, it will default to  pool_size .  border_mode : 'valid' or 'same'.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  4D tensor with shape: (nb_samples, channels, pooled_rows, pooled_cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, pooled_rows, pooled_cols, channels)  if dim_ordering='tf'.   [source]", 
            "title": "MaxPooling2D"
        }, 
        {
            "location": "/layers/pooling/#maxpooling3d", 
            "text": "keras.layers.pooling.MaxPooling3D(pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default')  Max pooling operation for 3D data (spatial or spatio-temporal).  Arguments   pool_size : tuple of 3 integers,\n    factors by which to downscale (dim1, dim2, dim3).\n    (2, 2, 2) will halve the size of the 3D input in each dimension.  strides : tuple of 3 integers, or None. Strides values.  border_mode : 'valid' or 'same'.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  5D tensor with shape: (samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)  if dim_ordering='th'\nor 5D tensor with shape: (samples, len_pool_dim1, len_pool_dim2, len_pool_dim3, channels)  if dim_ordering='tf'.  Output shape  5D tensor with shape: (nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)  if dim_ordering='th'\nor 5D tensor with shape: (samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)  if dim_ordering='tf'.   [source]", 
            "title": "MaxPooling3D"
        }, 
        {
            "location": "/layers/pooling/#averagepooling1d", 
            "text": "keras.layers.pooling.AveragePooling1D(pool_length=2, stride=None, border_mode='valid')  Average pooling for temporal data.  Arguments   pool_length : factor by which to downscale. 2 will halve the input.  stride : integer, or None. Stride value.\n    If None, it will default to  pool_length .  border_mode : 'valid' or 'same'.   Input shape  3D tensor with shape:  (samples, steps, features) .  Output shape  3D tensor with shape:  (samples, downsampled_steps, features) .   [source]", 
            "title": "AveragePooling1D"
        }, 
        {
            "location": "/layers/pooling/#averagepooling2d", 
            "text": "keras.layers.pooling.AveragePooling2D(pool_size=(2, 2), strides=None, border_mode='valid', dim_ordering='default')  Average pooling operation for spatial data.  Arguments   pool_size : tuple of 2 integers,\n    factors by which to downscale (vertical, horizontal).\n    (2, 2) will halve the image in each dimension.  strides : tuple of 2 integers, or None. Strides values.\n    If None, it will default to  pool_size .  border_mode : 'valid' or 'same'.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  4D tensor with shape: (nb_samples, channels, pooled_rows, pooled_cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, pooled_rows, pooled_cols, channels)  if dim_ordering='tf'.   [source]", 
            "title": "AveragePooling2D"
        }, 
        {
            "location": "/layers/pooling/#averagepooling3d", 
            "text": "keras.layers.pooling.AveragePooling3D(pool_size=(2, 2, 2), strides=None, border_mode='valid', dim_ordering='default')  Average pooling operation for 3D data (spatial or spatio-temporal).  Arguments   pool_size : tuple of 3 integers,\n    factors by which to downscale (dim1, dim2, dim3).\n    (2, 2, 2) will halve the size of the 3D input in each dimension.  strides : tuple of 3 integers, or None. Strides values.  border_mode : 'valid' or 'same'.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 4.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  5D tensor with shape: (samples, channels, len_pool_dim1, len_pool_dim2, len_pool_dim3)  if dim_ordering='th'\nor 5D tensor with shape: (samples, len_pool_dim1, len_pool_dim2, len_pool_dim3, channels)  if dim_ordering='tf'.  Output shape  5D tensor with shape: (nb_samples, channels, pooled_dim1, pooled_dim2, pooled_dim3)  if dim_ordering='th'\nor 5D tensor with shape: (samples, pooled_dim1, pooled_dim2, pooled_dim3, channels)  if dim_ordering='tf'.   [source]", 
            "title": "AveragePooling3D"
        }, 
        {
            "location": "/layers/pooling/#globalmaxpooling1d", 
            "text": "keras.layers.pooling.GlobalMaxPooling1D()  Global max pooling operation for temporal data.  Input shape  3D tensor with shape:  (samples, steps, features) .  Output shape  2D tensor with shape:  (samples, features) .   [source]", 
            "title": "GlobalMaxPooling1D"
        }, 
        {
            "location": "/layers/pooling/#globalaveragepooling1d", 
            "text": "keras.layers.pooling.GlobalAveragePooling1D()  Global average pooling operation for temporal data.  Input shape  3D tensor with shape:  (samples, steps, features) .  Output shape  2D tensor with shape:  (samples, features) .   [source]", 
            "title": "GlobalAveragePooling1D"
        }, 
        {
            "location": "/layers/pooling/#globalmaxpooling2d", 
            "text": "keras.layers.pooling.GlobalMaxPooling2D(dim_ordering='default')  Global max pooling operation for spatial data.  Arguments   dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  2D tensor with shape: (nb_samples, channels)   [source]", 
            "title": "GlobalMaxPooling2D"
        }, 
        {
            "location": "/layers/pooling/#globalaveragepooling2d", 
            "text": "keras.layers.pooling.GlobalAveragePooling2D(dim_ordering='default')  Global average pooling operation for spatial data.  Arguments   dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  2D tensor with shape: (nb_samples, channels)", 
            "title": "GlobalAveragePooling2D"
        }, 
        {
            "location": "/layers/local/", 
            "text": "[source]\n\n\nLocallyConnected1D\n\n\nkeras.layers.local.LocallyConnected1D(nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample_length=1, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, input_length=None)\n\n\n\n\nLocally-connected layer for 1D inputs.\n\n\nThe \nLocallyConnected1D\n layer works similarly to\nthe \nConvolution1D\n layer, except that weights are unshared,\nthat is, a different set of filters is applied at each different patch\nof the input.\nWhen using this layer as the first layer in a model,\neither provide the keyword argument \ninput_dim\n\n(int, e.g. 128 for sequences of 128-dimensional vectors), or \ninput_shape\n\n(tuple of integers, e.g. \ninput_shape=(10, 128)\n\nfor sequences of 10 vectors of 128-dimensional vectors).\nAlso, note that this layer can only be used with\na fully-specified input shape (\nNone\n dimensions not allowed).\n\n\nExample\n\n\n# apply a unshared weight convolution 1d of length 3 to a sequence with\n# 10 timesteps, with 64 output filters\nmodel = Sequential()\nmodel.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))\n# now model.output_shape == (None, 8, 64)\n# add a new conv1d on top\nmodel.add(LocallyConnected1D(32, 3))\n# now model.output_shape == (None, 6, 32)\n\n\n\n\nArguments\n\n\n\n\nnb_filter\n: Dimensionality of the output.\n\n\nfilter_length\n: The extension (spatial or temporal) of each filter.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n),\n    or alternatively, Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of numpy arrays to set as initial weights.\n\n\nborder_mode\n: Only support 'valid'. Please make good use of\n    ZeroPadding1D to achieve same output length.\n\n\nsubsample_length\n: factor by which to subsample output.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\nbias\n: whether to include a bias (i.e. make the layer affine rather than linear).\n\n\ninput_dim\n: Number of channels/dimensions in the input.\n    Either this argument or the keyword argument \ninput_shape\nmust be\n    provided when using this layer as the first layer in a model.\n\n\ninput_length\n: Length of input sequences, when it is constant.\n    This argument is required if you are going to connect\n    \nFlatten\n then \nDense\n layers upstream\n    (without it, the shape of the dense outputs cannot be computed).\n\n\n\n\nInput shape\n\n\n3D tensor with shape: \n(samples, steps, input_dim)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(samples, new_steps, nb_filter)\n.\n\nsteps\n value might have changed due to padding.\n\n\n\n\n[source]\n\n\nLocallyConnected2D\n\n\nkeras.layers.local.LocallyConnected2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)\n\n\n\n\nLocally-connected layer for 2D inputs.\n\n\nThe \nLocallyConnected2D\n layer works similarly\nto the \nConvolution2D\n layer, except that weights are unshared,\nthat is, a different set of filters is applied at each\ndifferent patch of the input.\nWhen using this layer as the\nfirst layer in a model, provide the keyword argument \ninput_shape\n (tuple\nof integers, does not include the sample axis), e.g.\n\ninput_shape=(3, 128, 128)\n for 128x128 RGB pictures.\nAlso, note that this layer can only be used with\na fully-specified input shape (\nNone\n dimensions not allowed).\n\n\nExamples\n\n\n# apply a 3x3 unshared weights convolution with 64 output filters on a 32x32 image:\nmodel = Sequential()\nmodel.add(LocallyConnected2D(64, 3, 3, input_shape=(3, 32, 32)))\n# now model.output_shape == (None, 64, 30, 30)\n# notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters\n\n# add a 3x3 unshared weights convolution on top, with 32 output filters:\nmodel.add(LocallyConnected2D(32, 3, 3))\n# now model.output_shape == (None, 32, 28, 28)\n\n\n\n\nArguments\n\n\n\n\nnb_filter\n: Number of convolution filters to use.\n\n\nnb_row\n: Number of rows in the convolution kernel.\n\n\nnb_col\n: Number of columns in the convolution kernel.\n\n\ninit\n: name of initialization function for the weights of the layer\n    (see \ninitializations\n), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a \nweights\n argument.\n\n\nactivation\n: name of activation function to use\n    (see \nactivations\n),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).\n\n\nweights\n: list of numpy arrays to set as initial weights.\n\n\nborder_mode\n: Only support 'valid'. Please make good use of\n    ZeroPadding2D to achieve same output shape.\n\n\nsubsample\n: tuple of length 2. Factor by which to subsample output.\n    Also called strides elsewhere.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the main weights matrix.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\nactivity_regularizer\n: instance of \nActivityRegularizer\n,\n    applied to the network output.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.\n\n\nb_constraint\n: instance of the \nconstraints\n module,\n    applied to the bias.\n\n\ndim_ordering\n: 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.\n\n\nbias\n: whether to include a bias (i.e. make the layer affine rather than linear).\n\n\n\n\nInput shape\n\n\n4D tensor with shape:\n\n(samples, channels, rows, cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, rows, cols, channels)\n if dim_ordering='tf'.\n\n\nOutput shape\n\n\n4D tensor with shape:\n\n(samples, nb_filter, new_rows, new_cols)\n if dim_ordering='th'\nor 4D tensor with shape:\n\n(samples, new_rows, new_cols, nb_filter)\n if dim_ordering='tf'.\n\nrows\n and \ncols\n values might have changed due to padding.", 
            "title": "Locally-connected Layers"
        }, 
        {
            "location": "/layers/local/#locallyconnected1d", 
            "text": "keras.layers.local.LocallyConnected1D(nb_filter, filter_length, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample_length=1, W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True, input_dim=None, input_length=None)  Locally-connected layer for 1D inputs.  The  LocallyConnected1D  layer works similarly to\nthe  Convolution1D  layer, except that weights are unshared,\nthat is, a different set of filters is applied at each different patch\nof the input.\nWhen using this layer as the first layer in a model,\neither provide the keyword argument  input_dim \n(int, e.g. 128 for sequences of 128-dimensional vectors), or  input_shape \n(tuple of integers, e.g.  input_shape=(10, 128) \nfor sequences of 10 vectors of 128-dimensional vectors).\nAlso, note that this layer can only be used with\na fully-specified input shape ( None  dimensions not allowed).  Example  # apply a unshared weight convolution 1d of length 3 to a sequence with\n# 10 timesteps, with 64 output filters\nmodel = Sequential()\nmodel.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))\n# now model.output_shape == (None, 8, 64)\n# add a new conv1d on top\nmodel.add(LocallyConnected1D(32, 3))\n# now model.output_shape == (None, 6, 32)  Arguments   nb_filter : Dimensionality of the output.  filter_length : The extension (spatial or temporal) of each filter.  init : name of initialization function for the weights of the layer\n    (see  initializations ),\n    or alternatively, Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of numpy arrays to set as initial weights.  border_mode : Only support 'valid'. Please make good use of\n    ZeroPadding1D to achieve same output length.  subsample_length : factor by which to subsample output.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  bias : whether to include a bias (i.e. make the layer affine rather than linear).  input_dim : Number of channels/dimensions in the input.\n    Either this argument or the keyword argument  input_shape must be\n    provided when using this layer as the first layer in a model.  input_length : Length of input sequences, when it is constant.\n    This argument is required if you are going to connect\n     Flatten  then  Dense  layers upstream\n    (without it, the shape of the dense outputs cannot be computed).   Input shape  3D tensor with shape:  (samples, steps, input_dim) .  Output shape  3D tensor with shape:  (samples, new_steps, nb_filter) . steps  value might have changed due to padding.   [source]", 
            "title": "LocallyConnected1D"
        }, 
        {
            "location": "/layers/local/#locallyconnected2d", 
            "text": "keras.layers.local.LocallyConnected2D(nb_filter, nb_row, nb_col, init='glorot_uniform', activation=None, weights=None, border_mode='valid', subsample=(1, 1), dim_ordering='default', W_regularizer=None, b_regularizer=None, activity_regularizer=None, W_constraint=None, b_constraint=None, bias=True)  Locally-connected layer for 2D inputs.  The  LocallyConnected2D  layer works similarly\nto the  Convolution2D  layer, except that weights are unshared,\nthat is, a different set of filters is applied at each\ndifferent patch of the input.\nWhen using this layer as the\nfirst layer in a model, provide the keyword argument  input_shape  (tuple\nof integers, does not include the sample axis), e.g. input_shape=(3, 128, 128)  for 128x128 RGB pictures.\nAlso, note that this layer can only be used with\na fully-specified input shape ( None  dimensions not allowed).  Examples  # apply a 3x3 unshared weights convolution with 64 output filters on a 32x32 image:\nmodel = Sequential()\nmodel.add(LocallyConnected2D(64, 3, 3, input_shape=(3, 32, 32)))\n# now model.output_shape == (None, 64, 30, 30)\n# notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters\n\n# add a 3x3 unshared weights convolution on top, with 32 output filters:\nmodel.add(LocallyConnected2D(32, 3, 3))\n# now model.output_shape == (None, 32, 28, 28)  Arguments   nb_filter : Number of convolution filters to use.  nb_row : Number of rows in the convolution kernel.  nb_col : Number of columns in the convolution kernel.  init : name of initialization function for the weights of the layer\n    (see  initializations ), or alternatively,\n    Theano function to use for weights initialization.\n    This parameter is only relevant if you don't pass\n    a  weights  argument.  activation : name of activation function to use\n    (see  activations ),\n    or alternatively, elementwise Theano function.\n    If you don't specify anything, no activation is applied\n    (ie. \"linear\" activation: a(x) = x).  weights : list of numpy arrays to set as initial weights.  border_mode : Only support 'valid'. Please make good use of\n    ZeroPadding2D to achieve same output shape.  subsample : tuple of length 2. Factor by which to subsample output.\n    Also called strides elsewhere.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the main weights matrix.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  activity_regularizer : instance of  ActivityRegularizer ,\n    applied to the network output.  W_constraint : instance of the  constraints  module\n    (eg. maxnorm, nonneg), applied to the main weights matrix.  b_constraint : instance of the  constraints  module,\n    applied to the bias.  dim_ordering : 'th' or 'tf'. In 'th' mode, the channels dimension\n    (the depth) is at index 1, in 'tf' mode is it at index 3.  bias : whether to include a bias (i.e. make the layer affine rather than linear).   Input shape  4D tensor with shape: (samples, channels, rows, cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, rows, cols, channels)  if dim_ordering='tf'.  Output shape  4D tensor with shape: (samples, nb_filter, new_rows, new_cols)  if dim_ordering='th'\nor 4D tensor with shape: (samples, new_rows, new_cols, nb_filter)  if dim_ordering='tf'. rows  and  cols  values might have changed due to padding.", 
            "title": "LocallyConnected2D"
        }, 
        {
            "location": "/layers/recurrent/", 
            "text": "[source]\n\n\nRecurrent\n\n\nkeras.layers.recurrent.Recurrent(weights=None, return_sequences=False, go_backwards=False, stateful=False, unroll=False, consume_less='cpu', input_dim=None, input_length=None)\n\n\n\n\nAbstract base class for recurrent layers.\nDo not use in a model -- it's not a valid layer!\nUse its children classes \nLSTM\n, \nGRU\n and \nSimpleRNN\n instead.\n\n\nAll recurrent layers (\nLSTM\n, \nGRU\n, \nSimpleRNN\n) also\nfollow the specifications of this class and accept\nthe keyword arguments listed below.\n\n\nExample\n\n\n# as the first layer in a Sequential model\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(10, 64)))\n# now model.output_shape == (None, 32)\n# note: `None` is the batch dimension.\n\n# the following is identical:\nmodel = Sequential()\nmodel.add(LSTM(32, input_dim=64, input_length=10))\n\n# for subsequent layers, not need to specify the input size:\nmodel.add(LSTM(16))\n\n\n\n\nArguments\n\n\n\n\nweights\n: list of Numpy arrays to set as initial weights.\n    The list should have 3 elements, of shapes:\n    \n[(input_dim, output_dim), (output_dim, output_dim), (output_dim,)]\n.\n\n\nreturn_sequences\n: Boolean. Whether to return the last output\n    in the output sequence, or the full sequence.\n\n\ngo_backwards\n: Boolean (default False).\n    If True, process the input sequence backwards.\n\n\nstateful\n: Boolean (default False). If True, the last state\n    for each sample at index i in a batch will be used as initial\n    state for the sample of index i in the following batch.\n\n\nunroll\n: Boolean (default False). If True, the network will be unrolled,\n    else a symbolic loop will be used. When using TensorFlow, the network\n    is always unrolled, so this argument does not do anything.\n    Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n    Unrolling is only suitable for short sequences.\n\n\nconsume_less\n: one of \"cpu\", \"mem\", or \"gpu\" (LSTM/GRU only).\n    If set to \"cpu\", the RNN will use\n    an implementation that uses fewer, larger matrix products,\n    thus running faster on CPU but consuming more memory.\n    If set to \"mem\", the RNN will use more matrix products,\n    but smaller ones, thus running slower (may actually be faster on GPU)\n    while consuming less memory.\n    If set to \"gpu\" (LSTM/GRU only), the RNN will combine the input gate,\n    the forget gate and the output gate into a single matrix,\n    enabling more time-efficient parallelization on the GPU. Note: RNN\n    dropout must be shared for all gates, resulting in a slightly\n    reduced regularization.\n\n\ninput_dim\n: dimensionality of the input (integer).\n    This argument (or alternatively, the keyword argument \ninput_shape\n)\n    is required when using this layer as the first layer in a model.\n\n\ninput_length\n: Length of input sequences, to be specified\n    when it is constant.\n    This argument is required if you are going to connect\n    \nFlatten\n then \nDense\n layers upstream\n    (without it, the shape of the dense outputs cannot be computed).\n    Note that if the recurrent layer is not the first layer\n    in your model, you would need to specify the input length\n    at the level of the first layer\n    (e.g. via the \ninput_shape\n argument)\n\n\n\n\nInput shape\n\n\n3D tensor with shape \n(nb_samples, timesteps, input_dim)\n.\n\n\nOutput shape\n\n\n\n\nif \nreturn_sequences\n: 3D tensor with shape\n    \n(nb_samples, timesteps, output_dim)\n.\n\n\nelse, 2D tensor with shape \n(nb_samples, output_dim)\n.\n\n\n\n\nMasking\n\n\nThis layer supports masking for input data with a variable number\nof timesteps. To introduce masks to your data,\nuse an \nEmbedding\n layer with the \nmask_zero\n parameter\nset to \nTrue\n.\n\n\nNote on performance\n\n\nYou are likely to see better performance with RNNs in Theano compared\nto TensorFlow. Additionally, when using TensorFlow, it is often\npreferable to set \nunroll=True\n for better performance.\n\n\nNote on using statefulness in RNNs\n\n\nYou can set RNN layers to be 'stateful', which means that the states\ncomputed for the samples in one batch will be reused as initial states\nfor the samples in the next batch. This assumes a one-to-one mapping\nbetween samples in different successive batches.\n\n\nTo enable statefulness:\n    - specify \nstateful=True\n in the layer constructor.\n    - specify a fixed batch size for your model, by passing\n    if sequential model:\n      \nbatch_input_shape=(...)\n to the first layer in your model.\n    else for functional model with 1 or more Input layers:\n      \nbatch_shape=(...)\n to all the first layers in your model.\n    This is the expected shape of your inputs \nincluding the batch size\n.\n    It should be a tuple of integers, e.g. \n(32, 10, 100)\n.\n    - specify \nshuffle=False\n when calling fit().\n\n\nTo reset the states of your model, call \n.reset_states()\n on either\na specific layer, or on your entire model.\n\n\n\n\n[source]\n\n\nSimpleRNN\n\n\nkeras.layers.recurrent.SimpleRNN(output_dim, init='glorot_uniform', inner_init='orthogonal', activation='tanh', W_regularizer=None, U_regularizer=None, b_regularizer=None, dropout_W=0.0, dropout_U=0.0)\n\n\n\n\nFully-connected RNN where the output is to be fed back to input.\n\n\nArguments\n\n\n\n\noutput_dim\n: dimension of the internal projections and the final output.\n\n\ninit\n: weight initialization function.\n    Can be the name of an existing function (str),\n    or a Theano function (see: \ninitializations\n).\n\n\ninner_init\n: initialization function of the inner cells.\n\n\nactivation\n: activation function.\n    Can be the name of an existing function (str),\n    or a Theano function (see: \nactivations\n).\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nU_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\ndropout_W\n: float between 0 and 1. Fraction of the input units to drop for input gates.\n\n\ndropout_U\n: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n\n\n\nReferences\n\n\n\n\nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks\n\n\n\n\n\n\n[source]\n\n\nGRU\n\n\nkeras.layers.recurrent.GRU(output_dim, init='glorot_uniform', inner_init='orthogonal', activation='tanh', inner_activation='hard_sigmoid', W_regularizer=None, U_regularizer=None, b_regularizer=None, dropout_W=0.0, dropout_U=0.0)\n\n\n\n\nGated Recurrent Unit - Cho et al. 2014.\n\n\nArguments\n\n\n\n\noutput_dim\n: dimension of the internal projections and the final output.\n\n\ninit\n: weight initialization function.\n    Can be the name of an existing function (str),\n    or a Theano function (see: \ninitializations\n).\n\n\ninner_init\n: initialization function of the inner cells.\n\n\nactivation\n: activation function.\n    Can be the name of an existing function (str),\n    or a Theano function (see: \nactivations\n).\n\n\ninner_activation\n: activation function for the inner cells.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nU_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\ndropout_W\n: float between 0 and 1. Fraction of the input units to drop for input gates.\n\n\ndropout_U\n: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n\n\n\nReferences\n\n\n\n\nOn the Properties of Neural Machine Translation: Encoder-Decoder Approaches\n\n\nEmpirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n\n\nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks\n\n\n\n\n\n\n[source]\n\n\nLSTM\n\n\nkeras.layers.recurrent.LSTM(output_dim, init='glorot_uniform', inner_init='orthogonal', forget_bias_init='one', activation='tanh', inner_activation='hard_sigmoid', W_regularizer=None, U_regularizer=None, b_regularizer=None, dropout_W=0.0, dropout_U=0.0)\n\n\n\n\nLong-Short Term Memory unit - Hochreiter 1997.\n\n\nFor a step-by-step description of the algorithm, see\n\nthis tutorial\n.\n\n\nArguments\n\n\n\n\noutput_dim\n: dimension of the internal projections and the final output.\n\n\ninit\n: weight initialization function.\n    Can be the name of an existing function (str),\n    or a Theano function (see: \ninitializations\n).\n\n\ninner_init\n: initialization function of the inner cells.\n\n\nforget_bias_init\n: initialization function for the bias of the forget gate.\n    \nJozefowicz et al.\n\n    recommend initializing with ones.\n\n\nactivation\n: activation function.\n    Can be the name of an existing function (str),\n    or a Theano function (see: \nactivations\n).\n\n\ninner_activation\n: activation function for the inner cells.\n\n\nW_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the input weights matrices.\n\n\nU_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n\n\nb_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the bias.\n\n\ndropout_W\n: float between 0 and 1. Fraction of the input units to drop for input gates.\n\n\ndropout_U\n: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n\n\n\n\nReferences\n\n\n\n\nLong short-term memory\n (original 1997 paper)\n\n\nLearning to forget: Continual prediction with LSTM\n\n\nSupervised sequence labeling with recurrent neural networks\n\n\nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks", 
            "title": "Recurrent Layers"
        }, 
        {
            "location": "/layers/recurrent/#recurrent", 
            "text": "keras.layers.recurrent.Recurrent(weights=None, return_sequences=False, go_backwards=False, stateful=False, unroll=False, consume_less='cpu', input_dim=None, input_length=None)  Abstract base class for recurrent layers.\nDo not use in a model -- it's not a valid layer!\nUse its children classes  LSTM ,  GRU  and  SimpleRNN  instead.  All recurrent layers ( LSTM ,  GRU ,  SimpleRNN ) also\nfollow the specifications of this class and accept\nthe keyword arguments listed below.  Example  # as the first layer in a Sequential model\nmodel = Sequential()\nmodel.add(LSTM(32, input_shape=(10, 64)))\n# now model.output_shape == (None, 32)\n# note: `None` is the batch dimension.\n\n# the following is identical:\nmodel = Sequential()\nmodel.add(LSTM(32, input_dim=64, input_length=10))\n\n# for subsequent layers, not need to specify the input size:\nmodel.add(LSTM(16))  Arguments   weights : list of Numpy arrays to set as initial weights.\n    The list should have 3 elements, of shapes:\n     [(input_dim, output_dim), (output_dim, output_dim), (output_dim,)] .  return_sequences : Boolean. Whether to return the last output\n    in the output sequence, or the full sequence.  go_backwards : Boolean (default False).\n    If True, process the input sequence backwards.  stateful : Boolean (default False). If True, the last state\n    for each sample at index i in a batch will be used as initial\n    state for the sample of index i in the following batch.  unroll : Boolean (default False). If True, the network will be unrolled,\n    else a symbolic loop will be used. When using TensorFlow, the network\n    is always unrolled, so this argument does not do anything.\n    Unrolling can speed-up a RNN, although it tends to be more memory-intensive.\n    Unrolling is only suitable for short sequences.  consume_less : one of \"cpu\", \"mem\", or \"gpu\" (LSTM/GRU only).\n    If set to \"cpu\", the RNN will use\n    an implementation that uses fewer, larger matrix products,\n    thus running faster on CPU but consuming more memory.\n    If set to \"mem\", the RNN will use more matrix products,\n    but smaller ones, thus running slower (may actually be faster on GPU)\n    while consuming less memory.\n    If set to \"gpu\" (LSTM/GRU only), the RNN will combine the input gate,\n    the forget gate and the output gate into a single matrix,\n    enabling more time-efficient parallelization on the GPU. Note: RNN\n    dropout must be shared for all gates, resulting in a slightly\n    reduced regularization.  input_dim : dimensionality of the input (integer).\n    This argument (or alternatively, the keyword argument  input_shape )\n    is required when using this layer as the first layer in a model.  input_length : Length of input sequences, to be specified\n    when it is constant.\n    This argument is required if you are going to connect\n     Flatten  then  Dense  layers upstream\n    (without it, the shape of the dense outputs cannot be computed).\n    Note that if the recurrent layer is not the first layer\n    in your model, you would need to specify the input length\n    at the level of the first layer\n    (e.g. via the  input_shape  argument)   Input shape  3D tensor with shape  (nb_samples, timesteps, input_dim) .  Output shape   if  return_sequences : 3D tensor with shape\n     (nb_samples, timesteps, output_dim) .  else, 2D tensor with shape  (nb_samples, output_dim) .   Masking  This layer supports masking for input data with a variable number\nof timesteps. To introduce masks to your data,\nuse an  Embedding  layer with the  mask_zero  parameter\nset to  True .  Note on performance  You are likely to see better performance with RNNs in Theano compared\nto TensorFlow. Additionally, when using TensorFlow, it is often\npreferable to set  unroll=True  for better performance.  Note on using statefulness in RNNs  You can set RNN layers to be 'stateful', which means that the states\ncomputed for the samples in one batch will be reused as initial states\nfor the samples in the next batch. This assumes a one-to-one mapping\nbetween samples in different successive batches.  To enable statefulness:\n    - specify  stateful=True  in the layer constructor.\n    - specify a fixed batch size for your model, by passing\n    if sequential model:\n       batch_input_shape=(...)  to the first layer in your model.\n    else for functional model with 1 or more Input layers:\n       batch_shape=(...)  to all the first layers in your model.\n    This is the expected shape of your inputs  including the batch size .\n    It should be a tuple of integers, e.g.  (32, 10, 100) .\n    - specify  shuffle=False  when calling fit().  To reset the states of your model, call  .reset_states()  on either\na specific layer, or on your entire model.   [source]", 
            "title": "Recurrent"
        }, 
        {
            "location": "/layers/recurrent/#simplernn", 
            "text": "keras.layers.recurrent.SimpleRNN(output_dim, init='glorot_uniform', inner_init='orthogonal', activation='tanh', W_regularizer=None, U_regularizer=None, b_regularizer=None, dropout_W=0.0, dropout_U=0.0)  Fully-connected RNN where the output is to be fed back to input.  Arguments   output_dim : dimension of the internal projections and the final output.  init : weight initialization function.\n    Can be the name of an existing function (str),\n    or a Theano function (see:  initializations ).  inner_init : initialization function of the inner cells.  activation : activation function.\n    Can be the name of an existing function (str),\n    or a Theano function (see:  activations ).  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the input weights matrices.  U_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the recurrent weights matrices.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  dropout_W : float between 0 and 1. Fraction of the input units to drop for input gates.  dropout_U : float between 0 and 1. Fraction of the input units to drop for recurrent connections.   References   A Theoretically Grounded Application of Dropout in Recurrent Neural Networks    [source]", 
            "title": "SimpleRNN"
        }, 
        {
            "location": "/layers/recurrent/#gru", 
            "text": "keras.layers.recurrent.GRU(output_dim, init='glorot_uniform', inner_init='orthogonal', activation='tanh', inner_activation='hard_sigmoid', W_regularizer=None, U_regularizer=None, b_regularizer=None, dropout_W=0.0, dropout_U=0.0)  Gated Recurrent Unit - Cho et al. 2014.  Arguments   output_dim : dimension of the internal projections and the final output.  init : weight initialization function.\n    Can be the name of an existing function (str),\n    or a Theano function (see:  initializations ).  inner_init : initialization function of the inner cells.  activation : activation function.\n    Can be the name of an existing function (str),\n    or a Theano function (see:  activations ).  inner_activation : activation function for the inner cells.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the input weights matrices.  U_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the recurrent weights matrices.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  dropout_W : float between 0 and 1. Fraction of the input units to drop for input gates.  dropout_U : float between 0 and 1. Fraction of the input units to drop for recurrent connections.   References   On the Properties of Neural Machine Translation: Encoder-Decoder Approaches  Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling  A Theoretically Grounded Application of Dropout in Recurrent Neural Networks    [source]", 
            "title": "GRU"
        }, 
        {
            "location": "/layers/recurrent/#lstm", 
            "text": "keras.layers.recurrent.LSTM(output_dim, init='glorot_uniform', inner_init='orthogonal', forget_bias_init='one', activation='tanh', inner_activation='hard_sigmoid', W_regularizer=None, U_regularizer=None, b_regularizer=None, dropout_W=0.0, dropout_U=0.0)  Long-Short Term Memory unit - Hochreiter 1997.  For a step-by-step description of the algorithm, see this tutorial .  Arguments   output_dim : dimension of the internal projections and the final output.  init : weight initialization function.\n    Can be the name of an existing function (str),\n    or a Theano function (see:  initializations ).  inner_init : initialization function of the inner cells.  forget_bias_init : initialization function for the bias of the forget gate.\n     Jozefowicz et al. \n    recommend initializing with ones.  activation : activation function.\n    Can be the name of an existing function (str),\n    or a Theano function (see:  activations ).  inner_activation : activation function for the inner cells.  W_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the input weights matrices.  U_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the recurrent weights matrices.  b_regularizer : instance of  WeightRegularizer ,\n    applied to the bias.  dropout_W : float between 0 and 1. Fraction of the input units to drop for input gates.  dropout_U : float between 0 and 1. Fraction of the input units to drop for recurrent connections.   References   Long short-term memory  (original 1997 paper)  Learning to forget: Continual prediction with LSTM  Supervised sequence labeling with recurrent neural networks  A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", 
            "title": "LSTM"
        }, 
        {
            "location": "/layers/embeddings/", 
            "text": "[source]\n\n\nEmbedding\n\n\nkeras.layers.embeddings.Embedding(input_dim, output_dim, init='uniform', input_length=None, W_regularizer=None, activity_regularizer=None, W_constraint=None, mask_zero=False, weights=None, dropout=0.0)\n\n\n\n\nTurn positive integers (indexes) into dense vectors of fixed size.\neg. [[4], [20]] -\n [[0.25, 0.1], [0.6, -0.2]]\n\n\nThis layer can only be used as the first layer in a model.\n\n\nExample\n\n\n  model = Sequential()\n  model.add(Embedding(1000, 64, input_length=10))\n  # the model will take as input an integer matrix of size (batch, input_length).\n  # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n  # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n\n  input_array = np.random.randint(1000, size=(32, 10))\n\n  model.compile('rmsprop', 'mse')\n  output_array = model.predict(input_array)\n  assert output_array.shape == (32, 10, 64)\n\n\n\n\nArguments\n\n\n\n\ninput_dim\n: int \n 0. Size of the vocabulary, ie.\n  1 + maximum integer index occurring in the input data.\n\n\noutput_dim\n: int \n= 0. Dimension of the dense embedding.\n\n\ninit\n: name of initialization function for the weights\n  of the layer (see: \ninitializations\n),\n  or alternatively, Theano function to use for weights initialization.\n  This parameter is only relevant if you don't pass a \nweights\n argument.\n\n\nweights\n: list of Numpy arrays to set as initial weights.\n  The list should have 1 element, of shape \n(input_dim, output_dim)\n.\n\n\nW_regularizer\n: instance of the \nregularizers\n module\n(eg. L1 or L2 regularization), applied to the embedding matrix.\n\n\nW_constraint\n: instance of the \nconstraints\n module\n  (eg. maxnorm, nonneg), applied to the embedding matrix.\n\n\nmask_zero\n: Whether or not the input value 0 is a special \"padding\"\n  value that should be masked out.\n  This is useful for \nrecurrent layers\n which may take\n  variable length input. If this is \nTrue\n then all subsequent layers\n  in the model need to support masking or an exception will be raised.\n  If mask_zero is set to True, as a consequence, index 0 cannot be\n  used in the vocabulary (input_dim should equal |vocabulary| + 2).\n\n\ninput_length\n: Length of input sequences, when it is constant.\n  This argument is required if you are going to connect\n  \nFlatten\n then \nDense\n layers upstream\n  (without it, the shape of the dense outputs cannot be computed).\n\n\ndropout\n: float between 0 and 1. Fraction of the embeddings to drop.\n\n\n\n\nInput shape\n\n\n2D tensor with shape: \n(nb_samples, sequence_length)\n.\n\n\nOutput shape\n\n\n3D tensor with shape: \n(nb_samples, sequence_length, output_dim)\n.\n\n\nReferences\n\n\n\n\nA Theoretically Grounded Application of Dropout in Recurrent Neural Networks", 
            "title": "Embedding Layers"
        }, 
        {
            "location": "/layers/embeddings/#embedding", 
            "text": "keras.layers.embeddings.Embedding(input_dim, output_dim, init='uniform', input_length=None, W_regularizer=None, activity_regularizer=None, W_constraint=None, mask_zero=False, weights=None, dropout=0.0)  Turn positive integers (indexes) into dense vectors of fixed size.\neg. [[4], [20]] -  [[0.25, 0.1], [0.6, -0.2]]  This layer can only be used as the first layer in a model.  Example    model = Sequential()\n  model.add(Embedding(1000, 64, input_length=10))\n  # the model will take as input an integer matrix of size (batch, input_length).\n  # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n  # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n\n  input_array = np.random.randint(1000, size=(32, 10))\n\n  model.compile('rmsprop', 'mse')\n  output_array = model.predict(input_array)\n  assert output_array.shape == (32, 10, 64)  Arguments   input_dim : int   0. Size of the vocabulary, ie.\n  1 + maximum integer index occurring in the input data.  output_dim : int  = 0. Dimension of the dense embedding.  init : name of initialization function for the weights\n  of the layer (see:  initializations ),\n  or alternatively, Theano function to use for weights initialization.\n  This parameter is only relevant if you don't pass a  weights  argument.  weights : list of Numpy arrays to set as initial weights.\n  The list should have 1 element, of shape  (input_dim, output_dim) .  W_regularizer : instance of the  regularizers  module\n(eg. L1 or L2 regularization), applied to the embedding matrix.  W_constraint : instance of the  constraints  module\n  (eg. maxnorm, nonneg), applied to the embedding matrix.  mask_zero : Whether or not the input value 0 is a special \"padding\"\n  value that should be masked out.\n  This is useful for  recurrent layers  which may take\n  variable length input. If this is  True  then all subsequent layers\n  in the model need to support masking or an exception will be raised.\n  If mask_zero is set to True, as a consequence, index 0 cannot be\n  used in the vocabulary (input_dim should equal |vocabulary| + 2).  input_length : Length of input sequences, when it is constant.\n  This argument is required if you are going to connect\n   Flatten  then  Dense  layers upstream\n  (without it, the shape of the dense outputs cannot be computed).  dropout : float between 0 and 1. Fraction of the embeddings to drop.   Input shape  2D tensor with shape:  (nb_samples, sequence_length) .  Output shape  3D tensor with shape:  (nb_samples, sequence_length, output_dim) .  References   A Theoretically Grounded Application of Dropout in Recurrent Neural Networks", 
            "title": "Embedding"
        }, 
        {
            "location": "/layers/advanced-activations/", 
            "text": "[source]\n\n\nLeakyReLU\n\n\nkeras.layers.advanced_activations.LeakyReLU(alpha=0.3)\n\n\n\n\nLeaky version of a Rectified Linear Unit.\n\n\nIt allows a small gradient when the unit is not active:\n\nf(x) = alpha * x for x \n 0\n,\n\nf(x) = x for x \n= 0\n.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\nalpha\n: float \n= 0. Negative slope coefficient.\n\n\n\n\nReferences\n\n\n\n\nRectifier Nonlinearities Improve Neural Network Acoustic Models\n\n\n\n\n\n\n[source]\n\n\nPReLU\n\n\nkeras.layers.advanced_activations.PReLU(init='zero', weights=None, shared_axes=None)\n\n\n\n\nParametric Rectified Linear Unit.\n\n\nIt follows:\n\nf(x) = alphas * x for x \n 0\n,\n\nf(x) = x for x \n= 0\n,\nwhere \nalphas\n is a learned array with the same shape as x.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\ninit\n: initialization function for the weights.\n\n\nweights\n: initial weights, as a list of a single Numpy array.\n\n\nshared_axes\n: the axes along which to share learnable\n    parameters for the activation function.\n    For example, if the incoming feature maps\n    are from a 2D convolution\n    with output shape \n(batch, height, width, channels)\n,\n    and you wish to share parameters across space\n    so that each filter only has one set of parameters,\n    set \nshared_axes=[1, 2]\n.\n\n\n\n\nReferences\n\n\n\n\nDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification\n\n\n\n\n\n\n[source]\n\n\nELU\n\n\nkeras.layers.advanced_activations.ELU(alpha=1.0)\n\n\n\n\nExponential Linear Unit.\n\n\nIt follows:\n\nf(x) =  alpha * (exp(x) - 1.) for x \n 0\n,\n\nf(x) = x for x \n= 0\n.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\nalpha\n: scale for the negative factor.\n\n\n\n\nReferences\n\n\n\n\nFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n\n\n\n\n\n\n[source]\n\n\nParametricSoftplus\n\n\nkeras.layers.advanced_activations.ParametricSoftplus(alpha_init=0.2, beta_init=5.0, weights=None, shared_axes=None)\n\n\n\n\nParametric Softplus.\n\n\nIt follows:\n\nf(x) = alpha * log(1 + exp(beta * x))\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\nalpha_init\n: float. Initial value of the alpha weights.\n\n\nbeta_init\n: float. Initial values of the beta weights.\n\n\nweights\n: initial weights, as a list of 2 numpy arrays.\n\n\nshared_axes\n: the axes along which to share learnable\n    parameters for the activation function.\n    For example, if the incoming feature maps\n    are from a 2D convolution\n    with output shape \n(batch, height, width, channels)\n,\n    and you wish to share parameters across space\n    so that each filter only has one set of parameters,\n    set \nshared_axes=[1, 2]\n.\n\n\n\n\nReferences\n\n\n\n\nInferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs\n\n\n\n\n\n\n[source]\n\n\nThresholdedReLU\n\n\nkeras.layers.advanced_activations.ThresholdedReLU(theta=1.0)\n\n\n\n\nThresholded Rectified Linear Unit.\n\n\nIt follows:\n\nf(x) = x for x \n theta\n,\n\nf(x) = 0 otherwise\n.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\ntheta\n: float \n= 0. Threshold location of activation.\n\n\n\n\nReferences\n\n\n\n\nZero-Bias Autoencoders and the Benefits of Co-Adapting Features\n\n\n\n\n\n\n[source]\n\n\nSReLU\n\n\nkeras.layers.advanced_activations.SReLU(t_left_init='zero', a_left_init='glorot_uniform', t_right_init='glorot_uniform', a_right_init='one', shared_axes=None)\n\n\n\n\nS-shaped Rectified Linear Unit.\n\n\nIt follows:\n\nf(x) = t^r + a^r(x - t^r) for x \n= t^r\n,\n\nf(x) = x for t^r \n x \n t^l\n,\n\nf(x) = t^l + a^l(x - t^l) for x \n= t^l\n.\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as the input.\n\n\nArguments\n\n\n\n\nt_left_init\n: initialization function for the left part intercept\n\n\na_left_init\n: initialization function for the left part slope\n\n\nt_right_init\n: initialization function for the right part intercept\n\n\na_right_init\n: initialization function for the right part slope\n\n\nshared_axes\n: the axes along which to share learnable\n    parameters for the activation function.\n    For example, if the incoming feature maps\n    are from a 2D convolution\n    with output shape \n(batch, height, width, channels)\n,\n    and you wish to share parameters across space\n    so that each filter only has one set of parameters,\n    set \nshared_axes=[1, 2]\n.\n\n\n\n\nReferences\n\n\n\n\nDeep Learning with S-shaped Rectified Linear Activation Units", 
            "title": "Advanced Activations Layers"
        }, 
        {
            "location": "/layers/advanced-activations/#leakyrelu", 
            "text": "keras.layers.advanced_activations.LeakyReLU(alpha=0.3)  Leaky version of a Rectified Linear Unit.  It allows a small gradient when the unit is not active: f(x) = alpha * x for x   0 , f(x) = x for x  = 0 .  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   alpha : float  = 0. Negative slope coefficient.   References   Rectifier Nonlinearities Improve Neural Network Acoustic Models    [source]", 
            "title": "LeakyReLU"
        }, 
        {
            "location": "/layers/advanced-activations/#prelu", 
            "text": "keras.layers.advanced_activations.PReLU(init='zero', weights=None, shared_axes=None)  Parametric Rectified Linear Unit.  It follows: f(x) = alphas * x for x   0 , f(x) = x for x  = 0 ,\nwhere  alphas  is a learned array with the same shape as x.  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   init : initialization function for the weights.  weights : initial weights, as a list of a single Numpy array.  shared_axes : the axes along which to share learnable\n    parameters for the activation function.\n    For example, if the incoming feature maps\n    are from a 2D convolution\n    with output shape  (batch, height, width, channels) ,\n    and you wish to share parameters across space\n    so that each filter only has one set of parameters,\n    set  shared_axes=[1, 2] .   References   Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification    [source]", 
            "title": "PReLU"
        }, 
        {
            "location": "/layers/advanced-activations/#elu", 
            "text": "keras.layers.advanced_activations.ELU(alpha=1.0)  Exponential Linear Unit.  It follows: f(x) =  alpha * (exp(x) - 1.) for x   0 , f(x) = x for x  = 0 .  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   alpha : scale for the negative factor.   References   Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)    [source]", 
            "title": "ELU"
        }, 
        {
            "location": "/layers/advanced-activations/#parametricsoftplus", 
            "text": "keras.layers.advanced_activations.ParametricSoftplus(alpha_init=0.2, beta_init=5.0, weights=None, shared_axes=None)  Parametric Softplus.  It follows: f(x) = alpha * log(1 + exp(beta * x))  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   alpha_init : float. Initial value of the alpha weights.  beta_init : float. Initial values of the beta weights.  weights : initial weights, as a list of 2 numpy arrays.  shared_axes : the axes along which to share learnable\n    parameters for the activation function.\n    For example, if the incoming feature maps\n    are from a 2D convolution\n    with output shape  (batch, height, width, channels) ,\n    and you wish to share parameters across space\n    so that each filter only has one set of parameters,\n    set  shared_axes=[1, 2] .   References   Inferring Nonlinear Neuronal Computation Based on Physiologically Plausible Inputs    [source]", 
            "title": "ParametricSoftplus"
        }, 
        {
            "location": "/layers/advanced-activations/#thresholdedrelu", 
            "text": "keras.layers.advanced_activations.ThresholdedReLU(theta=1.0)  Thresholded Rectified Linear Unit.  It follows: f(x) = x for x   theta , f(x) = 0 otherwise .  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   theta : float  = 0. Threshold location of activation.   References   Zero-Bias Autoencoders and the Benefits of Co-Adapting Features    [source]", 
            "title": "ThresholdedReLU"
        }, 
        {
            "location": "/layers/advanced-activations/#srelu", 
            "text": "keras.layers.advanced_activations.SReLU(t_left_init='zero', a_left_init='glorot_uniform', t_right_init='glorot_uniform', a_right_init='one', shared_axes=None)  S-shaped Rectified Linear Unit.  It follows: f(x) = t^r + a^r(x - t^r) for x  = t^r , f(x) = x for t^r   x   t^l , f(x) = t^l + a^l(x - t^l) for x  = t^l .  Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as the input.  Arguments   t_left_init : initialization function for the left part intercept  a_left_init : initialization function for the left part slope  t_right_init : initialization function for the right part intercept  a_right_init : initialization function for the right part slope  shared_axes : the axes along which to share learnable\n    parameters for the activation function.\n    For example, if the incoming feature maps\n    are from a 2D convolution\n    with output shape  (batch, height, width, channels) ,\n    and you wish to share parameters across space\n    so that each filter only has one set of parameters,\n    set  shared_axes=[1, 2] .   References   Deep Learning with S-shaped Rectified Linear Activation Units", 
            "title": "SReLU"
        }, 
        {
            "location": "/layers/normalization/", 
            "text": "[source]\n\n\nBatchNormalization\n\n\nkeras.layers.normalization.BatchNormalization(epsilon=0.001, mode=0, axis=-1, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)\n\n\n\n\nBatch normalization layer (Ioffe and Szegedy, 2014).\n\n\nNormalize the activations of the previous layer at each batch,\ni.e. applies a transformation that maintains the mean activation\nclose to 0 and the activation standard deviation close to 1.\n\n\nArguments\n\n\n\n\nepsilon\n: small float \n 0. Fuzz parameter.\n    Theano expects epsilon \n= 1e-5.\n\n\nmode\n: integer, 0, 1 or 2.\n\n\n0: feature-wise normalization.\nEach feature map in the input will\nbe normalized separately. The axis on which\nto normalize is specified by the \naxis\n argument.\nNote that if the input is a 4D image tensor\nusing Theano conventions (samples, channels, rows, cols)\nthen you should set \naxis\n to \n1\n to normalize along\nthe channels axis.\nDuring training we use per-batch statistics to normalize\nthe data, and during testing we use running averages\ncomputed during the training phase.\n\n\n1: sample-wise normalization. This mode assumes a 2D input.\n\n\n2: feature-wise normalization, like mode 0, but\nusing per-batch statistics to normalize the data during both\ntesting and training.\n\n\n\n\n\n\naxis\n: integer, axis along which to normalize in mode 0. For instance,\n    if your input tensor has shape (samples, channels, rows, cols),\n    set axis to 1 to normalize per feature map (channels axis).\n\n\nmomentum\n: momentum in the computation of the\n    exponential average of the mean and standard deviation\n    of the data, for feature-wise normalization.\n\n\nweights\n: Initialization weights.\n    List of 2 Numpy arrays, with shapes:\n    \n[(input_shape,), (input_shape,)]\n\n    Note that the order of this list is [gamma, beta, mean, std]\n\n\nbeta_init\n: name of initialization function for shift parameter\n    (see \ninitializations\n), or alternatively,\n    Theano/TensorFlow function to use for weights initialization.\n    This parameter is only relevant if you don't pass a \nweights\n argument.\n\n\ngamma_init\n: name of initialization function for scale parameter (see\n    \ninitializations\n), or alternatively,\n    Theano/TensorFlow function to use for weights initialization.\n    This parameter is only relevant if you don't pass a \nweights\n argument.\n\n\ngamma_regularizer\n: instance of \nWeightRegularizer\n\n    (eg. L1 or L2 regularization), applied to the gamma vector.\n\n\nbeta_regularizer\n: instance of \nWeightRegularizer\n,\n    applied to the beta vector.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\nReferences\n\n\n\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", 
            "title": "Normalization Layers"
        }, 
        {
            "location": "/layers/normalization/#batchnormalization", 
            "text": "keras.layers.normalization.BatchNormalization(epsilon=0.001, mode=0, axis=-1, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)  Batch normalization layer (Ioffe and Szegedy, 2014).  Normalize the activations of the previous layer at each batch,\ni.e. applies a transformation that maintains the mean activation\nclose to 0 and the activation standard deviation close to 1.  Arguments   epsilon : small float   0. Fuzz parameter.\n    Theano expects epsilon  = 1e-5.  mode : integer, 0, 1 or 2.  0: feature-wise normalization.\nEach feature map in the input will\nbe normalized separately. The axis on which\nto normalize is specified by the  axis  argument.\nNote that if the input is a 4D image tensor\nusing Theano conventions (samples, channels, rows, cols)\nthen you should set  axis  to  1  to normalize along\nthe channels axis.\nDuring training we use per-batch statistics to normalize\nthe data, and during testing we use running averages\ncomputed during the training phase.  1: sample-wise normalization. This mode assumes a 2D input.  2: feature-wise normalization, like mode 0, but\nusing per-batch statistics to normalize the data during both\ntesting and training.    axis : integer, axis along which to normalize in mode 0. For instance,\n    if your input tensor has shape (samples, channels, rows, cols),\n    set axis to 1 to normalize per feature map (channels axis).  momentum : momentum in the computation of the\n    exponential average of the mean and standard deviation\n    of the data, for feature-wise normalization.  weights : Initialization weights.\n    List of 2 Numpy arrays, with shapes:\n     [(input_shape,), (input_shape,)] \n    Note that the order of this list is [gamma, beta, mean, std]  beta_init : name of initialization function for shift parameter\n    (see  initializations ), or alternatively,\n    Theano/TensorFlow function to use for weights initialization.\n    This parameter is only relevant if you don't pass a  weights  argument.  gamma_init : name of initialization function for scale parameter (see\n     initializations ), or alternatively,\n    Theano/TensorFlow function to use for weights initialization.\n    This parameter is only relevant if you don't pass a  weights  argument.  gamma_regularizer : instance of  WeightRegularizer \n    (eg. L1 or L2 regularization), applied to the gamma vector.  beta_regularizer : instance of  WeightRegularizer ,\n    applied to the beta vector.   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.  References   Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", 
            "title": "BatchNormalization"
        }, 
        {
            "location": "/layers/noise/", 
            "text": "[source]\n\n\nGaussianNoise\n\n\nkeras.layers.noise.GaussianNoise(sigma)\n\n\n\n\nApply additive zero-centered Gaussian noise.\n\n\nThis is useful to mitigate overfitting\n(you could see it as a form of random data augmentation).\nGaussian Noise (GS) is a natural choice as corruption process\nfor real valued inputs.\n\n\nAs it is a regularization layer, it is only active at training time.\n\n\nArguments\n\n\n\n\nsigma\n: float, standard deviation of the noise distribution.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\n\n\n[source]\n\n\nGaussianDropout\n\n\nkeras.layers.noise.GaussianDropout(p)\n\n\n\n\nApply multiplicative 1-centered Gaussian noise.\n\n\nAs it is a regularization layer, it is only active at training time.\n\n\nArguments\n\n\n\n\np\n: float, drop probability (as with \nDropout\n).\n    The multiplicative noise will have\n    standard deviation \nsqrt(p / (1 - p))\n.\n\n\n\n\nInput shape\n\n\nArbitrary. Use the keyword argument \ninput_shape\n\n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.\n\n\nOutput shape\n\n\nSame shape as input.\n\n\nReferences\n\n\n\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014", 
            "title": "Noise layers"
        }, 
        {
            "location": "/layers/noise/#gaussiannoise", 
            "text": "keras.layers.noise.GaussianNoise(sigma)  Apply additive zero-centered Gaussian noise.  This is useful to mitigate overfitting\n(you could see it as a form of random data augmentation).\nGaussian Noise (GS) is a natural choice as corruption process\nfor real valued inputs.  As it is a regularization layer, it is only active at training time.  Arguments   sigma : float, standard deviation of the noise distribution.   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.   [source]", 
            "title": "GaussianNoise"
        }, 
        {
            "location": "/layers/noise/#gaussiandropout", 
            "text": "keras.layers.noise.GaussianDropout(p)  Apply multiplicative 1-centered Gaussian noise.  As it is a regularization layer, it is only active at training time.  Arguments   p : float, drop probability (as with  Dropout ).\n    The multiplicative noise will have\n    standard deviation  sqrt(p / (1 - p)) .   Input shape  Arbitrary. Use the keyword argument  input_shape \n(tuple of integers, does not include the samples axis)\nwhen using this layer as the first layer in a model.  Output shape  Same shape as input.  References   Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014", 
            "title": "GaussianDropout"
        }, 
        {
            "location": "/layers/wrappers/", 
            "text": "[source]\n\n\nTimeDistributed\n\n\nkeras.layers.wrappers.TimeDistributed(layer)\n\n\n\n\nThis wrapper allows to apply a layer to every temporal slice of an input.\n\n\nThe input should be at least 3D, and the dimension of index one\nwill be considered to be the temporal dimension.\n\n\nConsider a batch of 32 samples,\nwhere each sample is a sequence of 10 vectors of 16 dimensions.\nThe batch input shape of the layer is then \n(32, 10, 16)\n,\nand the \ninput_shape\n, not including the samples dimension, is \n(10, 16)\n.\n\n\nYou can then use \nTimeDistributed\n to apply a \nDense\n layer\nto each of the 10 timesteps, independently:\n\n\n# as the first layer in a model\nmodel = Sequential()\nmodel.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n# now model.output_shape == (None, 10, 8)\n\n# subsequent layers: no need for input_shape\nmodel.add(TimeDistributed(Dense(32)))\n# now model.output_shape == (None, 10, 32)\n\n\n\n\nThe output will then have shape \n(32, 10, 8)\n.\n\n\nTimeDistributed\n can be used with arbitrary layers, not just \nDense\n,\nfor instance with a \nConvolution2D\n layer:\n\n\nmodel = Sequential()\nmodel.add(TimeDistributed(Convolution2D(64, 3, 3),\n              input_shape=(10, 3, 299, 299)))\n\n\n\n\nArguments\n\n\n\n\nlayer\n: a layer instance.\n\n\n\n\n\n\n[source]\n\n\nBidirectional\n\n\nkeras.layers.wrappers.Bidirectional(layer, merge_mode='concat', weights=None)\n\n\n\n\nBidirectional wrapper for RNNs.\n\n\nArguments\n\n\n\n\nlayer\n: \nRecurrent\n instance.\n\n\nmerge_mode\n: Mode by which outputs of the\n    forward and backward RNNs will be combined.\n    One of {'sum', 'mul', 'concat', 'ave', None}.\n    If None, the outputs will not be combined,\n    they will be returned as a list.\n\n\n\n\nExamples\n\n\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))\nmodel.add(Bidirectional(LSTM(10)))\nmodel.add(Dense(5))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')", 
            "title": "Layer wrappers"
        }, 
        {
            "location": "/layers/wrappers/#timedistributed", 
            "text": "keras.layers.wrappers.TimeDistributed(layer)  This wrapper allows to apply a layer to every temporal slice of an input.  The input should be at least 3D, and the dimension of index one\nwill be considered to be the temporal dimension.  Consider a batch of 32 samples,\nwhere each sample is a sequence of 10 vectors of 16 dimensions.\nThe batch input shape of the layer is then  (32, 10, 16) ,\nand the  input_shape , not including the samples dimension, is  (10, 16) .  You can then use  TimeDistributed  to apply a  Dense  layer\nto each of the 10 timesteps, independently:  # as the first layer in a model\nmodel = Sequential()\nmodel.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n# now model.output_shape == (None, 10, 8)\n\n# subsequent layers: no need for input_shape\nmodel.add(TimeDistributed(Dense(32)))\n# now model.output_shape == (None, 10, 32)  The output will then have shape  (32, 10, 8) .  TimeDistributed  can be used with arbitrary layers, not just  Dense ,\nfor instance with a  Convolution2D  layer:  model = Sequential()\nmodel.add(TimeDistributed(Convolution2D(64, 3, 3),\n              input_shape=(10, 3, 299, 299)))  Arguments   layer : a layer instance.    [source]", 
            "title": "TimeDistributed"
        }, 
        {
            "location": "/layers/wrappers/#bidirectional", 
            "text": "keras.layers.wrappers.Bidirectional(layer, merge_mode='concat', weights=None)  Bidirectional wrapper for RNNs.  Arguments   layer :  Recurrent  instance.  merge_mode : Mode by which outputs of the\n    forward and backward RNNs will be combined.\n    One of {'sum', 'mul', 'concat', 'ave', None}.\n    If None, the outputs will not be combined,\n    they will be returned as a list.   Examples  model = Sequential()\nmodel.add(Bidirectional(LSTM(10, return_sequences=True), input_shape=(5, 10)))\nmodel.add(Bidirectional(LSTM(10)))\nmodel.add(Dense(5))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')", 
            "title": "Bidirectional"
        }, 
        {
            "location": "/layers/writing-your-own-keras-layers/", 
            "text": "Writing your own Keras layers\n\n\nFor simple, stateless custom operations, you are probably better off using \nlayers.core.Lambda\n layers. But for any custom operation that has trainable weights, you should implement your own layer.\n\n\nHere is the skeleton of a Keras layer. There are only three methods you need to implement:\n\n\n\n\nbuild(input_shape)\n: this is where you will define your weights. Trainable weights should be added to the list \nself.trainable_weights\n. Other attributes of note are: \nself.non_trainable_weights\n (list) and \nself.updates\n (list of update tuples (tensor, new_tensor)). For an example of how to use \nnon_trainable_weights\n and \nupdates\n, see the code for the \nBatchNormalization\n layer.\n\n\ncall(x)\n: this is where the layer's logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to \ncall\n: the input tensor.\n\n\nget_output_shape_for(input_shape)\n: in case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference.\n\n\n\n\nfrom keras import backend as K\nfrom keras.engine.topology import Layer\nimport numpy as np\n\nclass MyLayer(Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(MyLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        input_dim = input_shape[1]\n        initial_weight_value = np.random.random((input_dim, output_dim))\n        self.W = K.variable(initial_weight_value)\n        self.trainable_weights = [self.W]\n\n    def call(self, x, mask=None):\n        return K.dot(x, self.W)\n\n    def get_output_shape_for(self, input_shape):\n        return (input_shape[0], self.output_dim)\n\n\n\n\nThe existing Keras layers provide ample examples of how to implement almost anything. Never hesitate to read the source code!", 
            "title": "Writing your own Keras layers"
        }, 
        {
            "location": "/layers/writing-your-own-keras-layers/#writing-your-own-keras-layers", 
            "text": "For simple, stateless custom operations, you are probably better off using  layers.core.Lambda  layers. But for any custom operation that has trainable weights, you should implement your own layer.  Here is the skeleton of a Keras layer. There are only three methods you need to implement:   build(input_shape) : this is where you will define your weights. Trainable weights should be added to the list  self.trainable_weights . Other attributes of note are:  self.non_trainable_weights  (list) and  self.updates  (list of update tuples (tensor, new_tensor)). For an example of how to use  non_trainable_weights  and  updates , see the code for the  BatchNormalization  layer.  call(x) : this is where the layer's logic lives. Unless you want your layer to support masking, you only have to care about the first argument passed to  call : the input tensor.  get_output_shape_for(input_shape) : in case your layer modifies the shape of its input, you should specify here the shape transformation logic. This allows Keras to do automatic shape inference.   from keras import backend as K\nfrom keras.engine.topology import Layer\nimport numpy as np\n\nclass MyLayer(Layer):\n    def __init__(self, output_dim, **kwargs):\n        self.output_dim = output_dim\n        super(MyLayer, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        input_dim = input_shape[1]\n        initial_weight_value = np.random.random((input_dim, output_dim))\n        self.W = K.variable(initial_weight_value)\n        self.trainable_weights = [self.W]\n\n    def call(self, x, mask=None):\n        return K.dot(x, self.W)\n\n    def get_output_shape_for(self, input_shape):\n        return (input_shape[0], self.output_dim)  The existing Keras layers provide ample examples of how to implement almost anything. Never hesitate to read the source code!", 
            "title": "Writing your own Keras layers"
        }, 
        {
            "location": "/preprocessing/sequence/", 
            "text": "pad_sequences\n\n\nkeras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32')\n\n\n\n\nTransform a list of \nnb_samples sequences\n (lists of scalars) into a 2D Numpy array of shape \n(nb_samples, nb_timesteps)\n. \nnb_timesteps\n is either the \nmaxlen\n argument if provided, or the length of the longest sequence otherwise. Sequences that are shorter than \nnb_timesteps\n are padded with zeros at the end.\n\n\n\n\n\n\nReturn\n: 2D Numpy array of shape \n(nb_samples, nb_timesteps)\n.\n\n\n\n\n\n\nArguments\n:\n\n\n\n\nsequences\n: List of lists of int or float.\n\n\nmaxlen\n: None or int. Maximum sequence length, longer sequences are truncated and shorter sequences are padded with zeros at the end.\n\n\ndtype\n: datatype of the Numpy array returned.\n\n\npadding\n: 'pre' or 'post', pad either before or after each sequence.\n\n\ntruncating\n: 'pre' or 'post', remove values from sequences larger than maxlen either in the beginning or in the end of the sequence\n\n\nvalue\n: float, value to pad the sequences to the desired value.\n\n\n\n\n\n\n\n\n\n\nskipgrams\n\n\nkeras.preprocessing.sequence.skipgrams(sequence, vocabulary_size,\n    window_size=4, negative_samples=1., shuffle=True,\n    categorical=False, sampling_table=None)\n\n\n\n\nTransforms a sequence of word indexes (list of int) into couples of the form:\n\n\n\n\n(word, word in the same window), with label 1 (positive samples).\n\n\n(word, random word from the vocabulary), with label 0 (negative samples).\n\n\n\n\nRead more about Skipgram in this gnomic paper by Mikolov et al.: \nEfficient Estimation of Word Representations in\nVector Space\n\n\n\n\n\n\nReturn\n: tuple \n(couples, labels)\n.\n\n\n\n\ncouples\n is a list of 2-elements lists of int: \n[word_index, other_word_index]\n.\n\n\nlabels\n is a list of 0 and 1, where 1 indicates that \nother_word_index\n was found in the same window as \nword_index\n, and 0 indicates that \nother_word_index\n was random.\n\n\nif categorical is set to True, the labels are categorical, ie. 1 becomes [0,1], and 0 becomes [1, 0].\n\n\n\n\n\n\n\n\nArguments\n:\n\n\n\n\nsequence\n: list of int indexes. If using a sampling_table, the index of a word should be its the rank in the dataset (starting at 1).\n\n\nvocabulary_size\n: int.\n\n\nwindow_size\n: int. maximum distance between two words in a positive couple.\n\n\nnegative_samples\n: float \n= 0. 0 for no negative (=random) samples. 1 for same number as positive samples. etc.\n\n\nshuffle\n: boolean. Whether to shuffle the samples.\n\n\ncategorical\n: boolean. Whether to make the returned labels categorical.\n\n\nsampling_table\n: Numpy array of shape \n(vocabulary_size,)\n where \nsampling_table[i]\n is the probability of sampling the word with index i (assumed to be i-th most common word in the dataset).\n\n\n\n\n\n\n\n\n\n\nmake_sampling_table\n\n\nkeras.preprocessing.sequence.make_sampling_table(size, sampling_factor=1e-5)\n\n\n\n\nUsed for generating the \nsampling_table\n argument for \nskipgrams\n. \nsampling_table[i]\n is the probability of sampling the word i-th most common word in a dataset (more common words should be sampled less frequently, for balance).\n\n\n\n\n\n\nReturn\n: Numpy array of shape \n(size,)\n.\n\n\n\n\n\n\nArguments\n:\n\n\n\n\nsize\n: size of the vocabulary considered.\n\n\nsampling_factor\n: lower values result in a longer probability decay (common words will be sampled less frequently). If set to 1, no subsampling will be performed (all sampling probabilities will be 1).", 
            "title": "Sequence Preprocessing"
        }, 
        {
            "location": "/preprocessing/sequence/#pad_sequences", 
            "text": "keras.preprocessing.sequence.pad_sequences(sequences, maxlen=None, dtype='int32')  Transform a list of  nb_samples sequences  (lists of scalars) into a 2D Numpy array of shape  (nb_samples, nb_timesteps) .  nb_timesteps  is either the  maxlen  argument if provided, or the length of the longest sequence otherwise. Sequences that are shorter than  nb_timesteps  are padded with zeros at the end.    Return : 2D Numpy array of shape  (nb_samples, nb_timesteps) .    Arguments :   sequences : List of lists of int or float.  maxlen : None or int. Maximum sequence length, longer sequences are truncated and shorter sequences are padded with zeros at the end.  dtype : datatype of the Numpy array returned.  padding : 'pre' or 'post', pad either before or after each sequence.  truncating : 'pre' or 'post', remove values from sequences larger than maxlen either in the beginning or in the end of the sequence  value : float, value to pad the sequences to the desired value.", 
            "title": "pad_sequences"
        }, 
        {
            "location": "/preprocessing/sequence/#skipgrams", 
            "text": "keras.preprocessing.sequence.skipgrams(sequence, vocabulary_size,\n    window_size=4, negative_samples=1., shuffle=True,\n    categorical=False, sampling_table=None)  Transforms a sequence of word indexes (list of int) into couples of the form:   (word, word in the same window), with label 1 (positive samples).  (word, random word from the vocabulary), with label 0 (negative samples).   Read more about Skipgram in this gnomic paper by Mikolov et al.:  Efficient Estimation of Word Representations in\nVector Space    Return : tuple  (couples, labels) .   couples  is a list of 2-elements lists of int:  [word_index, other_word_index] .  labels  is a list of 0 and 1, where 1 indicates that  other_word_index  was found in the same window as  word_index , and 0 indicates that  other_word_index  was random.  if categorical is set to True, the labels are categorical, ie. 1 becomes [0,1], and 0 becomes [1, 0].     Arguments :   sequence : list of int indexes. If using a sampling_table, the index of a word should be its the rank in the dataset (starting at 1).  vocabulary_size : int.  window_size : int. maximum distance between two words in a positive couple.  negative_samples : float  = 0. 0 for no negative (=random) samples. 1 for same number as positive samples. etc.  shuffle : boolean. Whether to shuffle the samples.  categorical : boolean. Whether to make the returned labels categorical.  sampling_table : Numpy array of shape  (vocabulary_size,)  where  sampling_table[i]  is the probability of sampling the word with index i (assumed to be i-th most common word in the dataset).", 
            "title": "skipgrams"
        }, 
        {
            "location": "/preprocessing/sequence/#make_sampling_table", 
            "text": "keras.preprocessing.sequence.make_sampling_table(size, sampling_factor=1e-5)  Used for generating the  sampling_table  argument for  skipgrams .  sampling_table[i]  is the probability of sampling the word i-th most common word in a dataset (more common words should be sampled less frequently, for balance).    Return : Numpy array of shape  (size,) .    Arguments :   size : size of the vocabulary considered.  sampling_factor : lower values result in a longer probability decay (common words will be sampled less frequently). If set to 1, no subsampling will be performed (all sampling probabilities will be 1).", 
            "title": "make_sampling_table"
        }, 
        {
            "location": "/preprocessing/text/", 
            "text": "text_to_word_sequence\n\n\nkeras.preprocessing.text.text_to_word_sequence(text, \n    filters=base_filter(), lower=True, split=\n \n)\n\n\n\n\nSplit a sentence into a list of words.\n\n\n\n\n\n\nReturn\n: List of words (str).\n\n\n\n\n\n\nArguments\n:\n\n\n\n\ntext\n: str.\n\n\nfilters\n: list (or concatenation) of characters to filter out, such as punctuation. Default: base_filter(), includes basic punctuation, tabs, and newlines.\n\n\nlower\n: boolean. Whether to set the text to lowercase.\n\n\nsplit\n: str. Separator for word splitting.\n\n\n\n\n\n\n\n\none_hot\n\n\nkeras.preprocessing.text.one_hot(text, n,\n    filters=base_filter(), lower=True, split=\n \n)\n\n\n\n\nOne-hot encode a text into a list of word indexes in a vocabulary of size n.\n\n\n\n\n\n\nReturn\n: List of integers in [1, n]. Each integer encodes a word (unicity non-guaranteed).\n\n\n\n\n\n\nArguments\n: Same as \ntext_to_word_sequence\n above.\n\n\n\n\nn\n: int. Size of vocabulary.\n\n\n\n\n\n\n\n\nTokenizer\n\n\nkeras.preprocessing.text.Tokenizer(nb_words=None, filters=base_filter(), \n    lower=True, split=\n \n)\n\n\n\n\nClass for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n\n\n\n\n\n\nArguments\n: Same as \ntext_to_word_sequence\n above.\n\n\n\n\nnb_words\n: None or int. Maximum number of words to work with (if set, tokenization will be restricted to the top nb_words most common words in the dataset).\n\n\n\n\n\n\n\n\nMethods\n:\n\n\n\n\n\n\nfit_on_texts(texts)\n: \n\n\n\n\nArguments\n:\n\n\ntexts\n: list of texts to train on.\n\n\n\n\n\n\n\n\n\n\n\n\ntexts_to_sequences(texts)\n\n\n\n\nArguments\n: \n\n\ntexts\n: list of texts to turn to sequences.\n\n\n\n\n\n\nReturn\n: list of sequences (one per text input).\n\n\n\n\n\n\n\n\ntexts_to_sequences_generator(texts)\n: generator version of the above. \n\n\n\n\nReturn\n: yield one sequence per input text.\n\n\n\n\n\n\n\n\ntexts_to_matrix(texts)\n:\n\n\n\n\nReturn\n: numpy array of shape \n(len(texts), nb_words)\n.\n\n\nArguments\n:\n\n\ntexts\n: list of texts to vectorize.\n\n\nmode\n: one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\").\n\n\n\n\n\n\n\n\n\n\n\n\nfit_on_sequences(sequences)\n: \n\n\n\n\nArguments\n:\n\n\nsequences\n: list of sequences to train on. \n\n\n\n\n\n\n\n\n\n\n\n\nsequences_to_matrix(sequences)\n:\n\n\n\n\nReturn\n: numpy array of shape \n(len(sequences), nb_words)\n.\n\n\nArguments\n:\n\n\nsequences\n: list of sequences to vectorize.\n\n\nmode\n: one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\").\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAttributes\n:\n\n\n\n\nword_counts\n: dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called. \n\n\nword_docs\n: dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.\n\n\nword_index\n: dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.\n\n\ndocument_count\n: int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.", 
            "title": "Text Preprocessing"
        }, 
        {
            "location": "/preprocessing/text/#text_to_word_sequence", 
            "text": "keras.preprocessing.text.text_to_word_sequence(text, \n    filters=base_filter(), lower=True, split=   )  Split a sentence into a list of words.    Return : List of words (str).    Arguments :   text : str.  filters : list (or concatenation) of characters to filter out, such as punctuation. Default: base_filter(), includes basic punctuation, tabs, and newlines.  lower : boolean. Whether to set the text to lowercase.  split : str. Separator for word splitting.", 
            "title": "text_to_word_sequence"
        }, 
        {
            "location": "/preprocessing/text/#one_hot", 
            "text": "keras.preprocessing.text.one_hot(text, n,\n    filters=base_filter(), lower=True, split=   )  One-hot encode a text into a list of word indexes in a vocabulary of size n.    Return : List of integers in [1, n]. Each integer encodes a word (unicity non-guaranteed).    Arguments : Same as  text_to_word_sequence  above.   n : int. Size of vocabulary.", 
            "title": "one_hot"
        }, 
        {
            "location": "/preprocessing/text/#tokenizer", 
            "text": "keras.preprocessing.text.Tokenizer(nb_words=None, filters=base_filter(), \n    lower=True, split=   )  Class for vectorizing texts, or/and turning texts into sequences (=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).    Arguments : Same as  text_to_word_sequence  above.   nb_words : None or int. Maximum number of words to work with (if set, tokenization will be restricted to the top nb_words most common words in the dataset).     Methods :    fit_on_texts(texts) :    Arguments :  texts : list of texts to train on.       texts_to_sequences(texts)   Arguments :   texts : list of texts to turn to sequences.    Return : list of sequences (one per text input).     texts_to_sequences_generator(texts) : generator version of the above.    Return : yield one sequence per input text.     texts_to_matrix(texts) :   Return : numpy array of shape  (len(texts), nb_words) .  Arguments :  texts : list of texts to vectorize.  mode : one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\").       fit_on_sequences(sequences) :    Arguments :  sequences : list of sequences to train on.        sequences_to_matrix(sequences) :   Return : numpy array of shape  (len(sequences), nb_words) .  Arguments :  sequences : list of sequences to vectorize.  mode : one of \"binary\", \"count\", \"tfidf\", \"freq\" (default: \"binary\").         Attributes :   word_counts : dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called.   word_docs : dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.  word_index : dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.  document_count : int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.", 
            "title": "Tokenizer"
        }, 
        {
            "location": "/preprocessing/image/", 
            "text": "ImageDataGenerator\n\n\nkeras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=0.,\n    width_shift_range=0.,\n    height_shift_range=0.,\n    shear_range=0.,\n    zoom_range=0.,\n    channel_shift_range=0.,\n    fill_mode='nearest',\n    cval=0.,\n    horizontal_flip=False,\n    vertical_flip=False,\n    rescale=None,\n    dim_ordering=K.image_dim_ordering())\n\n\n\n\nGenerate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely.\n\n\n\n\n\n\nArguments\n:\n\n\n\n\nfeaturewise_center\n: Boolean. Set input mean to 0 over the dataset.\n\n\nsamplewise_center\n: Boolean. Set each sample mean to 0.\n\n\nfeaturewise_std_normalization\n: Boolean. Divide inputs by std of the dataset.\n\n\nsamplewise_std_normalization\n: Boolean. Divide each input by its std.\n\n\nzca_whitening\n: Boolean. Apply ZCA whitening.\n\n\nrotation_range\n: Int. Degree range for random rotations.\n\n\nwidth_shift_range\n: Float (fraction of total width). Range for random horizontal shifts.\n\n\nheight_shift_range\n: Float (fraction of total height). Range for random vertical shifts.\n\n\nshear_range\n: Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n\n\nzoom_range\n: Float or [lower, upper]. Range for random zoom. If a float, \n[lower, upper] = [1-zoom_range, 1+zoom_range]\n.\n\n\nchannel_shift_range\n: Float. Range for random channel shifts.\n\n\nfill_mode\n: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.  Points outside the boundaries of the input are filled according to the given mode.\n\n\ncval\n: Float or Int. Value used for points outside the boundaries when \nfill_mode = \"constant\"\n.\n\n\nhorizontal_flip\n: Boolean. Randomly flip inputs horizontally.\n\n\nvertical_flip\n: Boolean. Randomly flip inputs vertically.\n\n\nrescale\n: rescaling factor. Defaults to None. If None or 0, no rescaling is applied,\n        otherwise we multiply the data by the value provided (before applying\n        any other transformation).\n\n\ndim_ordering\n: One of {\"th\", \"tf\"}.\n    \"tf\" mode means that the images should have shape \n(samples, width, height, channels)\n,\n    \"th\" mode means that the images should have shape \n(samples, channels, width, height)\n.\n    It defaults to the \nimage_dim_ordering\n value found in your\n    Keras config file at \n~/.keras/keras.json\n.\n    If you never set it, then it will be \"tf\".\n\n\n\n\n\n\n\n\nMethods\n:\n\n\n\n\nfit(X)\n: Compute the internal data stats related to the data-dependent transformations, based on an array of sample data.\n    Only required if featurewise_center or featurewise_std_normalization or zca_whitening.\n\n\nArguments\n:\n\n\nX\n: sample data.\n\n\naugment\n: Boolean (default: False). Whether to fit on randomly augmented samples.\n\n\nrounds\n: int (default: 1). If augment, how many augmentation passes over the data to use.\n\n\nseed\n: int (default: None). Random seed.\n\n\n\n\n\n\n\n\n\n\nflow(X, y)\n: Takes numpy data \n label arrays, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.\n\n\nArguments\n:\n\n\nX\n: data.\n\n\ny\n: labels.\n\n\nbatch_size\n: int (default: 32).\n\n\nshuffle\n: boolean (defaut: True).\n\n\nseed\n: int (default: None).\n\n\nsave_to_dir\n: None or str (default: None). This allows you to optimally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).\n\n\nsave_prefix\n: str (default: \n''\n). Prefix to use for filenames of saved pictures (only relevant if \nsave_to_dir\n is set).\n\n\nsave_format\n: one of \"png\", \"jpeg\" (only relevant if \nsave_to_dir\n is set). Default: \"jpeg\".\n\n\n\n\n\n\nyields\n: Tuples of \n(x, y)\n where \nx\n is a numpy array of image data and \ny\n is a numpy array of corresponding labels.\n    The generator loops indefinitely.\n\n\n\n\n\n\nflow_from_directory(directory)\n: Takes the path to a directory, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.\n\n\nArguments\n:\n\n\ndirectory\n: path to the target directory. It should contain one subdirectory per class,\n    and the subdirectories should contain PNG or JPG images. See \nthis script\n for more details.\n\n\ntarget_size\n: tuple of integers, default: \n(256, 256)\n. The dimensions to which all images found will be resized.\n\n\ncolor_mode\n: one of \"grayscale\", \"rbg\". Default: \"rgb\". Whether the images will be converted to have 1 or 3 color channels.\n\n\nclasses\n: optional list of class subdirectories (e.g. \n['dogs', 'cats']\n). Default: None. If not provided, the list of classes will be automatically inferred (and the order of the classes, which will map to the label indices, will be alphanumeric).\n\n\nclass_mode\n: one of \"categorical\", \"binary\", \"sparse\" or None. Default: \"categorical\". Determines the type of label arrays that are returned: \"categorical\" will be 2D one-hot encoded labels, \"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels. If None, no labels are returned (the generator will only yield batches of image data, which is useful to use \nmodel.predict_generator()\n, \nmodel.evaluate_generator()\n, etc.).\n\n\nbatch_size\n: size of the batches of data (default: 32).\n\n\nshuffle\n: whether to shuffle the data (default: True)\n\n\nseed\n: optional random seed for shuffling and transformations.\n\n\nsave_to_dir\n: None or str (default: None). This allows you to optimally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).\n\n\nsave_prefix\n: str. Prefix to use for filenames of saved pictures (only relevant if \nsave_to_dir\n is set).\n\n\nsave_format\n: one of \"png\", \"jpeg\" (only relevant if \nsave_to_dir\n is set). Default: \"jpeg\".\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExamples\n:\n\n\n\n\n\n\nExample of using \n.flow(X, y)\n:\n\n\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\ndatagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)\n\n# compute quantities required for featurewise normalization\n# (std, mean, and principal components if ZCA whitening is applied)\ndatagen.fit(X_train)\n\n# fits the model on batches with real-time data augmentation:\nmodel.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                    samples_per_epoch=len(X_train), nb_epoch=nb_epoch)\n\n# here's a more \nmanual\n example\nfor e in range(nb_epoch):\n    print 'Epoch', e\n    batches = 0\n    for X_batch, Y_batch in datagen.flow(X_train, Y_train, batch_size=32):\n        loss = model.train(X_batch, Y_batch)\n        batches += 1\n        if batches \n= len(X_train) / 32:\n            # we need to break the loop by hand because\n            # the generator loops indefinitely\n            break\n\n\n\n\nExample of using \n.flow_from_directory(directory)\n:\n\n\ntrain_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        'data/train',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        'data/validation',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nmodel.fit_generator(\n        train_generator,\n        samples_per_epoch=2000,\n        nb_epoch=50,\n        validation_data=validation_generator,\n        nb_val_samples=800)\n\n\n\n\nExample of transforming images and masks together.\n\n\n# we create two instances with the same arguments\ndata_gen_args = dict(featurewise_center=True,\n                     featurewise_std_normalization=True,\n                     rotation_range=90.,\n                     width_shift_range=0.1,\n                     height_shift_range=0.1,\n                     zoom_range=0.2)\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen = ImageDataGenerator(**data_gen_args)\n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\nimage_datagen.fit(images, augment=True, seed=seed)\nmask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow_from_directory(\n    'data/images',\n    class_mode=None,\n    seed=seed)\n\nmask_generator = mask_datagen.flow_from_directory(\n    'data/masks',\n    class_mode=None,\n    seed=seed)\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\nmodel.fit_generator(\n    train_generator,\n    samples_per_epoch=2000,\n    nb_epoch=50)", 
            "title": "Image Preprocessing"
        }, 
        {
            "location": "/preprocessing/image/#imagedatagenerator", 
            "text": "keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n    samplewise_center=False,\n    featurewise_std_normalization=False,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    rotation_range=0.,\n    width_shift_range=0.,\n    height_shift_range=0.,\n    shear_range=0.,\n    zoom_range=0.,\n    channel_shift_range=0.,\n    fill_mode='nearest',\n    cval=0.,\n    horizontal_flip=False,\n    vertical_flip=False,\n    rescale=None,\n    dim_ordering=K.image_dim_ordering())  Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely.    Arguments :   featurewise_center : Boolean. Set input mean to 0 over the dataset.  samplewise_center : Boolean. Set each sample mean to 0.  featurewise_std_normalization : Boolean. Divide inputs by std of the dataset.  samplewise_std_normalization : Boolean. Divide each input by its std.  zca_whitening : Boolean. Apply ZCA whitening.  rotation_range : Int. Degree range for random rotations.  width_shift_range : Float (fraction of total width). Range for random horizontal shifts.  height_shift_range : Float (fraction of total height). Range for random vertical shifts.  shear_range : Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)  zoom_range : Float or [lower, upper]. Range for random zoom. If a float,  [lower, upper] = [1-zoom_range, 1+zoom_range] .  channel_shift_range : Float. Range for random channel shifts.  fill_mode : One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.  Points outside the boundaries of the input are filled according to the given mode.  cval : Float or Int. Value used for points outside the boundaries when  fill_mode = \"constant\" .  horizontal_flip : Boolean. Randomly flip inputs horizontally.  vertical_flip : Boolean. Randomly flip inputs vertically.  rescale : rescaling factor. Defaults to None. If None or 0, no rescaling is applied,\n        otherwise we multiply the data by the value provided (before applying\n        any other transformation).  dim_ordering : One of {\"th\", \"tf\"}.\n    \"tf\" mode means that the images should have shape  (samples, width, height, channels) ,\n    \"th\" mode means that the images should have shape  (samples, channels, width, height) .\n    It defaults to the  image_dim_ordering  value found in your\n    Keras config file at  ~/.keras/keras.json .\n    If you never set it, then it will be \"tf\".     Methods :   fit(X) : Compute the internal data stats related to the data-dependent transformations, based on an array of sample data.\n    Only required if featurewise_center or featurewise_std_normalization or zca_whitening.  Arguments :  X : sample data.  augment : Boolean (default: False). Whether to fit on randomly augmented samples.  rounds : int (default: 1). If augment, how many augmentation passes over the data to use.  seed : int (default: None). Random seed.      flow(X, y) : Takes numpy data   label arrays, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.  Arguments :  X : data.  y : labels.  batch_size : int (default: 32).  shuffle : boolean (defaut: True).  seed : int (default: None).  save_to_dir : None or str (default: None). This allows you to optimally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).  save_prefix : str (default:  '' ). Prefix to use for filenames of saved pictures (only relevant if  save_to_dir  is set).  save_format : one of \"png\", \"jpeg\" (only relevant if  save_to_dir  is set). Default: \"jpeg\".    yields : Tuples of  (x, y)  where  x  is a numpy array of image data and  y  is a numpy array of corresponding labels.\n    The generator loops indefinitely.    flow_from_directory(directory) : Takes the path to a directory, and generates batches of augmented/normalized data. Yields batches indefinitely, in an infinite loop.  Arguments :  directory : path to the target directory. It should contain one subdirectory per class,\n    and the subdirectories should contain PNG or JPG images. See  this script  for more details.  target_size : tuple of integers, default:  (256, 256) . The dimensions to which all images found will be resized.  color_mode : one of \"grayscale\", \"rbg\". Default: \"rgb\". Whether the images will be converted to have 1 or 3 color channels.  classes : optional list of class subdirectories (e.g.  ['dogs', 'cats'] ). Default: None. If not provided, the list of classes will be automatically inferred (and the order of the classes, which will map to the label indices, will be alphanumeric).  class_mode : one of \"categorical\", \"binary\", \"sparse\" or None. Default: \"categorical\". Determines the type of label arrays that are returned: \"categorical\" will be 2D one-hot encoded labels, \"binary\" will be 1D binary labels, \"sparse\" will be 1D integer labels. If None, no labels are returned (the generator will only yield batches of image data, which is useful to use  model.predict_generator() ,  model.evaluate_generator() , etc.).  batch_size : size of the batches of data (default: 32).  shuffle : whether to shuffle the data (default: True)  seed : optional random seed for shuffling and transformations.  save_to_dir : None or str (default: None). This allows you to optimally specify a directory to which to save the augmented pictures being generated (useful for visualizing what you are doing).  save_prefix : str. Prefix to use for filenames of saved pictures (only relevant if  save_to_dir  is set).  save_format : one of \"png\", \"jpeg\" (only relevant if  save_to_dir  is set). Default: \"jpeg\".         Examples :    Example of using  .flow(X, y) :  (X_train, y_train), (X_test, y_test) = cifar10.load_data()\nY_train = np_utils.to_categorical(y_train, nb_classes)\nY_test = np_utils.to_categorical(y_test, nb_classes)\n\ndatagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    horizontal_flip=True)\n\n# compute quantities required for featurewise normalization\n# (std, mean, and principal components if ZCA whitening is applied)\ndatagen.fit(X_train)\n\n# fits the model on batches with real-time data augmentation:\nmodel.fit_generator(datagen.flow(X_train, Y_train, batch_size=32),\n                    samples_per_epoch=len(X_train), nb_epoch=nb_epoch)\n\n# here's a more  manual  example\nfor e in range(nb_epoch):\n    print 'Epoch', e\n    batches = 0\n    for X_batch, Y_batch in datagen.flow(X_train, Y_train, batch_size=32):\n        loss = model.train(X_batch, Y_batch)\n        batches += 1\n        if batches  = len(X_train) / 32:\n            # we need to break the loop by hand because\n            # the generator loops indefinitely\n            break  Example of using  .flow_from_directory(directory) :  train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1./255)\n\ntrain_generator = train_datagen.flow_from_directory(\n        'data/train',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nvalidation_generator = test_datagen.flow_from_directory(\n        'data/validation',\n        target_size=(150, 150),\n        batch_size=32,\n        class_mode='binary')\n\nmodel.fit_generator(\n        train_generator,\n        samples_per_epoch=2000,\n        nb_epoch=50,\n        validation_data=validation_generator,\n        nb_val_samples=800)  Example of transforming images and masks together.  # we create two instances with the same arguments\ndata_gen_args = dict(featurewise_center=True,\n                     featurewise_std_normalization=True,\n                     rotation_range=90.,\n                     width_shift_range=0.1,\n                     height_shift_range=0.1,\n                     zoom_range=0.2)\nimage_datagen = ImageDataGenerator(**data_gen_args)\nmask_datagen = ImageDataGenerator(**data_gen_args)\n\n# Provide the same seed and keyword arguments to the fit and flow methods\nseed = 1\nimage_datagen.fit(images, augment=True, seed=seed)\nmask_datagen.fit(masks, augment=True, seed=seed)\n\nimage_generator = image_datagen.flow_from_directory(\n    'data/images',\n    class_mode=None,\n    seed=seed)\n\nmask_generator = mask_datagen.flow_from_directory(\n    'data/masks',\n    class_mode=None,\n    seed=seed)\n\n# combine generators into one which yields image and masks\ntrain_generator = zip(image_generator, mask_generator)\n\nmodel.fit_generator(\n    train_generator,\n    samples_per_epoch=2000,\n    nb_epoch=50)", 
            "title": "ImageDataGenerator"
        }, 
        {
            "location": "/objectives/", 
            "text": "Usage of objectives\n\n\nAn objective function (or loss function, or optimization score function) is one of the two parameters required to compile a model:\n\n\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\n\n\n\n\nYou can either pass the name of an existing objective, or pass a Theano/TensorFlow symbolic function that returns a scalar for each data-point and takes the following two arguments:\n\n\n\n\ny_true\n: True labels. Theano/TensorFlow tensor.\n\n\ny_pred\n: Predictions. Theano/TensorFlow tensor of the same shape as y_true.\n\n\n\n\nThe actual optimized objective is the mean of the output array across all datapoints.\n\n\nFor a few examples of such functions, check out the \nobjectives source\n.\n\n\nAvailable objectives\n\n\n\n\nmean_squared_error\n / \nmse\n\n\nmean_absolute_error\n / \nmae\n\n\nmean_absolute_percentage_error\n / \nmape\n\n\nmean_squared_logarithmic_error\n / \nmsle\n\n\nsquared_hinge\n\n\nhinge\n\n\nbinary_crossentropy\n: Also known as logloss. \n\n\ncategorical_crossentropy\n: Also known as multiclass logloss. \nNote\n: using this objective requires that your labels are binary arrays of shape \n(nb_samples, nb_classes)\n.\n\n\nsparse_categorical_crossentropy\n: As above but accepts sparse labels. \nNote\n: this objective still requires that your labels have the same number of dimensions as your outputs; you may need to add a length-1 dimension to the shape of your labels, e.g with \nnp.expand_dims(y, -1)\n.\n\n\nkullback_leibler_divergence\n / \nkld\n: Information gain from a predicted probability distribution Q to a true probability distribution P. Gives a measure of difference between both distributions.\n\n\npoisson\n: Mean of \n(predictions - targets * log(predictions))\n\n\ncosine_proximity\n: The opposite (negative) of the mean cosine proximity between predictions and targets.\n\n\n\n\nNote\n: when using the \ncategorical_crossentropy\n objective, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros expect for a 1 at the index corresponding to the class of the sample). In order to convert \ninteger targets\n into \ncategorical targets\n, you can use the Keras utility \nto_categorical\n:\n\n\nfrom keras.utils.np_utils import to_categorical\n\ncategorical_labels = to_categorical(int_labels, nb_classes=None)", 
            "title": "Objectives"
        }, 
        {
            "location": "/objectives/#usage-of-objectives", 
            "text": "An objective function (or loss function, or optimization score function) is one of the two parameters required to compile a model:  model.compile(loss='mean_squared_error', optimizer='sgd')  You can either pass the name of an existing objective, or pass a Theano/TensorFlow symbolic function that returns a scalar for each data-point and takes the following two arguments:   y_true : True labels. Theano/TensorFlow tensor.  y_pred : Predictions. Theano/TensorFlow tensor of the same shape as y_true.   The actual optimized objective is the mean of the output array across all datapoints.  For a few examples of such functions, check out the  objectives source .", 
            "title": "Usage of objectives"
        }, 
        {
            "location": "/objectives/#available-objectives", 
            "text": "mean_squared_error  /  mse  mean_absolute_error  /  mae  mean_absolute_percentage_error  /  mape  mean_squared_logarithmic_error  /  msle  squared_hinge  hinge  binary_crossentropy : Also known as logloss.   categorical_crossentropy : Also known as multiclass logloss.  Note : using this objective requires that your labels are binary arrays of shape  (nb_samples, nb_classes) .  sparse_categorical_crossentropy : As above but accepts sparse labels.  Note : this objective still requires that your labels have the same number of dimensions as your outputs; you may need to add a length-1 dimension to the shape of your labels, e.g with  np.expand_dims(y, -1) .  kullback_leibler_divergence  /  kld : Information gain from a predicted probability distribution Q to a true probability distribution P. Gives a measure of difference between both distributions.  poisson : Mean of  (predictions - targets * log(predictions))  cosine_proximity : The opposite (negative) of the mean cosine proximity between predictions and targets.   Note : when using the  categorical_crossentropy  objective, your targets should be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros expect for a 1 at the index corresponding to the class of the sample). In order to convert  integer targets  into  categorical targets , you can use the Keras utility  to_categorical :  from keras.utils.np_utils import to_categorical\n\ncategorical_labels = to_categorical(int_labels, nb_classes=None)", 
            "title": "Available objectives"
        }, 
        {
            "location": "/metrics/", 
            "text": "Usage of metrics\n\n\nA metric is a function that is used to judge the performance of your model. Metric functions are to be supplied in the \nmetrics\n parameter when a model is compiled.\n\n\nA metric function is similar to an \nobjective function\n, except that the results from evaluating a metric are not used when training the model.\n\n\nYou can either pass the name of an existing metric, or pass a Theano/TensorFlow symbolic function (see \nCustom metrics\n).\n\n\nArguments\n\n\n\n\ny_true\n: True labels. Theano/TensorFlow tensor.\n\n\ny_pred\n: Predictions. Theano/TensorFlow tensor of the same shape as y_true.\n\n\n\n\nReturns\n\n\nSingle tensor value representing the mean of the output array across all\n  datapoints.\n\n\n\n\nAvailable metrics\n\n\nmatthews_correlation\n\n\nmatthews_correlation(y_true, y_pred)\n\n\n\n\nMatthews correlation metric.\n\n\nIt is only computed as a batch-wise average, not globally.\n\n\nComputes the Matthews correlation coefficient measure for quality\nof binary classification problems.\n\n\n\n\nprecision\n\n\nprecision(y_true, y_pred)\n\n\n\n\nPrecision metric.\n\n\nOnly computes a batch-wise average of precision.\n\n\nComputes the precision, a metric for multi-label classification of\nhow many selected items are relevant.\n\n\n\n\nrecall\n\n\nrecall(y_true, y_pred)\n\n\n\n\nRecall metric.\n\n\nOnly computes a batch-wise average of recall.\n\n\nComputes the recall, a metric for multi-label classification of\nhow many relevant items are selected.\n\n\n\n\nfbeta_score\n\n\nfbeta_score(y_true, y_pred, beta=1)\n\n\n\n\nComputes the F score.\n\n\nThe F score is the weighted harmonic mean of precision and recall.\nHere it is only computed as a batch-wise average, not globally.\n\n\nThis is useful for multi-label classification, where input samples can be\nclassified as sets of labels. By only using accuracy (precision) a model\nwould achieve a perfect score by simply assigning every class to every\ninput. In order to avoid this, a metric should penalize incorrect class\nassignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\ncomputes this, as a weighted mean of the proportion of correct class\nassignments vs. the proportion of incorrect class assignments.\n\n\nWith beta = 1, this is equivalent to a F-measure. With beta \n 1, assigning\ncorrect classes becomes more important, and with beta \n 1 the metric is\ninstead weighted towards penalizing incorrect class assignments.\n\n\n\n\nfmeasure\n\n\nfmeasure(y_true, y_pred)\n\n\n\n\nComputes the f-measure, the harmonic mean of precision and recall.\n\n\nHere it is only computed as a batch-wise average, not globally.\n\n\n\n\nCustom metrics\n\n\nCustom metrics can be defined and passed via the compilation step. The\nfunction would need to take \n(y_true, y_pred)\n as arguments and return\neither a single tensor value or a dict \nmetric_name -\n metric_value\n.\n\n\n# for custom metrics\nimport keras.backend as K\n\ndef mean_pred(y_true, y_pred):\n    return K.mean(y_pred)\n\ndef false_rates(y_true, y_pred):\n    false_neg = ...\n    false_pos = ...\n    return {\n        'false_neg': false_neg,\n        'false_pos': false_pos,\n    }\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy', mean_pred, false_rates])", 
            "title": "Metrics"
        }, 
        {
            "location": "/metrics/#usage-of-metrics", 
            "text": "A metric is a function that is used to judge the performance of your model. Metric functions are to be supplied in the  metrics  parameter when a model is compiled.  A metric function is similar to an  objective function , except that the results from evaluating a metric are not used when training the model.  You can either pass the name of an existing metric, or pass a Theano/TensorFlow symbolic function (see  Custom metrics ).", 
            "title": "Usage of metrics"
        }, 
        {
            "location": "/metrics/#arguments", 
            "text": "y_true : True labels. Theano/TensorFlow tensor.  y_pred : Predictions. Theano/TensorFlow tensor of the same shape as y_true.", 
            "title": "Arguments"
        }, 
        {
            "location": "/metrics/#returns", 
            "text": "Single tensor value representing the mean of the output array across all\n  datapoints.", 
            "title": "Returns"
        }, 
        {
            "location": "/metrics/#available-metrics", 
            "text": "", 
            "title": "Available metrics"
        }, 
        {
            "location": "/metrics/#matthews_correlation", 
            "text": "matthews_correlation(y_true, y_pred)  Matthews correlation metric.  It is only computed as a batch-wise average, not globally.  Computes the Matthews correlation coefficient measure for quality\nof binary classification problems.", 
            "title": "matthews_correlation"
        }, 
        {
            "location": "/metrics/#precision", 
            "text": "precision(y_true, y_pred)  Precision metric.  Only computes a batch-wise average of precision.  Computes the precision, a metric for multi-label classification of\nhow many selected items are relevant.", 
            "title": "precision"
        }, 
        {
            "location": "/metrics/#recall", 
            "text": "recall(y_true, y_pred)  Recall metric.  Only computes a batch-wise average of recall.  Computes the recall, a metric for multi-label classification of\nhow many relevant items are selected.", 
            "title": "recall"
        }, 
        {
            "location": "/metrics/#fbeta_score", 
            "text": "fbeta_score(y_true, y_pred, beta=1)  Computes the F score.  The F score is the weighted harmonic mean of precision and recall.\nHere it is only computed as a batch-wise average, not globally.  This is useful for multi-label classification, where input samples can be\nclassified as sets of labels. By only using accuracy (precision) a model\nwould achieve a perfect score by simply assigning every class to every\ninput. In order to avoid this, a metric should penalize incorrect class\nassignments as well (recall). The F-beta score (ranged from 0.0 to 1.0)\ncomputes this, as a weighted mean of the proportion of correct class\nassignments vs. the proportion of incorrect class assignments.  With beta = 1, this is equivalent to a F-measure. With beta   1, assigning\ncorrect classes becomes more important, and with beta   1 the metric is\ninstead weighted towards penalizing incorrect class assignments.", 
            "title": "fbeta_score"
        }, 
        {
            "location": "/metrics/#fmeasure", 
            "text": "fmeasure(y_true, y_pred)  Computes the f-measure, the harmonic mean of precision and recall.  Here it is only computed as a batch-wise average, not globally.", 
            "title": "fmeasure"
        }, 
        {
            "location": "/metrics/#custom-metrics", 
            "text": "Custom metrics can be defined and passed via the compilation step. The\nfunction would need to take  (y_true, y_pred)  as arguments and return\neither a single tensor value or a dict  metric_name -  metric_value .  # for custom metrics\nimport keras.backend as K\n\ndef mean_pred(y_true, y_pred):\n    return K.mean(y_pred)\n\ndef false_rates(y_true, y_pred):\n    false_neg = ...\n    false_pos = ...\n    return {\n        'false_neg': false_neg,\n        'false_pos': false_pos,\n    }\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy', mean_pred, false_rates])", 
            "title": "Custom metrics"
        }, 
        {
            "location": "/optimizers/", 
            "text": "Usage of optimizers\n\n\nAn optimizer is one of the two arguments required for compiling a Keras model:\n\n\nmodel = Sequential()\nmodel.add(Dense(64, init='uniform', input_dim=10))\nmodel.add(Activation('tanh'))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='mean_squared_error', optimizer=sgd)\n\n\n\n\nYou can either instantiate an optimizer before passing it to \nmodel.compile()\n , as in the above example, or you can call it by its name. In the latter case, the default parameters for the optimizer will be used.\n\n\n# pass optimizer by name: default parameters will be used\nmodel.compile(loss='mean_squared_error', optimizer='sgd')\n\n\n\n\n\n\nParameters common to all Keras optimizers\n\n\nThe parameters \nclipnorm\n and \nclipvalue\n can be used with all optimizers to control gradient clipping:\n\n\n# all parameter gradients will be clipped to\n# a maximum norm of 1.\nsgd = SGD(lr=0.01, clipnorm=1.)\n\n\n\n\n# all parameter gradients will be clipped to\n# a maximum value of 0.5 and\n# a minimum value of -0.5.\nsgd = SGD(lr=0.01, clipvalue=0.5)\n\n\n\n\n\n\n[source]\n\n\nSGD\n\n\nkeras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n\n\n\n\nStochastic gradient descent optimizer.\n\n\nIncludes support for momentum,\nlearning rate decay, and Nesterov momentum.\n\n\nArguments\n\n\n\n\nlr\n: float \n= 0. Learning rate.\n\n\nmomentum\n: float \n= 0. Parameter updates momentum.\n\n\ndecay\n: float \n= 0. Learning rate decay over each update.\n\n\nnesterov\n: boolean. Whether to apply Nesterov momentum.\n\n\n\n\n\n\n[source]\n\n\nRMSprop\n\n\nkeras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n\n\n\n\nRMSProp optimizer.\n\n\nIt is recommended to leave the parameters of this optimizer\nat their default values\n(except the learning rate, which can be freely tuned).\n\n\nThis optimizer is usually a good choice for recurrent\nneural networks.\n\n\nArguments\n\n\n\n\nlr\n: float \n= 0. Learning rate.\n\n\nrho\n: float \n= 0.\n\n\nepsilon\n: float \n= 0. Fuzz factor.\n\n\ndecay\n: float \n= 0. Learning rate decay over each update.\n\n\n\n\nReferences\n\n\n\n\nrmsprop: Divide the gradient by a running average of its recent magnitude\n\n\n\n\n\n\n[source]\n\n\nAdagrad\n\n\nkeras.optimizers.Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)\n\n\n\n\nAdagrad optimizer.\n\n\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n\nArguments\n\n\n\n\nlr\n: float \n= 0. Learning rate.\n\n\nepsilon\n: float \n= 0.\n\n\ndecay\n: float \n= 0. Learning rate decay over each update.\n\n\n\n\nReferences\n\n\n\n\nAdaptive Subgradient Methods for Online Learning and Stochastic Optimization\n\n\n\n\n\n\n[source]\n\n\nAdadelta\n\n\nkeras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n\n\n\n\nAdadelta optimizer.\n\n\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n\nArguments\n\n\n\n\nlr\n: float \n= 0. Learning rate.\n    It is recommended to leave it at the default value.\n\n\nrho\n: float \n= 0.\n\n\nepsilon\n: float \n= 0. Fuzz factor.\n\n\ndecay\n: float \n= 0. Learning rate decay over each update.\n\n\n\n\nReferences\n\n\n\n\nAdadelta - an adaptive learning rate method\n\n\n\n\n\n\n[source]\n\n\nAdam\n\n\nkeras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\n\n\n\nAdam optimizer.\n\n\nDefault parameters follow those provided in the original paper.\n\n\nArguments\n\n\n\n\nlr\n: float \n= 0. Learning rate.\n\n\nbeta_1\n: float, 0 \n beta \n 1. Generally close to 1.\n\n\nbeta_2\n: float, 0 \n beta \n 1. Generally close to 1.\n\n\nepsilon\n: float \n= 0. Fuzz factor.\n\n\ndecay\n: float \n= 0. Learning rate decay over each update.\n\n\n\n\nReferences\n\n\n\n\nAdam - A Method for Stochastic Optimization\n\n\n\n\n\n\n[source]\n\n\nAdamax\n\n\nkeras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\n\n\n\nAdamax optimizer from Adam paper's Section 7.\n\n\nIt is a variant of Adam based on the infinity norm.\nDefault parameters follow those provided in the paper.\n\n\nArguments\n\n\n\n\nlr\n: float \n= 0. Learning rate.\n\n\nbeta_1/beta_2\n: floats, 0 \n beta \n 1. Generally close to 1.\n\n\nepsilon\n: float \n= 0. Fuzz factor.\n\n\ndecay\n: float \n= 0. Learning rate decay over each update.\n\n\n\n\nReferences\n\n\n\n\nAdam - A Method for Stochastic Optimization\n\n\n\n\n\n\n[source]\n\n\nNadam\n\n\nkeras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n\n\n\n\nNesterov Adam optimizer.\n\n\nMuch like Adam is essentially RMSprop with momentum,\nNadam is Adam RMSprop with Nesterov momentum.\n\n\nDefault parameters follow those provided in the paper.\nIt is recommended to leave the parameters of this optimizer\nat their default values.\n\n\nArguments\n\n\n\n\nlr\n: float \n= 0. Learning rate.\n\n\nbeta_1/beta_2\n: floats, 0 \n beta \n 1. Generally close to 1.\n\n\nepsilon\n: float \n= 0. Fuzz factor.\n\n\n\n\nReferences\n\n\n\n\nNadam report\n\n\nOn the importance of initialization and momentum in deep learning\n\n\n\n\n\n\n[source]\n\n\nTFOptimizer\n\n\nkeras.optimizers.TFOptimizer(optimizer)\n\n\n\n\nWrapper class for native TensorFlow optimizers.", 
            "title": "Optimizers"
        }, 
        {
            "location": "/optimizers/#usage-of-optimizers", 
            "text": "An optimizer is one of the two arguments required for compiling a Keras model:  model = Sequential()\nmodel.add(Dense(64, init='uniform', input_dim=10))\nmodel.add(Activation('tanh'))\nmodel.add(Activation('softmax'))\n\nsgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nmodel.compile(loss='mean_squared_error', optimizer=sgd)  You can either instantiate an optimizer before passing it to  model.compile()  , as in the above example, or you can call it by its name. In the latter case, the default parameters for the optimizer will be used.  # pass optimizer by name: default parameters will be used\nmodel.compile(loss='mean_squared_error', optimizer='sgd')", 
            "title": "Usage of optimizers"
        }, 
        {
            "location": "/optimizers/#parameters-common-to-all-keras-optimizers", 
            "text": "The parameters  clipnorm  and  clipvalue  can be used with all optimizers to control gradient clipping:  # all parameter gradients will be clipped to\n# a maximum norm of 1.\nsgd = SGD(lr=0.01, clipnorm=1.)  # all parameter gradients will be clipped to\n# a maximum value of 0.5 and\n# a minimum value of -0.5.\nsgd = SGD(lr=0.01, clipvalue=0.5)   [source]", 
            "title": "Parameters common to all Keras optimizers"
        }, 
        {
            "location": "/optimizers/#sgd", 
            "text": "keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)  Stochastic gradient descent optimizer.  Includes support for momentum,\nlearning rate decay, and Nesterov momentum.  Arguments   lr : float  = 0. Learning rate.  momentum : float  = 0. Parameter updates momentum.  decay : float  = 0. Learning rate decay over each update.  nesterov : boolean. Whether to apply Nesterov momentum.    [source]", 
            "title": "SGD"
        }, 
        {
            "location": "/optimizers/#rmsprop", 
            "text": "keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)  RMSProp optimizer.  It is recommended to leave the parameters of this optimizer\nat their default values\n(except the learning rate, which can be freely tuned).  This optimizer is usually a good choice for recurrent\nneural networks.  Arguments   lr : float  = 0. Learning rate.  rho : float  = 0.  epsilon : float  = 0. Fuzz factor.  decay : float  = 0. Learning rate decay over each update.   References   rmsprop: Divide the gradient by a running average of its recent magnitude    [source]", 
            "title": "RMSprop"
        }, 
        {
            "location": "/optimizers/#adagrad", 
            "text": "keras.optimizers.Adagrad(lr=0.01, epsilon=1e-08, decay=0.0)  Adagrad optimizer.  It is recommended to leave the parameters of this optimizer\nat their default values.  Arguments   lr : float  = 0. Learning rate.  epsilon : float  = 0.  decay : float  = 0. Learning rate decay over each update.   References   Adaptive Subgradient Methods for Online Learning and Stochastic Optimization    [source]", 
            "title": "Adagrad"
        }, 
        {
            "location": "/optimizers/#adadelta", 
            "text": "keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)  Adadelta optimizer.  It is recommended to leave the parameters of this optimizer\nat their default values.  Arguments   lr : float  = 0. Learning rate.\n    It is recommended to leave it at the default value.  rho : float  = 0.  epsilon : float  = 0. Fuzz factor.  decay : float  = 0. Learning rate decay over each update.   References   Adadelta - an adaptive learning rate method    [source]", 
            "title": "Adadelta"
        }, 
        {
            "location": "/optimizers/#adam", 
            "text": "keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)  Adam optimizer.  Default parameters follow those provided in the original paper.  Arguments   lr : float  = 0. Learning rate.  beta_1 : float, 0   beta   1. Generally close to 1.  beta_2 : float, 0   beta   1. Generally close to 1.  epsilon : float  = 0. Fuzz factor.  decay : float  = 0. Learning rate decay over each update.   References   Adam - A Method for Stochastic Optimization    [source]", 
            "title": "Adam"
        }, 
        {
            "location": "/optimizers/#adamax", 
            "text": "keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)  Adamax optimizer from Adam paper's Section 7.  It is a variant of Adam based on the infinity norm.\nDefault parameters follow those provided in the paper.  Arguments   lr : float  = 0. Learning rate.  beta_1/beta_2 : floats, 0   beta   1. Generally close to 1.  epsilon : float  = 0. Fuzz factor.  decay : float  = 0. Learning rate decay over each update.   References   Adam - A Method for Stochastic Optimization    [source]", 
            "title": "Adamax"
        }, 
        {
            "location": "/optimizers/#nadam", 
            "text": "keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)  Nesterov Adam optimizer.  Much like Adam is essentially RMSprop with momentum,\nNadam is Adam RMSprop with Nesterov momentum.  Default parameters follow those provided in the paper.\nIt is recommended to leave the parameters of this optimizer\nat their default values.  Arguments   lr : float  = 0. Learning rate.  beta_1/beta_2 : floats, 0   beta   1. Generally close to 1.  epsilon : float  = 0. Fuzz factor.   References   Nadam report  On the importance of initialization and momentum in deep learning    [source]", 
            "title": "Nadam"
        }, 
        {
            "location": "/optimizers/#tfoptimizer", 
            "text": "keras.optimizers.TFOptimizer(optimizer)  Wrapper class for native TensorFlow optimizers.", 
            "title": "TFOptimizer"
        }, 
        {
            "location": "/activations/", 
            "text": "Usage of activations\n\n\nActivations can either be used through an \nActivation\n layer, or through the \nactivation\n argument supported by all forward layers:\n\n\nfrom keras.layers.core import Activation, Dense\n\nmodel.add(Dense(64))\nmodel.add(Activation('tanh'))\n\n\n\n\nis equivalent to:\n\n\nmodel.add(Dense(64, activation='tanh'))\n\n\n\n\nYou can also pass an element-wise Theano/TensorFlow function as an activation:\n\n\nfrom keras import backend as K\n\ndef tanh(x):\n    return K.tanh(x)\n\nmodel.add(Dense(64, activation=tanh))\nmodel.add(Activation(tanh))\n\n\n\n\nAvailable activations\n\n\n\n\nsoftmax\n: Softmax applied across inputs last dimension. Expects shape either \n(nb_samples, nb_timesteps, nb_dims)\n or \n(nb_samples, nb_dims)\n.\n\n\nsoftplus\n\n\nsoftsign\n\n\nrelu\n\n\ntanh\n\n\nsigmoid\n\n\nhard_sigmoid\n\n\nlinear\n\n\n\n\nOn Advanced Activations\n\n\nActivations that are more complex than a simple Theano/TensorFlow function (eg. learnable activations, configurable activations, etc.) are available as \nAdvanced Activation layers\n, and can be found in the module \nkeras.layers.advanced_activations\n. These include PReLU and LeakyReLU.", 
            "title": "Activations"
        }, 
        {
            "location": "/activations/#usage-of-activations", 
            "text": "Activations can either be used through an  Activation  layer, or through the  activation  argument supported by all forward layers:  from keras.layers.core import Activation, Dense\n\nmodel.add(Dense(64))\nmodel.add(Activation('tanh'))  is equivalent to:  model.add(Dense(64, activation='tanh'))  You can also pass an element-wise Theano/TensorFlow function as an activation:  from keras import backend as K\n\ndef tanh(x):\n    return K.tanh(x)\n\nmodel.add(Dense(64, activation=tanh))\nmodel.add(Activation(tanh))", 
            "title": "Usage of activations"
        }, 
        {
            "location": "/activations/#available-activations", 
            "text": "softmax : Softmax applied across inputs last dimension. Expects shape either  (nb_samples, nb_timesteps, nb_dims)  or  (nb_samples, nb_dims) .  softplus  softsign  relu  tanh  sigmoid  hard_sigmoid  linear", 
            "title": "Available activations"
        }, 
        {
            "location": "/activations/#on-advanced-activations", 
            "text": "Activations that are more complex than a simple Theano/TensorFlow function (eg. learnable activations, configurable activations, etc.) are available as  Advanced Activation layers , and can be found in the module  keras.layers.advanced_activations . These include PReLU and LeakyReLU.", 
            "title": "On Advanced Activations"
        }, 
        {
            "location": "/callbacks/", 
            "text": "Usage of callbacks\n\n\nA callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument \ncallbacks\n) to the \n.fit()\n method of the \nSequential\n model. The relevant methods of the callbacks will then be called at each stage of the training. \n\n\n\n\n[source]\n\n\nCallback\n\n\nkeras.callbacks.Callback()\n\n\n\n\nAbstract base class used to build new callbacks.\n\n\nProperties\n\n\n\n\nparams\n: dict. Training parameters\n    (eg. verbosity, batch size, number of epochs...).\n\n\nmodel\n: instance of \nkeras.models.Model\n.\n    Reference of the model being trained.\n\n\n\n\nThe \nlogs\n dictionary that callback methods\ntake as argument will contain keys for quantities relevant to\nthe current batch or epoch.\n\n\nCurrently, the \n.fit()\n method of the \nSequential\n model class\nwill include the following quantities in the \nlogs\n that\nit passes to its callbacks:\n\n\n\n\non_epoch_end\n: logs include \nacc\n and \nloss\n, and\n    optionally include \nval_loss\n\n    (if validation is enabled in \nfit\n), and \nval_acc\n\n    (if validation and accuracy monitoring are enabled).\n\n\non_batch_begin\n: logs include \nsize\n,\n    the number of samples in the current batch.\n\n\non_batch_end\n: logs include \nloss\n, and optionally \nacc\n\n    (if accuracy monitoring is enabled).\n\n\n\n\n\n\n[source]\n\n\nBaseLogger\n\n\nkeras.callbacks.BaseLogger()\n\n\n\n\nCallback that accumulates epoch averages of metrics.\n\n\nThis callback is automatically applied to every Keras model.\n\n\n\n\n[source]\n\n\nLambdaCallback\n\n\nkeras.callbacks.LambdaCallback(on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None)\n\n\n\n\nCallback for creating simple, custom callbacks on-the-fly.\n\n\nThis callback is constructed with anonymous functions that will be called\nat the appropriate time. Note that the callbacks expects positional\narguments, as:\n - \non_epoch_begin\n and \non_epoch_end\n expect two positional arguments:\n\nepoch\n, \nlogs\n\n - \non_batch_begin\n and \non_batch_end\n expect two positional arguments:\n\nbatch\n, \nlogs\n\n - \non_train_begin\n and \non_train_end\n expect one positional argument:\n\nlogs\n\n\nArguments\n\n\n\n\non_epoch_begin\n: called at the beginning of every epoch.\n\n\non_epoch_end\n: called at the end of every epoch.\n\n\non_batch_begin\n: called at the beginning of every batch.\n\n\non_batch_end\n: called at the end of every batch.\n\n\non_train_begin\n: called at the beginning of model training.\n\n\non_train_end\n: called at the end of model training.\n\n\n\n\nExample\n\n\n# Print the batch number at the beginning of every batch.\nbatch_print_callback = LambdaCallback(\n    on_batch_begin=lambda batch,logs: print(batch))\n\n# Plot the loss after every epoch.\nimport numpy as np\nimport matplotlib.pyplot as plt\nplot_loss_callback = LambdaCallback(\n    on_epoch_end=lambda epoch, logs: plt.plot(np.arange(epoch),\n                      logs['loss']))\n\n# Terminate some processes after having finished model training.\nprocesses = ...\ncleanup_callback = LambdaCallback(\n    on_train_end=lambda logs: [\n    p.terminate() for p in processes if p.is_alive()])\n\nmodel.fit(...,\n      callbacks=[batch_print_callback,\n         plot_loss_callback,\n         cleanup_callback])\n\n\n\n\n\n\n[source]\n\n\nEarlyStopping\n\n\nkeras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')\n\n\n\n\nStop training when a monitored quantity has stopped improving.\n\n\nArguments\n\n\n\n\nmonitor\n: quantity to be monitored.\n\n\nmin_delta\n: minimum change in the monitored quantity\n    to qualify as an improvement, i.e. an absolute\n    change of less than min_delta, will count as no\n    improvement.\n\n\npatience\n: number of epochs with no improvement\n    after which training will be stopped.\n\n\nverbose\n: verbosity mode.\n\n\nmode\n: one of {auto, min, max}. In \nmin\n mode,\n    training will stop when the quantity\n    monitored has stopped decreasing; in \nmax\n\n    mode it will stop when the quantity\n    monitored has stopped increasing; in \nauto\n\n    mode, the direction is automatically inferred\n    from the name of the monitored quantity.\n\n\n\n\n\n\n[source]\n\n\nRemoteMonitor\n\n\nkeras.callbacks.RemoteMonitor(root='http://localhost:9000', path='/publish/epoch/end/', field='data', headers=None)\n\n\n\n\nCallback used to stream events to a server.\n\n\nRequires the \nrequests\n library.\nEvents are sent to \nroot + '/publish/epoch/end/'\n by default. Calls are\nHTTP POST, with a \ndata\n argument which is a\nJSON-encoded dictionary of event data.\n\n\nArguments\n\n\n\n\nroot\n: String; root url of the target server.\n\n\npath\n: String; path relative to \nroot\n to which the events will be sent.\n\n\nfield\n: String; JSON field under which the data will be stored.\n\n\nheaders\n: Dictionary; optional custom HTTP headers.\n    Defaults to:\n\n\n`{'Accept'\n: 'application/json',\n\n\n'Content-Type'\n: 'application/json'}`\n\n\n\n\n\n\n\n\n\n\n[source]\n\n\nLearningRateScheduler\n\n\nkeras.callbacks.LearningRateScheduler(schedule)\n\n\n\n\nLearning rate scheduler.\n\n\nArguments\n\n\n\n\nschedule\n: a function that takes an epoch index as input\n    (integer, indexed from 0) and returns a new\n    learning rate as output (float).\n\n\n\n\n\n\n[source]\n\n\nTensorBoard\n\n\nkeras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)\n\n\n\n\nTensorboard basic visualizations.\n\n\nThis callback writes a log for TensorBoard, which allows\nyou to visualize dynamic graphs of your training and test\nmetrics, as well as activation histograms for the different\nlayers in your model.\n\n\nTensorBoard is a visualization tool provided with TensorFlow.\n\n\nIf you have installed TensorFlow with pip, you should be able\nto launch TensorBoard from the command line:\n\n\ntensorboard --logdir=/full_path_to_your_logs\n\n\n\n\nYou can find more information about TensorBoard\n- __\nhere\n.\n\n\nArguments\n\n\n\n\nlog_dir\n: the path of the directory where to save the log\n    files to be parsed by Tensorboard\n\n\nhistogram_freq\n: frequency (in epochs) at which to compute activation\n    histograms for the layers of the model. If set to 0,\n    histograms won't be computed.\n\n\nwrite_graph\n: whether to visualize the graph in Tensorboard.\n    The log file can become quite large when\n    write_graph is set to True.\n\n\n\n\n\n\n[source]\n\n\nReduceLROnPlateau\n\n\nkeras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n\n\n\n\nReduce learning rate when a metric has stopped improving.\n\n\nModels often benefit from reducing the learning rate by a factor\nof 2-10 once learning stagnates. This callback monitors a\nquantity and if no improvement is seen for a 'patience' number\nof epochs, the learning rate is reduced.\n\n\nExample\n\n\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                  patience=5, min_lr=0.001)\n    model.fit(X_train, Y_train, callbacks=[reduce_lr])\n\n\n\n\nArguments\n\n\n\n\nmonitor\n: quantity to be monitored.\n\n\nfactor\n: factor by which the learning rate will\n    be reduced. new_lr = lr * factor\n\n\npatience\n: number of epochs with no improvement\n    after which learning rate will be reduced.\n\n\nverbose\n: int. 0: quiet, 1: update messages.\n\n\nmode\n: one of {auto, min, max}. In \nmin\n mode,\n    lr will be reduced when the quantity\n    monitored has stopped decreasing; in \nmax\n\n    mode it will be reduced when the quantity\n    monitored has stopped increasing; in \nauto\n\n    mode, the direction is automatically inferred\n    from the name of the monitored quantity.\n\n\nepsilon\n: threshold for measuring the new optimum,\n    to only focus on significant changes.\n\n\ncooldown\n: number of epochs to wait before resuming\n    normal operation after lr has been reduced.\n\n\nmin_lr\n: lower bound on the learning rate.\n\n\n\n\n\n\n[source]\n\n\nCSVLogger\n\n\nkeras.callbacks.CSVLogger(filename, separator=',', append=False)\n\n\n\n\nCallback that streams epoch results to a csv file.\n\n\nSupports all values that can be represented as a string,\nincluding 1D iterables such as np.ndarray.\n\n\nExample\n\n\n    csv_logger = CSVLogger('training.log')\n    model.fit(X_train, Y_train, callbacks=[csv_logger])\n\n\n\n\nArguments\n\n\n\n\nfilename\n: filename of the csv file, e.g. 'run/log.csv'.\n\n\nseparator\n: string used to separate elements in the csv file.\n\n\nappend\n: True: append if file exists (useful for continuing\n    training). False: overwrite existing file,\n\n\n\n\n\n\n[source]\n\n\nHistory\n\n\nkeras.callbacks.History()\n\n\n\n\nCallback that records events into a \nHistory\n object.\n\n\nThis callback is automatically applied to\nevery Keras model. The \nHistory\n object\ngets returned by the \nfit\n method of models.\n\n\n\n\n[source]\n\n\nModelCheckpoint\n\n\nkeras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n\n\n\n\nSave the model after every epoch.\n\n\nfilepath\n can contain named formatting options,\nwhich will be filled the value of \nepoch\n and\nkeys in \nlogs\n (passed in \non_epoch_end\n).\n\n\nFor example: if \nfilepath\n is \nweights.{epoch:02d}-{val_loss:.2f}.hdf5\n,\nthen the model checkpoints will be saved with the epoch number and\nthe validation loss in the filename.\n\n\nArguments\n\n\n\n\nfilepath\n: string, path to save the model file.\n\n\nmonitor\n: quantity to monitor.\n\n\nverbose\n: verbosity mode, 0 or 1.\n\n\nsave_best_only\n: if \nsave_best_only=True\n,\n    the latest best model according to\n    the quantity monitored will not be overwritten.\n\n\nmode\n: one of {auto, min, max}.\n    If \nsave_best_only=True\n, the decision\n    to overwrite the current save file is made\n    based on either the maximization or the\n    minimization of the monitored quantity. For \nval_acc\n,\n    this should be \nmax\n, for \nval_loss\n this should\n    be \nmin\n, etc. In \nauto\n mode, the direction is\n    automatically inferred from the name of the monitored quantity.\n\n\nsave_weights_only\n: if True, then only the model's weights will be\n    saved (\nmodel.save_weights(filepath)\n), else the full model\n    is saved (\nmodel.save(filepath)\n).\n\n\nperiod\n: Interval (number of epochs) between checkpoints.\n\n\n\n\n\n\n[source]\n\n\nProgbarLogger\n\n\nkeras.callbacks.ProgbarLogger()\n\n\n\n\nCallback that prints metrics to stdout.\n\n\n\n\nCreate a callback\n\n\nYou can create a custom callback by extending the base class \nkeras.callbacks.Callback\n. A callback has access to its associated model through the class property \nself.model\n.\n\n\nHere's a simple example saving a list of losses over each batch during training:\n\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n\n\n\n\n\n\nExample: recording loss history\n\n\nclass LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=784, init='uniform'))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nhistory = LossHistory()\nmodel.fit(X_train, Y_train, batch_size=128, nb_epoch=20, verbose=0, callbacks=[history])\n\nprint history.losses\n# outputs\n'''\n[0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789]\n'''\n\n\n\n\n\n\nExample: model checkpoints\n\n\nfrom keras.callbacks import ModelCheckpoint\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=784, init='uniform'))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n'''\nsaves the model weights after each epoch if the validation loss decreased\n'''\ncheckpointer = ModelCheckpoint(filepath=\n/tmp/weights.hdf5\n, verbose=1, save_best_only=True)\nmodel.fit(X_train, Y_train, batch_size=128, nb_epoch=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])", 
            "title": "Callbacks"
        }, 
        {
            "location": "/callbacks/#usage-of-callbacks", 
            "text": "A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument  callbacks ) to the  .fit()  method of the  Sequential  model. The relevant methods of the callbacks will then be called at each stage of the training.    [source]", 
            "title": "Usage of callbacks"
        }, 
        {
            "location": "/callbacks/#callback", 
            "text": "keras.callbacks.Callback()  Abstract base class used to build new callbacks.  Properties   params : dict. Training parameters\n    (eg. verbosity, batch size, number of epochs...).  model : instance of  keras.models.Model .\n    Reference of the model being trained.   The  logs  dictionary that callback methods\ntake as argument will contain keys for quantities relevant to\nthe current batch or epoch.  Currently, the  .fit()  method of the  Sequential  model class\nwill include the following quantities in the  logs  that\nit passes to its callbacks:   on_epoch_end : logs include  acc  and  loss , and\n    optionally include  val_loss \n    (if validation is enabled in  fit ), and  val_acc \n    (if validation and accuracy monitoring are enabled).  on_batch_begin : logs include  size ,\n    the number of samples in the current batch.  on_batch_end : logs include  loss , and optionally  acc \n    (if accuracy monitoring is enabled).    [source]", 
            "title": "Callback"
        }, 
        {
            "location": "/callbacks/#baselogger", 
            "text": "keras.callbacks.BaseLogger()  Callback that accumulates epoch averages of metrics.  This callback is automatically applied to every Keras model.   [source]", 
            "title": "BaseLogger"
        }, 
        {
            "location": "/callbacks/#lambdacallback", 
            "text": "keras.callbacks.LambdaCallback(on_epoch_begin=None, on_epoch_end=None, on_batch_begin=None, on_batch_end=None, on_train_begin=None, on_train_end=None)  Callback for creating simple, custom callbacks on-the-fly.  This callback is constructed with anonymous functions that will be called\nat the appropriate time. Note that the callbacks expects positional\narguments, as:\n -  on_epoch_begin  and  on_epoch_end  expect two positional arguments: epoch ,  logs \n -  on_batch_begin  and  on_batch_end  expect two positional arguments: batch ,  logs \n -  on_train_begin  and  on_train_end  expect one positional argument: logs  Arguments   on_epoch_begin : called at the beginning of every epoch.  on_epoch_end : called at the end of every epoch.  on_batch_begin : called at the beginning of every batch.  on_batch_end : called at the end of every batch.  on_train_begin : called at the beginning of model training.  on_train_end : called at the end of model training.   Example  # Print the batch number at the beginning of every batch.\nbatch_print_callback = LambdaCallback(\n    on_batch_begin=lambda batch,logs: print(batch))\n\n# Plot the loss after every epoch.\nimport numpy as np\nimport matplotlib.pyplot as plt\nplot_loss_callback = LambdaCallback(\n    on_epoch_end=lambda epoch, logs: plt.plot(np.arange(epoch),\n                      logs['loss']))\n\n# Terminate some processes after having finished model training.\nprocesses = ...\ncleanup_callback = LambdaCallback(\n    on_train_end=lambda logs: [\n    p.terminate() for p in processes if p.is_alive()])\n\nmodel.fit(...,\n      callbacks=[batch_print_callback,\n         plot_loss_callback,\n         cleanup_callback])   [source]", 
            "title": "LambdaCallback"
        }, 
        {
            "location": "/callbacks/#earlystopping", 
            "text": "keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')  Stop training when a monitored quantity has stopped improving.  Arguments   monitor : quantity to be monitored.  min_delta : minimum change in the monitored quantity\n    to qualify as an improvement, i.e. an absolute\n    change of less than min_delta, will count as no\n    improvement.  patience : number of epochs with no improvement\n    after which training will be stopped.  verbose : verbosity mode.  mode : one of {auto, min, max}. In  min  mode,\n    training will stop when the quantity\n    monitored has stopped decreasing; in  max \n    mode it will stop when the quantity\n    monitored has stopped increasing; in  auto \n    mode, the direction is automatically inferred\n    from the name of the monitored quantity.    [source]", 
            "title": "EarlyStopping"
        }, 
        {
            "location": "/callbacks/#remotemonitor", 
            "text": "keras.callbacks.RemoteMonitor(root='http://localhost:9000', path='/publish/epoch/end/', field='data', headers=None)  Callback used to stream events to a server.  Requires the  requests  library.\nEvents are sent to  root + '/publish/epoch/end/'  by default. Calls are\nHTTP POST, with a  data  argument which is a\nJSON-encoded dictionary of event data.  Arguments   root : String; root url of the target server.  path : String; path relative to  root  to which the events will be sent.  field : String; JSON field under which the data will be stored.  headers : Dictionary; optional custom HTTP headers.\n    Defaults to:  `{'Accept' : 'application/json',  'Content-Type' : 'application/json'}`      [source]", 
            "title": "RemoteMonitor"
        }, 
        {
            "location": "/callbacks/#learningratescheduler", 
            "text": "keras.callbacks.LearningRateScheduler(schedule)  Learning rate scheduler.  Arguments   schedule : a function that takes an epoch index as input\n    (integer, indexed from 0) and returns a new\n    learning rate as output (float).    [source]", 
            "title": "LearningRateScheduler"
        }, 
        {
            "location": "/callbacks/#tensorboard", 
            "text": "keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False)  Tensorboard basic visualizations.  This callback writes a log for TensorBoard, which allows\nyou to visualize dynamic graphs of your training and test\nmetrics, as well as activation histograms for the different\nlayers in your model.  TensorBoard is a visualization tool provided with TensorFlow.  If you have installed TensorFlow with pip, you should be able\nto launch TensorBoard from the command line:  tensorboard --logdir=/full_path_to_your_logs  You can find more information about TensorBoard\n- __ here .  Arguments   log_dir : the path of the directory where to save the log\n    files to be parsed by Tensorboard  histogram_freq : frequency (in epochs) at which to compute activation\n    histograms for the layers of the model. If set to 0,\n    histograms won't be computed.  write_graph : whether to visualize the graph in Tensorboard.\n    The log file can become quite large when\n    write_graph is set to True.    [source]", 
            "title": "TensorBoard"
        }, 
        {
            "location": "/callbacks/#reducelronplateau", 
            "text": "keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)  Reduce learning rate when a metric has stopped improving.  Models often benefit from reducing the learning rate by a factor\nof 2-10 once learning stagnates. This callback monitors a\nquantity and if no improvement is seen for a 'patience' number\nof epochs, the learning rate is reduced.  Example      reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                  patience=5, min_lr=0.001)\n    model.fit(X_train, Y_train, callbacks=[reduce_lr])  Arguments   monitor : quantity to be monitored.  factor : factor by which the learning rate will\n    be reduced. new_lr = lr * factor  patience : number of epochs with no improvement\n    after which learning rate will be reduced.  verbose : int. 0: quiet, 1: update messages.  mode : one of {auto, min, max}. In  min  mode,\n    lr will be reduced when the quantity\n    monitored has stopped decreasing; in  max \n    mode it will be reduced when the quantity\n    monitored has stopped increasing; in  auto \n    mode, the direction is automatically inferred\n    from the name of the monitored quantity.  epsilon : threshold for measuring the new optimum,\n    to only focus on significant changes.  cooldown : number of epochs to wait before resuming\n    normal operation after lr has been reduced.  min_lr : lower bound on the learning rate.    [source]", 
            "title": "ReduceLROnPlateau"
        }, 
        {
            "location": "/callbacks/#csvlogger", 
            "text": "keras.callbacks.CSVLogger(filename, separator=',', append=False)  Callback that streams epoch results to a csv file.  Supports all values that can be represented as a string,\nincluding 1D iterables such as np.ndarray.  Example      csv_logger = CSVLogger('training.log')\n    model.fit(X_train, Y_train, callbacks=[csv_logger])  Arguments   filename : filename of the csv file, e.g. 'run/log.csv'.  separator : string used to separate elements in the csv file.  append : True: append if file exists (useful for continuing\n    training). False: overwrite existing file,    [source]", 
            "title": "CSVLogger"
        }, 
        {
            "location": "/callbacks/#history", 
            "text": "keras.callbacks.History()  Callback that records events into a  History  object.  This callback is automatically applied to\nevery Keras model. The  History  object\ngets returned by the  fit  method of models.   [source]", 
            "title": "History"
        }, 
        {
            "location": "/callbacks/#modelcheckpoint", 
            "text": "keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)  Save the model after every epoch.  filepath  can contain named formatting options,\nwhich will be filled the value of  epoch  and\nkeys in  logs  (passed in  on_epoch_end ).  For example: if  filepath  is  weights.{epoch:02d}-{val_loss:.2f}.hdf5 ,\nthen the model checkpoints will be saved with the epoch number and\nthe validation loss in the filename.  Arguments   filepath : string, path to save the model file.  monitor : quantity to monitor.  verbose : verbosity mode, 0 or 1.  save_best_only : if  save_best_only=True ,\n    the latest best model according to\n    the quantity monitored will not be overwritten.  mode : one of {auto, min, max}.\n    If  save_best_only=True , the decision\n    to overwrite the current save file is made\n    based on either the maximization or the\n    minimization of the monitored quantity. For  val_acc ,\n    this should be  max , for  val_loss  this should\n    be  min , etc. In  auto  mode, the direction is\n    automatically inferred from the name of the monitored quantity.  save_weights_only : if True, then only the model's weights will be\n    saved ( model.save_weights(filepath) ), else the full model\n    is saved ( model.save(filepath) ).  period : Interval (number of epochs) between checkpoints.    [source]", 
            "title": "ModelCheckpoint"
        }, 
        {
            "location": "/callbacks/#progbarlogger", 
            "text": "keras.callbacks.ProgbarLogger()  Callback that prints metrics to stdout.", 
            "title": "ProgbarLogger"
        }, 
        {
            "location": "/callbacks/#create-a-callback", 
            "text": "You can create a custom callback by extending the base class  keras.callbacks.Callback . A callback has access to its associated model through the class property  self.model .  Here's a simple example saving a list of losses over each batch during training:  class LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))", 
            "title": "Create a callback"
        }, 
        {
            "location": "/callbacks/#example-recording-loss-history", 
            "text": "class LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self.losses = []\n\n    def on_batch_end(self, batch, logs={}):\n        self.losses.append(logs.get('loss'))\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=784, init='uniform'))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\nhistory = LossHistory()\nmodel.fit(X_train, Y_train, batch_size=128, nb_epoch=20, verbose=0, callbacks=[history])\n\nprint history.losses\n# outputs\n'''\n[0.66047596406559383, 0.3547245744908703, ..., 0.25953155204159617, 0.25901699725311789]\n'''", 
            "title": "Example: recording loss history"
        }, 
        {
            "location": "/callbacks/#example-model-checkpoints", 
            "text": "from keras.callbacks import ModelCheckpoint\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=784, init='uniform'))\nmodel.add(Activation('softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n\n'''\nsaves the model weights after each epoch if the validation loss decreased\n'''\ncheckpointer = ModelCheckpoint(filepath= /tmp/weights.hdf5 , verbose=1, save_best_only=True)\nmodel.fit(X_train, Y_train, batch_size=128, nb_epoch=20, verbose=0, validation_data=(X_test, Y_test), callbacks=[checkpointer])", 
            "title": "Example: model checkpoints"
        }, 
        {
            "location": "/datasets/", 
            "text": "Datasets\n\n\nCIFAR10 small image classification\n\n\nDataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.\n\n\nUsage:\n\n\nfrom keras.datasets import cifar10\n\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n\n\n\n\n\n\nReturn:\n\n\n2 tuples:\n\n\nX_train, X_test\n: uint8 array of RGB image data with shape (nb_samples, 3, 32, 32).\n\n\ny_train, y_test\n: uint8 array of category labels (integers in range 0-9) with shape (nb_samples,).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCIFAR100 small image classification\n\n\nDataset of 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images.\n\n\nUsage:\n\n\nfrom keras.datasets import cifar100\n\n(X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')\n\n\n\n\n\n\n\n\nReturn:\n\n\n\n\n2 tuples:\n\n\nX_train, X_test\n: uint8 array of RGB image data with shape (nb_samples, 3, 32, 32).\n\n\ny_train, y_test\n: uint8 array of category labels with shape (nb_samples,).\n\n\n\n\n\n\n\n\n\n\n\n\nArguments:\n\n\n\n\nlabel_mode\n: \"fine\" or \"coarse\".\n\n\n\n\n\n\n\n\n\n\nIMDB Movie reviews sentiment classification\n\n\nDataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a \nsequence\n of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n\n\nAs a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n\n\nUsage:\n\n\nfrom keras.datasets import imdb\n\n(X_train, y_train), (X_test, y_test) = imdb.load_data(path=\nimdb_full.pkl\n,\n                                                      nb_words=None,\n                                                      skip_top=0,\n                                                      maxlen=None,\n                                                      seed=113,\n                                                      start_char=1,\n                                                      oov_char=2,\n                                                      index_from=3)\n\n\n\n\n\n\n\n\nReturn:\n\n\n\n\n2 tuples:\n\n\nX_train, X_test\n: list of sequences, which are lists of indexes (integers). If the nb_words argument was specific, the maximum possible index value is nb_words-1. If the maxlen argument was specified, the largest possible sequence length is maxlen.\n\n\ny_train, y_test\n: list of integer labels (1 or 0). \n\n\n\n\n\n\n\n\n\n\n\n\nArguments:\n\n\n\n\npath\n: if you do have the data locally (at \n'~/.keras/datasets/' + path\n), if will be downloaded to this location (in cPickle format).\n\n\nnb_words\n: integer or None. Top most frequent words to consider. Any less frequent word will appear as 0 in the sequence data.\n\n\nskip_top\n: integer. Top most frequent words to ignore (they will appear as 0s in the sequence data).\n\n\nmaxlen\n: int. Maximum sequence length. Any longer sequence will be truncated.\n\n\nseed\n: int. Seed for reproducible data shuffling.\n\n\nstart_char\n: char. The start of a sequence will be marked with this character.\n    Set to 1 because 0 is usually the padding character.\n\n\noov_char\n: char. words that were cut out because of the \nnb_words\n\n    or \nskip_top\n limit will be replaced with this character.\n\n\nindex_from\n: int. Index actual words with this index and higher.\n\n\n\n\n\n\n\n\n\n\nReuters newswire topics classification\n\n\nDataset of 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions).\n\n\nUsage:\n\n\nfrom keras.datasets import reuters\n\n(X_train, y_train), (X_test, y_test) = reuters.load_data(path=\nreuters.pkl\n,\n                                                         nb_words=None,\n                                                         skip_top=0,\n                                                         maxlen=None,\n                                                         test_split=0.2,\n                                                         seed=113,\n                                                         start_char=1,\n                                                         oov_char=2,\n                                                         index_from=3)\n\n\n\n\nThe specifications are the same as that of the IMDB dataset, with the addition of:\n\n\n- __test_split__: float. Fraction of the dataset to be used as test data.\n\n\n\nThis dataset also makes available the word index used for encoding the sequences:\n\n\nword_index = reuters.get_word_index(path=\nreuters_word_index.pkl\n)\n\n\n\n\n\n\n\n\nReturn:\n A dictionary where key are words (str) and values are indexes (integer). eg. \nword_index[\"giraffe\"]\n might return \n1234\n. \n\n\n\n\n\n\nArguments:\n\n\n\n\npath\n: if you do have the index file locally (at \n'~/.keras/datasets/' + path\n), if will be downloaded to this location (in cPickle format).\n\n\n\n\n\n\n\n\nMNIST database of handwritten digits\n\n\nDataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n\n\nUsage:\n\n\nfrom keras.datasets import mnist\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\n\n\n\n\n\n\n\n\nReturn:\n\n\n\n\n2 tuples:\n\n\nX_train, X_test\n: uint8 array of grayscale image data with shape (nb_samples, 28, 28).\n\n\ny_train, y_test\n: uint8 array of digit labels (integers in range 0-9) with shape (nb_samples,).\n\n\n\n\n\n\n\n\n\n\n\n\nArguments:\n\n\n\n\npath\n: if you do have the index file locally (at \n'~/.keras/datasets/' + path\n), if will be downloaded to this location (in cPickle format).", 
            "title": "Datasets"
        }, 
        {
            "location": "/datasets/#datasets", 
            "text": "", 
            "title": "Datasets"
        }, 
        {
            "location": "/datasets/#cifar10-small-image-classification", 
            "text": "Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.", 
            "title": "CIFAR10 small image classification"
        }, 
        {
            "location": "/datasets/#usage", 
            "text": "from keras.datasets import cifar10\n\n(X_train, y_train), (X_test, y_test) = cifar10.load_data()   Return:  2 tuples:  X_train, X_test : uint8 array of RGB image data with shape (nb_samples, 3, 32, 32).  y_train, y_test : uint8 array of category labels (integers in range 0-9) with shape (nb_samples,).", 
            "title": "Usage:"
        }, 
        {
            "location": "/datasets/#cifar100-small-image-classification", 
            "text": "Dataset of 50,000 32x32 color training images, labeled over 100 categories, and 10,000 test images.", 
            "title": "CIFAR100 small image classification"
        }, 
        {
            "location": "/datasets/#usage_1", 
            "text": "from keras.datasets import cifar100\n\n(X_train, y_train), (X_test, y_test) = cifar100.load_data(label_mode='fine')    Return:   2 tuples:  X_train, X_test : uint8 array of RGB image data with shape (nb_samples, 3, 32, 32).  y_train, y_test : uint8 array of category labels with shape (nb_samples,).       Arguments:   label_mode : \"fine\" or \"coarse\".", 
            "title": "Usage:"
        }, 
        {
            "location": "/datasets/#imdb-movie-reviews-sentiment-classification", 
            "text": "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a  sequence  of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".  As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.", 
            "title": "IMDB Movie reviews sentiment classification"
        }, 
        {
            "location": "/datasets/#usage_2", 
            "text": "from keras.datasets import imdb\n\n(X_train, y_train), (X_test, y_test) = imdb.load_data(path= imdb_full.pkl ,\n                                                      nb_words=None,\n                                                      skip_top=0,\n                                                      maxlen=None,\n                                                      seed=113,\n                                                      start_char=1,\n                                                      oov_char=2,\n                                                      index_from=3)    Return:   2 tuples:  X_train, X_test : list of sequences, which are lists of indexes (integers). If the nb_words argument was specific, the maximum possible index value is nb_words-1. If the maxlen argument was specified, the largest possible sequence length is maxlen.  y_train, y_test : list of integer labels (1 or 0).        Arguments:   path : if you do have the data locally (at  '~/.keras/datasets/' + path ), if will be downloaded to this location (in cPickle format).  nb_words : integer or None. Top most frequent words to consider. Any less frequent word will appear as 0 in the sequence data.  skip_top : integer. Top most frequent words to ignore (they will appear as 0s in the sequence data).  maxlen : int. Maximum sequence length. Any longer sequence will be truncated.  seed : int. Seed for reproducible data shuffling.  start_char : char. The start of a sequence will be marked with this character.\n    Set to 1 because 0 is usually the padding character.  oov_char : char. words that were cut out because of the  nb_words \n    or  skip_top  limit will be replaced with this character.  index_from : int. Index actual words with this index and higher.", 
            "title": "Usage:"
        }, 
        {
            "location": "/datasets/#reuters-newswire-topics-classification", 
            "text": "Dataset of 11,228 newswires from Reuters, labeled over 46 topics. As with the IMDB dataset, each wire is encoded as a sequence of word indexes (same conventions).", 
            "title": "Reuters newswire topics classification"
        }, 
        {
            "location": "/datasets/#usage_3", 
            "text": "from keras.datasets import reuters\n\n(X_train, y_train), (X_test, y_test) = reuters.load_data(path= reuters.pkl ,\n                                                         nb_words=None,\n                                                         skip_top=0,\n                                                         maxlen=None,\n                                                         test_split=0.2,\n                                                         seed=113,\n                                                         start_char=1,\n                                                         oov_char=2,\n                                                         index_from=3)  The specifications are the same as that of the IMDB dataset, with the addition of:  - __test_split__: float. Fraction of the dataset to be used as test data.  This dataset also makes available the word index used for encoding the sequences:  word_index = reuters.get_word_index(path= reuters_word_index.pkl )    Return:  A dictionary where key are words (str) and values are indexes (integer). eg.  word_index[\"giraffe\"]  might return  1234 .     Arguments:   path : if you do have the index file locally (at  '~/.keras/datasets/' + path ), if will be downloaded to this location (in cPickle format).", 
            "title": "Usage:"
        }, 
        {
            "location": "/datasets/#mnist-database-of-handwritten-digits", 
            "text": "Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.", 
            "title": "MNIST database of handwritten digits"
        }, 
        {
            "location": "/datasets/#usage_4", 
            "text": "from keras.datasets import mnist\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()    Return:   2 tuples:  X_train, X_test : uint8 array of grayscale image data with shape (nb_samples, 28, 28).  y_train, y_test : uint8 array of digit labels (integers in range 0-9) with shape (nb_samples,).       Arguments:   path : if you do have the index file locally (at  '~/.keras/datasets/' + path ), if will be downloaded to this location (in cPickle format).", 
            "title": "Usage:"
        }, 
        {
            "location": "/applications/", 
            "text": "Applications\n\n\nKeras Applications are deep learning models that are made available alongside pre-trained weights.\nThese models can be used for prediction, feature extraction, and fine-tuning.\n\n\nWeights are downloaded automatically when instantiating a model. They are stored at \n~/.keras/models/\n.\n\n\nAvailable models\n\n\nModels for image classification with weights trained on ImageNet:\n\n\n\n\nXception\n\n\nVGG16\n\n\nVGG19\n\n\nResNet50\n\n\nInceptionV3\n\n\n\n\nAll of these architectures (except Xception) are compatible with both TensorFlow and Theano, and upon instantiation the models will be built according to the image dimension ordering set in your Keras configuration file at \n~/.keras/keras.json\n. For instance, if you have set \nimage_dim_ordering=tf\n, then any model loaded from this repository will get built according to the TensorFlow dimension ordering convention, \"Width-Height-Depth\".\n\n\nThe Xception model is only available for TensorFlow, due to its reliance on \nSeparableConvolution\n layers.\n\n\nModel for music audio file auto-tagging (taking as input Mel-spectrograms):\n\n\n\n\nMusicTaggerCRNN\n\n\n\n\n\n\nUsage examples for image classification models\n\n\nClassify ImageNet classes with ResNet50\n\n\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\nimport numpy as np\n\nmodel = ResNet50(weights='imagenet')\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\npreds = model.predict(x)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]\n\n\n\n\nExtract features with VGG16\n\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\nmodel = VGG16(weights='imagenet', include_top=False)\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nfeatures = model.predict(x)\n\n\n\n\nExtract features from an arbitrary intermediate layer with VGG19\n\n\nfrom keras.applications.vgg19 import VGG19\nfrom keras.preprocessing import image\nfrom keras.applications.vgg19 import preprocess_input\nfrom keras.models import Model\nimport numpy as np\n\nbase_model = VGG19(weights='imagenet')\nmodel = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nblock4_pool_features = model.predict(x)\n\n\n\n\nFine-tune InceptionV3 on a new set of classes\n\n\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\n\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer -- let's say we have 200 classes\npredictions = Dense(200, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(input=base_model.input, output=predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n# train the model on the new data for a few epochs\nmodel.fit_generator(...)\n\n# at this point, the top layers are well trained and we can start fine-tuning\n# convolutional layers from inception V3. We will freeze the bottom N layers\n# and train the remaining top layers.\n\n# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\nfor i, layer in enumerate(base_model.layers):\n   print(i, layer.name)\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 172 layers and unfreeze the rest:\nfor layer in model.layers[:172]:\n   layer.trainable = False\nfor layer in model.layers[172:]:\n   layer.trainable = True\n\n# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nfrom keras.optimizers import SGD\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\nmodel.fit_generator(...)\n\n\n\n\nBuild InceptionV3 over a custom input tensor\n\n\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.layers import Input\n\n# this could also be the output a different Keras model or layer\ninput_tensor = Input(shape=(224, 224, 3))  # this assumes K.image_dim_ordering() == 'tf'\n\nmodel = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)\n\n\n\n\n\n\nDocumentation for individual models\n\n\n\n\nXception\n\n\nVGG16\n\n\nVGG19\n\n\nResNet50\n\n\nInceptionV3\n\n\nMusicTaggerCRNN\n\n\n\n\n\n\nXception\n\n\nkeras.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None)\n\n\n\n\nXception V1 model, with weights pre-trained on ImageNet.\n\n\nOn ImageNet, this model gets to a top-1 validation accuracy of 0.790\nand a top-5 validation accuracy of 0.945.\n\n\nNote that this model is only available for the TensorFlow backend,\ndue to its reliance on \nSeparableConvolution\n layers. Additionally it only supports\nthe dimension ordering \"tf\" (width, height, channels).\n\n\nThe default input size for this model is 299x299.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the fully-connected layer at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \"imagenet\" (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\n\n\nReturns\n\n\nA Keras model instance.\n\n\nReferences\n\n\n\n\nXception: Deep Learning with Depthwise Separable Convolutions\n\n\n\n\nLicense\n\n\nThese weights are trained by ourselves and are released under the MIT license.\n\n\n\n\nVGG16\n\n\nkeras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None)\n\n\n\n\nVGG16 model, with weights pre-trained on ImageNet.\n\n\nThis model is available for both the Theano and TensorFlow backend, and can be built both\nwith \"th\" dim ordering (channels, width, height) or \"tf\" dim ordering (width, height, channels).\n\n\nThe default input size for this model is 224x224.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the 3 fully-connected layers at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \"imagenet\" (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\n\n\nReturns\n\n\nA Keras model instance.\n\n\nReferences\n\n\n\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition\n: please cite this paper if you use the VGG models in your work.\n\n\n\n\nLicense\n\n\nThese weights are ported from the ones \nreleased by VGG at Oxford\n under the \nCreative Commons Attribution License\n.\n\n\n\n\nVGG19\n\n\nkeras.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None)\n\n\n\n\nVGG19 model, with weights pre-trained on ImageNet.\n\n\nThis model is available for both the Theano and TensorFlow backend, and can be built both\nwith \"th\" dim ordering (channels, width, height) or \"tf\" dim ordering (width, height, channels).\n\n\nThe default input size for this model is 224x224.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the 3 fully-connected layers at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \"imagenet\" (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\n\n\nReturns\n\n\nA Keras model instance.\n\n\nReferences\n\n\n\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition\n\n\n\n\nLicense\n\n\nThese weights are ported from the ones \nreleased by VGG at Oxford\n under the \nCreative Commons Attribution License\n.\n\n\n\n\nResNet50\n\n\nkeras.applications.resnet50.ResNet50(include_top=True, weights='imagenet', input_tensor=None)\n\n\n\n\nResNet50 model, with weights pre-trained on ImageNet.\n\n\nThis model is available for both the Theano and TensorFlow backend, and can be built both\nwith \"th\" dim ordering (channels, width, height) or \"tf\" dim ordering (width, height, channels).\n\n\nThe default input size for this model is 224x224.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the fully-connected layer at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \"imagenet\" (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\n\n\nReturns\n\n\nA Keras model instance.\n\n\nReferences\n\n\n\n\nDeep Residual Learning for Image Recognition\n\n\n\n\nLicense\n\n\nThese weights are ported from the ones \nreleased by Kaiming He\n under the \nMIT license\n.\n\n\n\n\nInceptionV3\n\n\nkeras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None)\n\n\n\n\nInception V3 model, with weights pre-trained on ImageNet.\n\n\nThis model is available for both the Theano and TensorFlow backend, and can be built both\nwith \"th\" dim ordering (channels, width, height) or \"tf\" dim ordering (width, height, channels).\n\n\nThe default input size for this model is 299x299.\n\n\nArguments\n\n\n\n\ninclude_top: whether to include the fully-connected layer at the top of the network.\n\n\nweights: one of \nNone\n (random initialization) or \"imagenet\" (pre-training on ImageNet).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\n\n\nReturns\n\n\nA Keras model instance.\n\n\nReferences\n\n\n\n\nRethinking the Inception Architecture for Computer Vision\n\n\n\n\nLicense\n\n\nThese weights are trained by ourselves and are released under the MIT license.\n\n\n\n\nMusicTaggerCRNN\n\n\nkeras.applications.music_tagger_crnn.MusicTaggerCRNN(weights='msd', input_tensor=None, include_top=True)\n\n\n\n\nA convolutional-recurrent model taking as input a vectorized representation of the MelSpectrogram of a music track and capable of outputting the musical genre of the track. You can use \nkeras.applications.music_tagger_crnn.preprocess_input\n to convert a sound file to a vectorized spectrogram. This requires to have installed the \nLibrosa\n library. See \nthe usage example\n.\n\n\nArguments\n\n\n\n\nweights: one of \nNone\n (random initialization) or \"msd\" (pre-training on \nMillion Song Dataset\n).\n\n\ninput_tensor: optional Keras tensor (i.e. output of \nlayers.Input()\n) to use as image input for the model.\n\n\ninclude_top: whether to include the 1 fully-connected layer (output layer) at the top of the network. If False, the network outputs 32-dim features.\n\n\n\n\nReturns\n\n\nA Keras model instance.\n\n\nReferences\n\n\n\n\nConvolutional Recurrent Neural Networks for Music Classification\n\n\n\n\nLicense\n\n\nThese weights are ported from the ones \nreleased by Keunwoo Choi\n under the \nMIT license\n.\n\n\nExamples: music tagging and audio feature extraction\n\n\nfrom keras.applications.music_tagger_crnn import MusicTaggerCRNN\nfrom keras.applications.music_tagger_crnn import preprocess_input, decode_predictions\nimport numpy as np\n\n# 1. Tagging\nmodel = MusicTaggerCRNN(weights='msd')\n\naudio_path = 'audio_file.mp3'\nmelgram = preprocess_input(audio_path)\nmelgrams = np.expand_dims(melgram, axis=0)\n\npreds = model.predict(melgrams)\nprint('Predicted:')\nprint(decode_predictions(preds))\n# print: ('Predicted:', [[('rock', 0.097071797), ('pop', 0.042456303), ('alternative', 0.032439161), ('indie', 0.024491295), ('female vocalists', 0.016455274)]])\n\n#. 2. Feature extraction\nmodel = MusicTaggerCRNN(weights='msd', include_top=False)\n\naudio_path = 'audio_file.mp3'\nmelgram = preprocess_input(audio_path)\nmelgrams = np.expand_dims(melgram, axis=0)\n\nfeats = model.predict(melgrams)\nprint('Features:')\nprint(feats[0, :10])\n# print: ('Features:', [-0.19160545 0.94259131 -0.9991011 0.47644514 -0.19089699 0.99033844 0.1103896 -0.00340496 0.14823607 0.59856361])", 
            "title": "Applications"
        }, 
        {
            "location": "/applications/#applications", 
            "text": "Keras Applications are deep learning models that are made available alongside pre-trained weights.\nThese models can be used for prediction, feature extraction, and fine-tuning.  Weights are downloaded automatically when instantiating a model. They are stored at  ~/.keras/models/ .", 
            "title": "Applications"
        }, 
        {
            "location": "/applications/#available-models", 
            "text": "", 
            "title": "Available models"
        }, 
        {
            "location": "/applications/#models-for-image-classification-with-weights-trained-on-imagenet", 
            "text": "Xception  VGG16  VGG19  ResNet50  InceptionV3   All of these architectures (except Xception) are compatible with both TensorFlow and Theano, and upon instantiation the models will be built according to the image dimension ordering set in your Keras configuration file at  ~/.keras/keras.json . For instance, if you have set  image_dim_ordering=tf , then any model loaded from this repository will get built according to the TensorFlow dimension ordering convention, \"Width-Height-Depth\".  The Xception model is only available for TensorFlow, due to its reliance on  SeparableConvolution  layers.", 
            "title": "Models for image classification with weights trained on ImageNet:"
        }, 
        {
            "location": "/applications/#model-for-music-audio-file-auto-tagging-taking-as-input-mel-spectrograms", 
            "text": "MusicTaggerCRNN", 
            "title": "Model for music audio file auto-tagging (taking as input Mel-spectrograms):"
        }, 
        {
            "location": "/applications/#usage-examples-for-image-classification-models", 
            "text": "", 
            "title": "Usage examples for image classification models"
        }, 
        {
            "location": "/applications/#classify-imagenet-classes-with-resnet50", 
            "text": "from keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing import image\nfrom keras.applications.resnet50 import preprocess_input, decode_predictions\nimport numpy as np\n\nmodel = ResNet50(weights='imagenet')\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\npreds = model.predict(x)\n# decode the results into a list of tuples (class, description, probability)\n# (one such list for each sample in the batch)\nprint('Predicted:', decode_predictions(preds, top=3)[0])\n# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]", 
            "title": "Classify ImageNet classes with ResNet50"
        }, 
        {
            "location": "/applications/#extract-features-with-vgg16", 
            "text": "from keras.applications.vgg16 import VGG16\nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nimport numpy as np\n\nmodel = VGG16(weights='imagenet', include_top=False)\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nfeatures = model.predict(x)", 
            "title": "Extract features with VGG16"
        }, 
        {
            "location": "/applications/#extract-features-from-an-arbitrary-intermediate-layer-with-vgg19", 
            "text": "from keras.applications.vgg19 import VGG19\nfrom keras.preprocessing import image\nfrom keras.applications.vgg19 import preprocess_input\nfrom keras.models import Model\nimport numpy as np\n\nbase_model = VGG19(weights='imagenet')\nmodel = Model(input=base_model.input, output=base_model.get_layer('block4_pool').output)\n\nimg_path = 'elephant.jpg'\nimg = image.load_img(img_path, target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nx = preprocess_input(x)\n\nblock4_pool_features = model.predict(x)", 
            "title": "Extract features from an arbitrary intermediate layer with VGG19"
        }, 
        {
            "location": "/applications/#fine-tune-inceptionv3-on-a-new-set-of-classes", 
            "text": "from keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras import backend as K\n\n# create the base pre-trained model\nbase_model = InceptionV3(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = base_model.output\nx = GlobalAveragePooling2D()(x)\n# let's add a fully-connected layer\nx = Dense(1024, activation='relu')(x)\n# and a logistic layer -- let's say we have 200 classes\npredictions = Dense(200, activation='softmax')(x)\n\n# this is the model we will train\nmodel = Model(input=base_model.input, output=predictions)\n\n# first: train only the top layers (which were randomly initialized)\n# i.e. freeze all convolutional InceptionV3 layers\nfor layer in base_model.layers:\n    layer.trainable = False\n\n# compile the model (should be done *after* setting layers to non-trainable)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n\n# train the model on the new data for a few epochs\nmodel.fit_generator(...)\n\n# at this point, the top layers are well trained and we can start fine-tuning\n# convolutional layers from inception V3. We will freeze the bottom N layers\n# and train the remaining top layers.\n\n# let's visualize layer names and layer indices to see how many layers\n# we should freeze:\nfor i, layer in enumerate(base_model.layers):\n   print(i, layer.name)\n\n# we chose to train the top 2 inception blocks, i.e. we will freeze\n# the first 172 layers and unfreeze the rest:\nfor layer in model.layers[:172]:\n   layer.trainable = False\nfor layer in model.layers[172:]:\n   layer.trainable = True\n\n# we need to recompile the model for these modifications to take effect\n# we use SGD with a low learning rate\nfrom keras.optimizers import SGD\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n\n# we train our model again (this time fine-tuning the top 2 inception blocks\n# alongside the top Dense layers\nmodel.fit_generator(...)", 
            "title": "Fine-tune InceptionV3 on a new set of classes"
        }, 
        {
            "location": "/applications/#build-inceptionv3-over-a-custom-input-tensor", 
            "text": "from keras.applications.inception_v3 import InceptionV3\nfrom keras.layers import Input\n\n# this could also be the output a different Keras model or layer\ninput_tensor = Input(shape=(224, 224, 3))  # this assumes K.image_dim_ordering() == 'tf'\n\nmodel = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=True)", 
            "title": "Build InceptionV3 over a custom input tensor"
        }, 
        {
            "location": "/applications/#documentation-for-individual-models", 
            "text": "Xception  VGG16  VGG19  ResNet50  InceptionV3  MusicTaggerCRNN", 
            "title": "Documentation for individual models"
        }, 
        {
            "location": "/applications/#xception", 
            "text": "keras.applications.xception.Xception(include_top=True, weights='imagenet', input_tensor=None)  Xception V1 model, with weights pre-trained on ImageNet.  On ImageNet, this model gets to a top-1 validation accuracy of 0.790\nand a top-5 validation accuracy of 0.945.  Note that this model is only available for the TensorFlow backend,\ndue to its reliance on  SeparableConvolution  layers. Additionally it only supports\nthe dimension ordering \"tf\" (width, height, channels).  The default input size for this model is 299x299.", 
            "title": "Xception"
        }, 
        {
            "location": "/applications/#arguments", 
            "text": "include_top: whether to include the fully-connected layer at the top of the network.  weights: one of  None  (random initialization) or \"imagenet\" (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.", 
            "title": "Arguments"
        }, 
        {
            "location": "/applications/#returns", 
            "text": "A Keras model instance.", 
            "title": "Returns"
        }, 
        {
            "location": "/applications/#references", 
            "text": "Xception: Deep Learning with Depthwise Separable Convolutions", 
            "title": "References"
        }, 
        {
            "location": "/applications/#license", 
            "text": "These weights are trained by ourselves and are released under the MIT license.", 
            "title": "License"
        }, 
        {
            "location": "/applications/#vgg16", 
            "text": "keras.applications.vgg16.VGG16(include_top=True, weights='imagenet', input_tensor=None)  VGG16 model, with weights pre-trained on ImageNet.  This model is available for both the Theano and TensorFlow backend, and can be built both\nwith \"th\" dim ordering (channels, width, height) or \"tf\" dim ordering (width, height, channels).  The default input size for this model is 224x224.", 
            "title": "VGG16"
        }, 
        {
            "location": "/applications/#arguments_1", 
            "text": "include_top: whether to include the 3 fully-connected layers at the top of the network.  weights: one of  None  (random initialization) or \"imagenet\" (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.", 
            "title": "Arguments"
        }, 
        {
            "location": "/applications/#returns_1", 
            "text": "A Keras model instance.", 
            "title": "Returns"
        }, 
        {
            "location": "/applications/#references_1", 
            "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition : please cite this paper if you use the VGG models in your work.", 
            "title": "References"
        }, 
        {
            "location": "/applications/#license_1", 
            "text": "These weights are ported from the ones  released by VGG at Oxford  under the  Creative Commons Attribution License .", 
            "title": "License"
        }, 
        {
            "location": "/applications/#vgg19", 
            "text": "keras.applications.vgg19.VGG19(include_top=True, weights='imagenet', input_tensor=None)  VGG19 model, with weights pre-trained on ImageNet.  This model is available for both the Theano and TensorFlow backend, and can be built both\nwith \"th\" dim ordering (channels, width, height) or \"tf\" dim ordering (width, height, channels).  The default input size for this model is 224x224.", 
            "title": "VGG19"
        }, 
        {
            "location": "/applications/#arguments_2", 
            "text": "include_top: whether to include the 3 fully-connected layers at the top of the network.  weights: one of  None  (random initialization) or \"imagenet\" (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.", 
            "title": "Arguments"
        }, 
        {
            "location": "/applications/#returns_2", 
            "text": "A Keras model instance.", 
            "title": "Returns"
        }, 
        {
            "location": "/applications/#references_2", 
            "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition", 
            "title": "References"
        }, 
        {
            "location": "/applications/#license_2", 
            "text": "These weights are ported from the ones  released by VGG at Oxford  under the  Creative Commons Attribution License .", 
            "title": "License"
        }, 
        {
            "location": "/applications/#resnet50", 
            "text": "keras.applications.resnet50.ResNet50(include_top=True, weights='imagenet', input_tensor=None)  ResNet50 model, with weights pre-trained on ImageNet.  This model is available for both the Theano and TensorFlow backend, and can be built both\nwith \"th\" dim ordering (channels, width, height) or \"tf\" dim ordering (width, height, channels).  The default input size for this model is 224x224.", 
            "title": "ResNet50"
        }, 
        {
            "location": "/applications/#arguments_3", 
            "text": "include_top: whether to include the fully-connected layer at the top of the network.  weights: one of  None  (random initialization) or \"imagenet\" (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.", 
            "title": "Arguments"
        }, 
        {
            "location": "/applications/#returns_3", 
            "text": "A Keras model instance.", 
            "title": "Returns"
        }, 
        {
            "location": "/applications/#references_3", 
            "text": "Deep Residual Learning for Image Recognition", 
            "title": "References"
        }, 
        {
            "location": "/applications/#license_3", 
            "text": "These weights are ported from the ones  released by Kaiming He  under the  MIT license .", 
            "title": "License"
        }, 
        {
            "location": "/applications/#inceptionv3", 
            "text": "keras.applications.inception_v3.InceptionV3(include_top=True, weights='imagenet', input_tensor=None)  Inception V3 model, with weights pre-trained on ImageNet.  This model is available for both the Theano and TensorFlow backend, and can be built both\nwith \"th\" dim ordering (channels, width, height) or \"tf\" dim ordering (width, height, channels).  The default input size for this model is 299x299.", 
            "title": "InceptionV3"
        }, 
        {
            "location": "/applications/#arguments_4", 
            "text": "include_top: whether to include the fully-connected layer at the top of the network.  weights: one of  None  (random initialization) or \"imagenet\" (pre-training on ImageNet).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.", 
            "title": "Arguments"
        }, 
        {
            "location": "/applications/#returns_4", 
            "text": "A Keras model instance.", 
            "title": "Returns"
        }, 
        {
            "location": "/applications/#references_4", 
            "text": "Rethinking the Inception Architecture for Computer Vision", 
            "title": "References"
        }, 
        {
            "location": "/applications/#license_4", 
            "text": "These weights are trained by ourselves and are released under the MIT license.", 
            "title": "License"
        }, 
        {
            "location": "/applications/#musictaggercrnn", 
            "text": "keras.applications.music_tagger_crnn.MusicTaggerCRNN(weights='msd', input_tensor=None, include_top=True)  A convolutional-recurrent model taking as input a vectorized representation of the MelSpectrogram of a music track and capable of outputting the musical genre of the track. You can use  keras.applications.music_tagger_crnn.preprocess_input  to convert a sound file to a vectorized spectrogram. This requires to have installed the  Librosa  library. See  the usage example .", 
            "title": "MusicTaggerCRNN"
        }, 
        {
            "location": "/applications/#arguments_5", 
            "text": "weights: one of  None  (random initialization) or \"msd\" (pre-training on  Million Song Dataset ).  input_tensor: optional Keras tensor (i.e. output of  layers.Input() ) to use as image input for the model.  include_top: whether to include the 1 fully-connected layer (output layer) at the top of the network. If False, the network outputs 32-dim features.", 
            "title": "Arguments"
        }, 
        {
            "location": "/applications/#returns_5", 
            "text": "A Keras model instance.", 
            "title": "Returns"
        }, 
        {
            "location": "/applications/#references_5", 
            "text": "Convolutional Recurrent Neural Networks for Music Classification", 
            "title": "References"
        }, 
        {
            "location": "/applications/#license_5", 
            "text": "These weights are ported from the ones  released by Keunwoo Choi  under the  MIT license .", 
            "title": "License"
        }, 
        {
            "location": "/applications/#examples-music-tagging-and-audio-feature-extraction", 
            "text": "from keras.applications.music_tagger_crnn import MusicTaggerCRNN\nfrom keras.applications.music_tagger_crnn import preprocess_input, decode_predictions\nimport numpy as np\n\n# 1. Tagging\nmodel = MusicTaggerCRNN(weights='msd')\n\naudio_path = 'audio_file.mp3'\nmelgram = preprocess_input(audio_path)\nmelgrams = np.expand_dims(melgram, axis=0)\n\npreds = model.predict(melgrams)\nprint('Predicted:')\nprint(decode_predictions(preds))\n# print: ('Predicted:', [[('rock', 0.097071797), ('pop', 0.042456303), ('alternative', 0.032439161), ('indie', 0.024491295), ('female vocalists', 0.016455274)]])\n\n#. 2. Feature extraction\nmodel = MusicTaggerCRNN(weights='msd', include_top=False)\n\naudio_path = 'audio_file.mp3'\nmelgram = preprocess_input(audio_path)\nmelgrams = np.expand_dims(melgram, axis=0)\n\nfeats = model.predict(melgrams)\nprint('Features:')\nprint(feats[0, :10])\n# print: ('Features:', [-0.19160545 0.94259131 -0.9991011 0.47644514 -0.19089699 0.99033844 0.1103896 -0.00340496 0.14823607 0.59856361])", 
            "title": "Examples: music tagging and audio feature extraction"
        }, 
        {
            "location": "/backend/", 
            "text": "Keras backends\n\n\nWhat is a \"backend\"?\n\n\nKeras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.\n\n\nAt this time, Keras has two backend implementations available: the \nTensorFlow\n backend and the \nTheano\n backend.\n\n\n\n\nTensorFlow\n is an open-source symbolic tensor manipulation framework developed by Google, Inc.\n\n\nTheano\n is an open-source symbolic tensor manipulation framework developed by LISA/MILA Lab at Universit\u00e9 de Montr\u00e9al.\n\n\n\n\nIn the future, we are likely to add more backend options. If you are interested in developing a new backend, get in touch!\n\n\n\n\nSwitching from one backend to another\n\n\nIf you have run Keras at least once, you will find the Keras configuration file at:\n\n\n~/.keras/keras.json\n\n\nIf it isn't there, you can create it.\n\n\nThe default configuration file looks like this:\n\n\n{\n    \nimage_dim_ordering\n: \ntf\n,\n    \nepsilon\n: 1e-07,\n    \nfloatx\n: \nfloat32\n,\n    \nbackend\n: \ntensorflow\n\n}\n\n\n\n\nSimply change the field \nbackend\n to either \n\"theano\"\n or \n\"tensorflow\"\n, and Keras will use the new configuration next time you run any Keras code.\n\n\nYou can also define the environment variable \nKERAS_BACKEND\n and this will\noverride what is defined in your config file :\n\n\nKERAS_BACKEND=tensorflow python -c \nfrom keras import backend\n\nUsing TensorFlow backend.\n\n\n\n\n\n\nUsing the abstract Keras backend to write new code\n\n\nIf you want the Keras modules you write to be compatible with both Theano and TensorFlow, you have to write them via the abstract Keras backend API. Here's an intro.\n\n\nYou can import the backend module via:\n\n\nfrom keras import backend as K\n\n\n\n\nThe code below instantiates an input placeholder. It's equivalent to \ntf.placeholder()\n or \nT.matrix()\n, \nT.tensor3()\n, etc.\n\n\ninput = K.placeholder(shape=(2, 4, 5))\n# also works:\ninput = K.placeholder(shape=(None, 4, 5))\n# also works:\ninput = K.placeholder(ndim=3)\n\n\n\n\nThe code below instantiates a shared variable. It's equivalent to \ntf.variable()\n or \ntheano.shared()\n.\n\n\nval = np.random.random((3, 4, 5))\nvar = K.variable(value=val)\n\n# all-zeros variable:\nvar = K.zeros(shape=(3, 4, 5))\n# all-ones:\nvar = K.ones(shape=(3, 4, 5))\n\n\n\n\nMost tensor operations you will need can be done as you would in TensorFlow or Theano:\n\n\na = b + c * K.abs(d)\nc = K.dot(a, K.transpose(b))\na = K.sum(b, axis=2)\na = K.softmax(b)\na = concatenate([b, c], axis=-1)\n# etc...\n\n\n\n\n\n\nBackend functions\n\n\nepsilon\n\n\nepsilon()\n\n\n\n\nReturns the value of the fuzz\nfactor used in numeric expressions.\n\n\nReturns\n\n\nA float.\n\n\nExample\n\n\n keras.backend.epsilon()\n1e-08\n\n\n\n\n\n\nset_epsilon\n\n\nset_epsilon(e)\n\n\n\n\nSets the value of the fuzz\nfactor used in numeric expressions.\n\n\nArguments\n\n\n\n\ne\n: float. New value of epsilon.\n\n\n\n\nExample\n\n\n from keras import backend as K\n\n K.epsilon()\n1e-08\n\n K.set_epsilon(1e-05)\n\n K.epsilon()\n1e-05\n\n\n\n\n\n\nfloatx\n\n\nfloatx()\n\n\n\n\nReturns the default float type, as a string\n(e.g. 'float16', 'float32', 'float64').\n\n\nReturns\n\n\nString, the current default float type.\n\n\nExample\n\n\n keras.backend.floatx()\n'float32'\n\n\n\n\n\n\nset_floatx\n\n\nset_floatx(floatx)\n\n\n\n\nSets the default float type.\n\n\nArguments\n\n\n\n\nString\n: 'float16', 'float32', or 'float64'.\n\n\n\n\nExample\n\n\n from keras import backend as K\n\n K.floatx()\n'float32'\n\n K.set_floatx('float16')\n\n K.floatx()\n'float16'\n\n\n\n\n\n\ncast_to_floatx\n\n\ncast_to_floatx(x)\n\n\n\n\nCast a Numpy array to the default Keras float type.\n\n\nArguments\n\n\n\n\nx\n: Numpy array.\n\n\n\n\nReturns\n\n\nThe same Numpy array, cast to its new type.\n\n\nExample\n\n\n from keras import backend as K\n\n K.floatx()\n'float32'\n\n arr = numpy.array([1.0, 2.0], dtype='float64')\n\n arr.dtype\ndtype('float64')\n\n new_arr = K.cast_to_floatx(arr)\n\n new_arr\narray([ 1.,  2.], dtype=float32)\n\n new_arr.dtype\ndtype('float32')\n\n\n\n\n\n\nimage_dim_ordering\n\n\nimage_dim_ordering()\n\n\n\n\nReturns the default image dimension ordering\nconvention ('th' or 'tf').\n\n\nReturns\n\n\nA string, either \n'th'\n or \n'tf'\n\n\nExample\n\n\n keras.backend.image_dim_ordering()\n'th'\n\n\n\n\n\n\nset_image_dim_ordering\n\n\nset_image_dim_ordering(dim_ordering)\n\n\n\n\nSets the value of the image dimension\nordering convention ('th' or 'tf').\n\n\nArguments\n\n\n\n\ndim_ordering\n: string. \n'th'\n or \n'tf'\n.\n\n\n\n\nExample\n\n\n from keras import backend as K\n\n K.image_dim_ordering()\n'th'\n\n K.set_image_dim_ordering('tf')\n\n K.image_dim_ordering()\n'tf'\n\n\n\n\n\n\nget_uid\n\n\nget_uid(prefix='')\n\n\n\n\nProvides a unique UID given a string prefix.\n\n\nArguments\n\n\n\n\nprefix\n: string.\n\n\n\n\nReturns\n\n\nAn integer.\n\n\nExample\n\n\n keras.backend.get_uid('dense')\n\n 1\n\n keras.backend.get_uid('dense')\n\n 2\n\n\n\n\n\n\nis_keras_tensor\n\n\nis_keras_tensor(x)\n\n\n\n\nReturns whether \nx\n is a Keras tensor.\n\n\nArguments\n\n\n\n\nx\n: a potential tensor.\n\n\n\n\nReturns\n\n\nA boolean: whether the argument is a Keras tensor.\n\n\nExamples\n\n\n from keras import backend as K\n\n np_var = numpy.array([1, 2])\n\n K.is_keras_tensor(np_var)\nFalse\n\n keras_var = K.variable(np_var)\n\n K.is_keras_tensor(keras_var)  # A variable is not a Tensor.\nFalse\n\n keras_placeholder = K.placeholder(shape=(2, 4, 5))\n\n K.is_keras_tensor(keras_placeholder)  # A placeholder is a Tensor.\nTrue\n\n\n\n\n\n\nclear_session\n\n\nclear_session()\n\n\n\n\nDestroys the current TF graph and creates a new one.\n\n\nUseful to avoid clutter from old models / layers.\n\n\n\n\nmanual_variable_initialization\n\n\nmanual_variable_initialization(value)\n\n\n\n\nSets the manual variable initialization flag.\n\n\nThis boolean flag determines whether\nvariables should be initialized\nas they are instantiated (default), or if\nthe user should handle the initialization\n(e.g. via \ntf.initialize_all_variables()\n).\n\n\nArguments\n\n\n\n\nvalue\n: Python boolean.\n\n\n\n\n\n\nlearning_phase\n\n\nlearning_phase()\n\n\n\n\nReturns the learning phase flag.\n\n\nThe learning phase flag is a bool tensor (0 = test, 1 = train)\nto be passed as input to any Keras function\nthat uses a different behavior at train time and test time.\n\n\n\n\nset_learning_phase\n\n\nset_learning_phase(value)\n\n\n\n\nSets the learning phase to a fixed value,\neither 0 or 1 (integers).\n\n\nRaises\n\n\n\n\nValueError\n: if \nvalue\n is neither \n0\n nor \n1\n.\n\n\n\n\n\n\nis_sparse\n\n\nis_sparse(tensor)\n\n\n\n\nReturns whether a tensor is a sparse tensor.\n\n\nArguments\n\n\n\n\ntensor\n: A tensor instance.\n\n\n\n\nReturns\n\n\nA boolean.\n\n\nExample\n\n\n from keras import backend as K\n\n a = K.placeholder((2, 2), sparse=False)\n\n print(K.is_sparse(a))\nFalse\n\n b = K.placeholder((2, 2), sparse=True)\n\n print(K.is_sparse(b))\nTrue\n\n\n\n\n\n\nto_dense\n\n\nto_dense(tensor)\n\n\n\n\nConverts a sparse tensor into a dense tensor\nand returns it.\n\n\nArguments\n\n\n\n\ntensor\n: A tensor instance (potentially sparse).\n\n\n\n\nReturns\n\n\nA dense tensor.\n\n\nExamples\n\n\n from keras import backend as K\n\n b = K.placeholder((2, 2), sparse=True)\n\n print(K.is_sparse(b))\nTrue\n\n c = K.to_dense(b)\n\n print(K.is_sparse(c))\nFalse\n\n\n\n\n\n\nvariable\n\n\nvariable(value, dtype=None, name=None)\n\n\n\n\nInstantiates a variable and returns it.\n\n\nArguments\n\n\n\n\nvalue\n: Numpy array, initial value of the tensor.\n\n\ndtype\n: Tensor type.\n\n\nname\n: Optional name string for the tensor.\n\n\n\n\nReturns\n\n\nA variable instance (with Keras metadata included).\n\n\nExamples\n\n\n from keras import backend as K\n\n val = np.array([[1, 2], [3, 4]])\n\n kvar = K.variable(value=val, dtype='float64', name='example_var')\n\n K.dtype(kvar)\n'float64'\n\n print(kvar)\nexample_var\n\n kvar.eval()\narray([[ 1.,  2.],\n   [ 3.,  4.]])\n\n\n\n\n\n\nplaceholder\n\n\nplaceholder(shape=None, ndim=None, dtype=None, sparse=False, name=None)\n\n\n\n\nInstantiates a placeholder tensor and returns it.\n\n\nArguments\n\n\n\n\nshape\n: Shape of the placeholder\n(integer tuple, may include \nNone\n entries).\n\n\nndim\n: Number of axes of the tensor.\nAt least one of {\nshape\n, \nndim\n} must be specified.\nIf both are specified, \nshape\n is used.\n\n\ndtype\n: Placeholder type.\n\n\nname\n: Optional name string for the placeholder.\n\n\n\n\nReturns\n\n\nTensor instance (with Keras metadata included).\n\n\nExamples\n\n\n from keras import backend as K\n\n input_ph = K.placeholder(shape=(2, 4, 5))\n\n input_ph._keras_shape\n(2, 4, 5)\n\n input_ph\n\ntf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32\n\n\n\n\n\n\n\nshape\n\n\nshape(x)\n\n\n\n\nReturns the symbolic shape of a tensor or variable.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\n\n\nReturns\n\n\nA symbolic shape (which is itself a tensor).\n\n\nExamples\n\n\n__TensorFlow example__\n\n\n from keras import backend as K\n\n tf_session = K.get_session()\n\n val = np.array([[1, 2], [3, 4]])\n\n kvar = K.variable(value=val)\n\n input = keras.backend.placeholder(shape=(2, 4, 5))\n\n K.shape(kvar)\n\ntf.Tensor 'Shape_8:0' shape=(2,) dtype=int32\n\n\n K.shape(input)\n\ntf.Tensor 'Shape_9:0' shape=(3,) dtype=int32\n\n__To get integer shape (Instead, you can use K.int_shape(x))__\n\n\n K.shape(kvar).eval(session=tf_session)\narray([2, 2], dtype=int32)\n\n K.shape(input).eval(session=tf_session)\narray([2, 4, 5], dtype=int32)\n\n\n\n\n\n\nint_shape\n\n\nint_shape(x)\n\n\n\n\nReturns the shape of a Keras tensor or a Keras variable as a tuple of\nintegers or None entries.\n\n\nArguments\n\n\n\n\nx\n: Tensor or variable.\n\n\n\n\nReturns\n\n\nA tuple of integers (or None entries).\n\n\nExamples\n\n\n from keras import backend as K\n\n input = K.placeholder(shape=(2, 4, 5))\n\n K.int_shape(input)\n(2, 4, 5)\n\n val = np.array([[1, 2], [3, 4]])\n\n kvar = K.variable(value=val)\n\n K.int_shape(kvar)\n(2, 2)\n\n\n\n\n\n\nndim\n\n\nndim(x)\n\n\n\n\nReturns the number of axes in a tensor, as an integer.\n\n\nArguments\n\n\n\n\nx\n: Tensor or variable.\n\n\n\n\nReturns\n\n\nInteger (scalar), number of axes.\n\n\nExamples\n\n\n from keras import backend as K\n\n input = K.placeholder(shape=(2, 4, 5))\n\n val = np.array([[1, 2], [3, 4]])\n\n kvar = K.variable(value=val)\n\n K.ndim(input)\n3\n\n K.ndim(kvar)\n2\n\n\n\n\n\n\ndtype\n\n\ndtype(x)\n\n\n\n\nReturns the dtype of a Keras tensor or variable, as a string.\n\n\nArguments\n\n\n\n\nx\n: Tensor or variable.\n\n\n\n\nReturns\n\n\nString, dtype of \nx\n.\n\n\nExamples\n\n\n from keras import backend as K\n\n K.dtype(K.placeholder(shape=(2,4,5)))\n'float32'\n\n K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))\n'float32'\n\n K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))\n'float64'\n__Keras variable__\n\n\n kvar = K.variable(np.array([[1, 2], [3, 4]]))\n\n K.dtype(kvar)\n'float32_ref'\n\n kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n\n K.dtype(kvar)\n'float32_ref'\n\n\n\n\n\n\neval\n\n\neval(x)\n\n\n\n\nEvaluates the value of a variable.\nReturns a Numpy array.\n\n\nArguments\n\n\n\n\nx\n: A variable.\n\n\n\n\nReturns\n\n\nA Numpy array.\n\n\nExamples\n\n\n from keras import backend as K\n\n kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n\n K.eval(kvar)\narray([[ 1.,  2.],\n   [ 3.,  4.]], dtype=float32)\n\n\n\n\n\n\nzeros\n\n\nzeros(shape, dtype=None, name=None)\n\n\n\n\nInstantiates an all-zeros variable and returns it.\n\n\nArguments\n\n\n\n\nshape\n: Tuple of integers, shape of returned Keras variable\n\n\ndtype\n: String, data type of returned Keras variable\n\n\nname\n: String, name of returned Keras variable\n\n\n\n\nReturns\n\n\nA variable (including Keras metadata), filled with \n0.0\n.\n\n\nExample\n\n\n from keras import backend as K\n\n kvar = K.zeros((3,4))\n\n K.eval(kvar)\narray([[ 0.,  0.,  0.,  0.],\n   [ 0.,  0.,  0.,  0.],\n   [ 0.,  0.,  0.,  0.]], dtype=float32)\n\n\n\n\n\n\nones\n\n\nones(shape, dtype=None, name=None)\n\n\n\n\nInstantiates an all-ones tensor variable and returns it.\n\n\nArguments\n\n\n\n\nshape\n: Tuple of integers, shape of returned Keras variable.\n\n\ndtype\n: String, data type of returned Keras variable.\n\n\nname\n: String, name of returned Keras variable.\n\n\n\n\nReturns\n\n\nA Keras variable, filled with \n1.0\n.\n\n\nExample\n\n\n from keras import backend as K\n\n kvar = K.ones((3,4))\n\n K.eval(kvar)\narray([[ 1.,  1.,  1.,  1.],\n   [ 1.,  1.,  1.,  1.],\n   [ 1.,  1.,  1.,  1.]], dtype=float32)\n\n\n\n\n\n\neye\n\n\neye(size, dtype=None, name=None)\n\n\n\n\nInstantiate an identity matrix and returns it.\n\n\nArguments\n\n\n\n\nsize\n: Integer, number of rows/columns.\n\n\ndtype\n: String, data type of returned Keras variable.\n\n\nname\n: String, name of returned Keras variable.\n\n\n\n\nReturns\n\n\nA Keras variable, an identity matrix.\n\n\nExample\n\n\n from keras import backend as K\n\n kvar = K.eye(3)\n\n K.eval(kvar)\narray([[ 1.,  0.,  0.],\n   [ 0.,  1.,  0.],\n   [ 0.,  0.,  1.]], dtype=float32)\n\n\n\n\n\n\nzeros_like\n\n\nzeros_like(x, dtype=None, name=None)\n\n\n\n\nInstantiates an all-zeros Keras variable\nof the same shape as another Keras variable or tensor and returns it.\n\n\nArguments\n\n\n\n\nx\n: Keras variable or Keras tensor.\n\n\ndtype\n: String, dtype of returned Keras variable.\n None uses the dtype of x.\n\n\n\n\nReturns\n\n\nA Keras variable with the shape of x filled with zeros.\n\n\nExample\n\n\n from keras import backend as K\n\n kvar = K.variable(np.random.random((2,3)))\n\n kvar_zeros = K.zeros_like(kvar)\n\n K.eval(kvar_zeros)\narray([[ 0.,  0.,  0.],\n   [ 0.,  0.,  0.]], dtype=float32)\n\n\n\n\n\n\nones_like\n\n\nones_like(x, dtype=None, name=None)\n\n\n\n\nInstantiates an all-ones Keras variable\nof the same shape as another Keras variable or tensor and returns it.\n\n\nArguments\n\n\n\n\nx\n: Keras variable or tensor.\n\n\ndtype\n: String, dtype of returned Keras variable.\n None uses the dtype of x.\n\n\n\n\nReturns\n\n\nA Keras variable with the shape of x filled with ones.\n\n\nExample\n\n\n from keras import backend as K\n\n kvar = K.variable(np.random.random((2,3)))\n\n kvar_ones = K.ones_like(kvar)\n\n K.eval(kvar_ones)\narray([[ 1.,  1.,  1.],\n   [ 1.,  1.,  1.]], dtype=float32)\n\n\n\n\n\n\nrandom_uniform_variable\n\n\nrandom_uniform_variable(shape, low, high, dtype=None, name=None, seed=None)\n\n\n\n\nInstantiates an Keras variable filled with\nsamples drawn from a uniform distribution and returns it.\n\n\nArguments\n\n\n\n\nshape\n: Tuple of integers, shape of returned Keras variable.\n\n\nlow\n: Float, lower boundary of the output inteval.\n\n\nhigh\n: Float, upper boundary of the output interval.\n\n\ndtype\n: String, dtype of returned Keras variable.\n\n\nname\n: String, name of returned Keras variable.\n\n\nseed\n: Integer, random seed.\n\n\n\n\nReturns\n\n\nA Keras variable, filled with drawn samples.\n\n\nExample\n\n\n__TensorFlow example__\n\n\n kvar = K.random_uniform_variable((2,3), 0, 1)\n\n kvar\n\ntensorflow.python.ops.variables.Variable object at 0x10ab40b10\n\n\n K.eval(kvar)\narray([[ 0.10940075,  0.10047495,  0.476143  ],\n   [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)\n\n\n\n\n\n\nrandom_normal_variable\n\n\nrandom_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None)\n\n\n\n\nInstantiates an Keras variable filled with\nsamples drawn from a normal distribution and returns it.\n\n\nArguments\n\n\n\n\nshape\n: Tuple of integers, shape of returned Keras variable.\n\n\nmean\n: Float, mean of the normal distribution.\n\n\nscale\n: Float, standard deviation of the normal distribution.\n\n\ndtype\n: String, dtype of returned Keras variable.\n\n\nname\n: String, name of returned Keras variable.\n\n\nseed\n: Integer, random seed.\n\n\n\n\nReturns\n\n\nA Keras variable, filled with drawn samples.\n\n\nExample\n\n\n__TensorFlow example__\n\n\n kvar = K.random_normal_variable((2,3), 0, 1)\n\n kvar\n\ntensorflow.python.ops.variables.Variable object at 0x10ab12dd0\n\n\n K.eval(kvar)\narray([[ 1.19591331,  0.68685907, -0.63814116],\n   [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)\n\n\n\n\n\n\ncount_params\n\n\ncount_params(x)\n\n\n\n\nReturns the number of scalars in a Keras variable.\n\n\nArguments\n\n\n\n\nx\n: Keras variable.\n\n\n\n\nReturns\n\n\nInteger, the number of scalars in \nx\n.\n\n\nExample\n\n\n kvar = K.zeros((2,3))\n\n K.count_params(kvar)\n6\n\n K.eval(kvar)\narray([[ 0.,  0.,  0.],\n   [ 0.,  0.,  0.]], dtype=float32)\n\n\n\n\n\n\ncast\n\n\ncast(x, dtype)\n\n\n\n\nCasts a tensor to a different dtype and returns it.\n\n\nYou can cast a Keras variable but it still returns a Keras tensor.\n\n\nArguments\n\n\n\n\nx\n: Keras tensor (or variable).\n\n\ndtype\n: String, either (\n'float16'\n, \n'float32'\n, or \n'float64'\n).\n\n\n\n\nReturns\n\n\nKeras tensor with dtype \ndtype\n.\n\n\nExample\n\n\n from keras import backend as K\n\n input = K.placeholder((2, 3), dtype='float32')\n\n input\n\ntf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32\n\n__It doesn't work in-place as below.__\n\n\n K.cast(input, dtype='float16')\n\ntf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16\n\n\n input\n\ntf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32\n\n__you need to assign it.__\n\n\n input = K.cast(input, dtype='float16')\n\n input\n\ntf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16\n\n\n\n\n\n\n\ndot\n\n\ndot(x, y)\n\n\n\n\nMultiplies 2 tensors (and/or variables) and returns a \ntensor\n.\nWhen attempting to multiply a ND tensor\nwith a ND tensor, it reproduces the Theano behavior.\n(e.g. (2, 3).(4, 3, 5) = (2, 4, 5))\n\n\nArguments\n\n\n\n\nx\n: Tensor or variable.\n\n\ny\n: Tensor or variable.\n\n\n\n\nReturns\n\n\nA tensor, dot product of \nx\n and \ny\n.\n\n\nExamples\n\n\n__dot product between tensors__\n\n\n x = K.placeholder(shape=(2, 3))\n\n y = K.placeholder(shape=(3, 4))\n\n xy = K.dot(x, y)\n\n xy\n\ntf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32\n\n\n\n\n\n__dot product between tensors__\n\n\n x = K.placeholder(shape=(32, 28, 3))\n\n y = K.placeholder(shape=(3, 4))\n\n xy = K.dot(x, y)\n\n xy\n\ntf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32\n\n\n\n\n\n__Theano-like behavior example__\n\n\n x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n\n y = K.ones((4, 3, 5))\n\n xy = K.dot(x, y)\n\n K.int_shape(xy)\n(2, 4, 5)\n\n\n\n\n\n\nbatch_dot\n\n\nbatch_dot(x, y, axes=None)\n\n\n\n\nBatchwise dot product.\n\n\nbatch_dot\n is used to compute dot product of \nx\n and \ny\n when\n\nx\n and \ny\n are data in batch, i.e. in a shape of\n\n(batch_size, :)\n.\n\nbatch_dot\n results in a tensor or variable with less dimensions\nthan the input. If the number of dimensions is reduced to 1,\nwe use \nexpand_dims\n to make sure that ndim is at least 2.\n\n\nArguments\n\n\nx, y: Keras tensors or variables with \nndim \n= 2\n\n- \naxes\n: list of (or single) int with target dimensions.\nThe lengths of \naxes[0]\n and \naxes[1]\n should be the same.\n\n\nReturns\n\n\nA tensor with shape equal to the concatenation of \nx\n's shape\n(less the dimension that was summed over) and \ny\n's shape\n(less the batch dimension and the dimension that was summed over).\nIf the final rank is 1, we reshape it to \n(batch_size, 1)\n.\n\n\nExamples\n\n\nAssume \nx = [[1, 2], [3, 4]]\n and \ny = [[5, 6], [7, 8]]\n\n\nbatch_dot(x, y, axes=1) = [[17, 53]]\n which is the main diagonal\nof \nx.dot(y.T)\n, although we never have to calculate the off-diagonal\nelements.\n\n\nShape inference:\nLet \nx\n's shape be \n(100, 20)\n and \ny\n's shape be \n(100, 30, 20)\n.\nIf \naxes\n is (1, 2), to find the output shape of resultant tensor,\nloop through each dimension in \nx\n's shape and \ny\n's shape:\n\n\n\n\nx.shape[0]\n : 100 : append to output shape\n\n\nx.shape[1]\n : 20 : do not append to output shape,\ndimension 1 of \nx\n has been summed over. (\ndot_axes[0]\n = 1)\n\n\ny.shape[0]\n : 100 : do not append to output shape,\nalways ignore first dimension of \ny\n\n\ny.shape[1]\n : 30 : append to output shape\n\n\ny.shape[2]\n : 20 : do not append to output shape,\ndimension 2 of \ny\n has been summed over. (\ndot_axes[1]\n = 2)\n\noutput_shape\n = \n(100, 30)\n\n\n\n\n x_batch = K.ones(shape=(32, 20, 1))\n\n y_batch = K.ones(shape=(32, 30, 20))\n\n xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n\n K.int_shape(xy_batch_dot)\n(32, 1, 30)\n\n\n\n\n\n\ntranspose\n\n\ntranspose(x)\n\n\n\n\nTransposes a tensor and returns it.\n\n\nArguments\n\n\n\n\nx\n: Tensor or variable.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\nExamples\n\n\n var = K.variable([[1, 2, 3], [4, 5, 6]])\n\n K.eval(var)\narray([[ 1.,  2.,  3.],\n   [ 4.,  5.,  6.]], dtype=float32)\n\n var_transposed = K.transpose(var)\n\n K.eval(var_transposed)\narray([[ 1.,  4.],\n   [ 2.,  5.],\n   [ 3.,  6.]], dtype=float32)\n\n\n\n\n input = K.placeholder((2, 3))\n\n input\n\ntf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32\n\n\n input_transposed = K.transpose(input)\n\n input_transposed\n\ntf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32\n\n\n\n\n\n\n\n\ngather\n\n\ngather(reference, indices)\n\n\n\n\nRetrieves the elements of indices \nindices\n\nin the tensor \nreference\n.\n\n\nArguments\n\n\n\n\nreference\n: A tensor.\n\n\nindices\n: An integer tensor of indices.\n\n\n\n\nReturns\n\n\nA tensor of same type as \nreference\n.\n\n\n\n\nmax\n\n\nmax(x, axis=None, keepdims=False)\n\n\n\n\nMaximum value in a tensor.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: An integer, the axis to find maximum values.\n\n\nkeepdims\n: A boolean, whether to keep the dimensions or not.\nIf \nkeepdims\n is \nFalse\n, the rank of the tensor is reduced\nby 1. If \nkeepdims\n is \nTrue\n,\nthe reduced dimension is retained with length 1.\n\n\n\n\nReturns\n\n\nA tensor with maximum values of \nx\n.\n\n\n\n\nmin\n\n\nmin(x, axis=None, keepdims=False)\n\n\n\n\nMinimum value in a tensor.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: An integer, the axis to find minimum values.\n\n\nkeepdims\n: A boolean, whether to keep the dimensions or not.\nIf \nkeepdims\n is \nFalse\n, the rank of the tensor is reduced\nby 1. If \nkeepdims\n is \nTrue\n,\nthe reduced dimension is retained with length 1.\n\n\n\n\nReturns\n\n\nA tensor with miminum values of \nx\n.\n\n\n\n\nsum\n\n\nsum(x, axis=None, keepdims=False)\n\n\n\n\nSum of the values in a tensor, alongside the specified axis.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: An integer, the axis to sum over.\n\n\nkeepdims\n: A boolean, whether to keep the dimensions or not.\nIf \nkeepdims\n is \nFalse\n, the rank of the tensor is reduced\nby 1. If \nkeepdims\n is \nTrue\n,\nthe reduced dimension is retained with length 1.\n\n\n\n\nReturns\n\n\nA tensor with sum of \nx\n.\n\n\n\n\nprod\n\n\nprod(x, axis=None, keepdims=False)\n\n\n\n\nMultiplies the values in a tensor, alongside the specified axis.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: An integer, the axis to compute the product.\n\n\nkeepdims\n: A boolean, whether to keep the dimensions or not.\nIf \nkeepdims\n is \nFalse\n, the rank of the tensor is reduced\nby 1. If \nkeepdims\n is \nTrue\n,\nthe reduced dimension is retained with length 1.\n\n\n\n\nReturns\n\n\nA tensor with the product of elements of \nx\n.\n\n\n\n\nvar\n\n\nvar(x, axis=None, keepdims=False)\n\n\n\n\nVariance of a tensor, alongside the specified axis.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: An integer, the axis to compute the variance.\n\n\nkeepdims\n: A boolean, whether to keep the dimensions or not.\nIf \nkeepdims\n is \nFalse\n, the rank of the tensor is reduced\nby 1. If \nkeepdims\n is \nTrue\n,\nthe reduced dimension is retained with length 1.\n\n\n\n\nReturns\n\n\nA tensor with the variance of elements of \nx\n.\n\n\n\n\nstd\n\n\nstd(x, axis=None, keepdims=False)\n\n\n\n\nStandard deviation of a tensor, alongside the specified axis.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: An integer, the axis to compute the standard deviation.\n\n\nkeepdims\n: A boolean, whether to keep the dimensions or not.\nIf \nkeepdims\n is \nFalse\n, the rank of the tensor is reduced\nby 1. If \nkeepdims\n is \nTrue\n,\nthe reduced dimension is retained with length 1.\n\n\n\n\nReturns\n\n\nA tensor with the standard deviation of elements of \nx\n.\n\n\n\n\nmean\n\n\nmean(x, axis=None, keepdims=False)\n\n\n\n\nMean of a tensor, alongside the specified axis.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\naxis\n: A list of integer. Axes to compute the mean.\n\n\nkeepdims\n: A boolean, whether to keep the dimensions or not.\nIf \nkeepdims\n is \nFalse\n, the rank of the tensor is reduced\nby 1 for each entry in \naxis\n. If \nkeep_dims\n is \nTrue\n,\nthe reduced dimensions are retained with length 1.\n\n\n\n\nReturns\n\n\nA tensor with the mean of elements of \nx\n.\n\n\n\n\nany\n\n\nany(x, axis=None, keepdims=False)\n\n\n\n\nBitwise reduction (logical OR).\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\naxis\n: axis along which to perform the reduction.\n\n\nkeepdims\n: whether the drop or broadcast the reduction axes.\n\n\n\n\nReturns\n\n\nA uint8 tensor (0s and 1s).\n\n\n\n\nall\n\n\nall(x, axis=None, keepdims=False)\n\n\n\n\nBitwise reduction (logical AND).\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\naxis\n: axis along which to perform the reduction.\n\n\nkeepdims\n: whether the drop or broadcast the reduction axes.\n\n\n\n\nReturns\n\n\nA uint8 tensor (0s and 1s).\n\n\n\n\nargmax\n\n\nargmax(x, axis=-1)\n\n\n\n\nReturns the index of the maximum value along an axis.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\naxis\n: axis along which to perform the reduction.\n\n\nkeepdims\n: whether the drop or broadcast the reduction axes.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nargmin\n\n\nargmin(x, axis=-1)\n\n\n\n\nReturns the index of the minimum value along an axis.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\naxis\n: axis along which to perform the reduction.\n\n\nkeepdims\n: whether the drop or broadcast the reduction axes.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nsquare\n\n\nsquare(x)\n\n\n\n\nElement-wise square.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nabs\n\n\nabs(x)\n\n\n\n\nElement-wise absolute value.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nsqrt\n\n\nsqrt(x)\n\n\n\n\nElement-wise square root.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nexp\n\n\nexp(x)\n\n\n\n\nElement-wise exponential.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nlog\n\n\nlog(x)\n\n\n\n\nElement-wise log.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nround\n\n\nround(x)\n\n\n\n\nElement-wise rounding to the closest integer.\n\n\nIn case of tie, the rounding mode used is \"half to even\".\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nsign\n\n\nsign(x)\n\n\n\n\nElement-wise sign.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\npow\n\n\npow(x, a)\n\n\n\n\nElement-wise exponentiation.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nclip\n\n\nclip(x, min_value, max_value)\n\n\n\n\nElement-wise value clipping.\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nequal\n\n\nequal(x, y)\n\n\n\n\nElement-wise equality between two tensors.\n\n\nReturns\n\n\nA bool tensor.\n\n\n\n\nnot_equal\n\n\nnot_equal(x, y)\n\n\n\n\nElement-wise inequality between two tensors.\n\n\nReturns\n\n\nA bool tensor.\n\n\n\n\ngreater\n\n\ngreater(x, y)\n\n\n\n\nElement-wise truth value of (x \n y).\n\n\nReturns\n\n\nA bool tensor.\n\n\n\n\ngreater_equal\n\n\ngreater_equal(x, y)\n\n\n\n\nElement-wise truth value of (x \n= y).\n\n\nReturns\n\n\nA bool tensor.\n\n\n\n\nlesser\n\n\nlesser(x, y)\n\n\n\n\nElement-wise truth value of (x \n y).\n\n\nReturns\n\n\nA bool tensor.\n\n\n\n\nlesser_equal\n\n\nlesser_equal(x, y)\n\n\n\n\nElement-wise truth value of (x \n= y).\n\n\nReturns\n\n\nA bool tensor.\n\n\n\n\nmaximum\n\n\nmaximum(x, y)\n\n\n\n\nElement-wise maximum of two tensors.\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nminimum\n\n\nminimum(x, y)\n\n\n\n\nElement-wise minimum of two tensors.\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nsin\n\n\nsin(x)\n\n\n\n\nComputes sin of x element-wise.\n\n\nReturns\n\n\nA tensor.\n\n\n\n\ncos\n\n\ncos(x)\n\n\n\n\nComputes cos of x element-wise.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nnormalize_batch_in_training\n\n\nnormalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)\n\n\n\n\nComputes mean and std for batch then apply batch_normalization on batch.\n\n\nReturns\n\n\nA tuple length of 3, \n(normalized_tensor, mean, variance)\n.\n\n\n\n\nbatch_normalization\n\n\nbatch_normalization(x, mean, var, beta, gamma, epsilon=0.001)\n\n\n\n\nApplies batch normalization on x given mean, var, beta and gamma:\n\n\noutput = (x - mean) / (sqrt(var) + epsilon) * gamma + beta\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nconcatenate\n\n\nconcatenate(tensors, axis=-1)\n\n\n\n\nConcatenates a list of tensors alongside the specified axis.\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nreshape\n\n\nreshape(x, shape)\n\n\n\n\nReshapes a tensor to the specified shape.\n\n\nReturns\n\n\nA tensor.\n\n\n\n\npermute_dimensions\n\n\npermute_dimensions(x, pattern)\n\n\n\n\nPermutes axes in a tensor.\n\n\nArguments\n\n\n\n\npattern\n: should be a tuple of\ndimension indices, e.g. (0, 2, 1).\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nresize_images\n\n\nresize_images(X, height_factor, width_factor, dim_ordering)\n\n\n\n\nResizes the images contained in a 4D tensor of shape\n- \n[batch, channels, height, width]\n (for 'th' dim_ordering)\n- \n[batch, height, width, channels]\n (for 'tf' dim_ordering)\nby a factor of \n(height_factor, width_factor)\n. Both factors should be\npositive integers.\n\n\nReturns\n\n\nA tensor.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\nresize_volumes\n\n\nresize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering)\n\n\n\n\nResizes the volume contained in a 5D tensor of shape\n- \n[batch, channels, depth, height, width]\n (for 'th' dim_ordering)\n- \n[batch, depth, height, width, channels]\n (for 'tf' dim_ordering)\nby a factor of \n(depth_factor, height_factor, width_factor)\n.\nAll three factors should be positive integers.\n\n\nReturns\n\n\nA tensor.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\nrepeat_elements\n\n\nrepeat_elements(x, rep, axis)\n\n\n\n\nRepeats the elements of a tensor along an axis, like \nnp.repeat\n.\n\n\nIf \nx\n has shape \n(s1, s2, s3)\n and \naxis\n is \n1\n, the output\nwill have shape \n(s1, s2 * rep, s3)\n.\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nrepeat\n\n\nrepeat(x, n)\n\n\n\n\nRepeats a 2D tensor.\n\n\nif \nx\n has shape (samples, dim) and \nn\n is \n2\n,\nthe output will have shape \n(samples, 2, dim)\n.\n\n\nReturns\n\n\nA tensor.\n\n\n\n\narange\n\n\narange(start, stop=None, step=1, dtype='int32')\n\n\n\n\nCreates a 1-D tensor containing a sequence of integers.\n\n\nThe function arguments use the same convention as\nTheano's arange: if only one argument is provided,\nit is in fact the \"stop\" argument.\n\n\nThe default type of the returned tensor is \n'int32'\n to\nmatch TensorFlow's default.\n\n\n\n\ntile\n\n\ntile(x, n)\n\n\n\n\nCreates a tensor by tiling \nx\n by \nn\n.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable\n\n\nn\n: A list of integer. The length must be the same as the number of\ndimensions in \nx\n.\n\n\n\n\nReturns\n\n\nA tiled tensor.\n\n\n\n\nflatten\n\n\nflatten(x)\n\n\n\n\nFlatten a tensor.\n\n\nReturns\n\n\nA tensor, reshaped into 1-D\n\n\n\n\nbatch_flatten\n\n\nbatch_flatten(x)\n\n\n\n\nTurn a n-D tensor into a 2D tensor where\nthe first dimension is conserved.\n\n\nIn other words, it flattens each data samples of a batch.\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nexpand_dims\n\n\nexpand_dims(x, dim=-1)\n\n\n\n\nAdds a 1-sized dimension at index \"dim\".\n\n\nReturns\n\n\nA tensor with expended dimensions.\n\n\n\n\nsqueeze\n\n\nsqueeze(x, axis)\n\n\n\n\nRemoves a 1-dimension from the tensor at index \"axis\".\n\n\nReturns\n\n\nA tensor with the same data as \nx\n but reduced dimensions.\n\n\n\n\ntemporal_padding\n\n\ntemporal_padding(x, padding=1)\n\n\n\n\nPads the middle dimension of a 3D tensor\nwith \"padding\" zeros left and right.\n\n\nReturns\n\n\nA padded 3D tensor.\n\n\n\n\nasymmetric_temporal_padding\n\n\nasymmetric_temporal_padding(x, left_pad=1, right_pad=1)\n\n\n\n\nPad the middle dimension of a 3D tensor\nwith \"left_pad\" zeros left and \"right_pad\" right.\n\n\nReturns\n\n\nA padded 3D tensor.\n\n\n\n\nspatial_2d_padding\n\n\nspatial_2d_padding(x, padding=(1, 1), dim_ordering='default')\n\n\n\n\nPads the 2nd and 3rd dimensions of a 4D tensor\nwith \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.\n\n\nReturns\n\n\nA padded 4D tensor.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\nasymmetric_spatial_2d_padding\n\n\nasymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1, left_pad=1, right_pad=1, dim_ordering='default')\n\n\n\n\nPad the rows and columns of a 4D tensor\nwith \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\" (resp.) zeros\nrows on top, bottom; cols on left, right.\n\n\nReturns\n\n\nA padded 4D tensor.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\nspatial_3d_padding\n\n\nspatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default')\n\n\n\n\nPads 5D tensor with zeros for the depth, height, width dimension with\n\"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right\n\n\nFor 'tf' dim_ordering, the 2nd, 3rd and 4th dimension will be padded.\nFor 'th' dim_ordering, the 3rd, 4th and 5th dimension will be padded.\n\n\nReturns\n\n\nA padded 5D tensor.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\nstack\n\n\nstack(x)\n\n\n\n\nStacks a list of rank \nR\n tensors into a rank \nR+1\n tensor.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\none_hot\n\n\none_hot(indices, nb_classes)\n\n\n\n\nInput: nD integer tensor of shape \n(batch_size, dim1, dim2, ... dim(n-1))\n\n- \nOutput\n: (n + 1)D one hot representation of the input\nwith shape \n(batch_size, dim1, dim2, ... dim(n-1), nb_classes)\n\n\nReturns\n\n\nThe one-hot tensor.\n\n\n\n\nreverse\n\n\nreverse(x, axes)\n\n\n\n\nReverse a tensor along the the specified axes\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nget_value\n\n\nget_value(x)\n\n\n\n\nReturns the value of a variable.\n\n\nArguments\n\n\n\n\nx\n: input variable.\n\n\n\n\nReturns\n\n\nA Numpy array.\n\n\n\n\nbatch_get_value\n\n\nbatch_get_value(xs)\n\n\n\n\nReturns the value of more than one tensor variable.\n\n\nArguments\n\n\n\n\nx\n: list of variables.\n\n\n\n\nReturns\n\n\nA list of Numpy arrays.\n\n\n\n\nset_value\n\n\nset_value(x, value)\n\n\n\n\nSets the value of a variable,\nfrom a Numpy array. It returns \nNone\n.\n\n\n\n\nbatch_set_value\n\n\nbatch_set_value(tuples)\n\n\n\n\nSets the values of many tensor variables at once.\nIt returns \nNone\n.\n\n\nArguments\n\n\n\n\ntuples\n: a list of tuples \n(tensor, value)\n.\n\nvalue\n should be a Numpy array.\n\n\n\n\n\n\nget_variable_shape\n\n\nget_variable_shape(x)\n\n\n\n\nReturns shape of a variable.\n\n\nArguments\n\n\nA variable.\n\n\nReturns\n\n\nA tuple of integers.\n\n\n\n\nprint_tensor\n\n\nprint_tensor(x, message='')\n\n\n\n\nPrint the message and the tensor when evaluated and return the same\ntensor.\n\n\n\n\nfunction\n\n\nfunction(inputs, outputs, updates=[])\n\n\n\n\nInstantiates a Keras function.\n\n\nArguments\n\n\n\n\ninputs\n: list of placeholder/variable tensors.\n\n\noutputs\n: list of output tensors.\n\n\nupdates\n: list of update tuples (old_tensor, new_tensor).\n\n\n\n\n\n\ngradients\n\n\ngradients(loss, variables)\n\n\n\n\nReturns the gradients of \nvariables\n (list of tensor variables)\nwith regard to \nloss\n.\n\n\n\n\nstop_gradient\n\n\nstop_gradient(variables)\n\n\n\n\nReturns \nvariables\n but with zero gradient with respect to every other\nvariables.\n\n\n\n\nrnn\n\n\nrnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)\n\n\n\n\nIterates over the time dimension of a tensor.\n\n\nArguments\n\n\n\n\ninputs\n: tensor of temporal data of shape \n(samples, time, ...)\n\n(at least 3D).\n\n\nstep_function\n:\n\n\nParameters\n:\n\n\ninput\n: tensor with shape \n(samples, ...)\n (no time dimension),\nrepresenting input for the batch of samples at a certain\ntime step.\n\n\nstates\n: list of tensors.\n\n\n\n\n\n\nReturns\n:\n\n\noutput\n: tensor with shape \n(samples, output_dim)\n\n(no time dimension).\n\n\nnew_states\n: list of tensors, same length and shapes\nas 'states'. The first state in the list must be the\noutput tensor at the previous timestep.\n\n\n\n\n\n\ninitial_states\n: tensor with shape (samples, output_dim)\n(no time dimension),\ncontaining the initial values for the states used in\nthe step function.\n\n\ngo_backwards\n: boolean. If True, do the iteration over\nthe time dimension in reverse order.\n\n\nmask\n: binary tensor with shape \n(samples, time, 1)\n,\nwith a zero for every element that is masked.\n\n\nconstants\n: a list of constant values passed at each step.\n\n\nunroll\n: whether to unroll the RNN or to use a symbolic loop (\nwhile_loop\n or \nscan\n depending on backend).\n\n\ninput_length\n: not relevant in the TensorFlow implementation.\nMust be specified if using unrolling with Theano.\n\n\n\n\nReturns\n\n\nA tuple, \n(last_output, outputs, new_states)\n.\n\n\n\n\nlast_output\n: the latest output of the rnn, of shape \n(samples, ...)\n\n\noutputs\n: tensor with shape \n(samples, time, ...)\n where each\n    entry \noutputs[s, t]\n is the output of the step function\n    at time \nt\n for sample \ns\n.\n\n\nnew_states\n: list of tensors, latest states returned by\n    the step function, of shape \n(samples, ...)\n.\n\n\n\n\nRaises\n\n\n\n\nValueError\n: if input dimension is less than 3.\n\n\nValueError\n: if \nunroll\n is \nTrue\n but input timestep is not a fixed number.\n\n\nValueError\n: if \nmask\n is provided (not \nNone\n) but states is not provided\n(\nlen(states)\n == 0).\n\n\n\n\n\n\nswitch\n\n\nswitch(condition, then_expression, else_expression)\n\n\n\n\nSwitches between two operations\ndepending on a scalar value (\nint\n or \nbool\n).\nNote that both \nthen_expression\n and \nelse_expression\n\nshould be symbolic tensors of the \nsame shape\n.\n\n\nArguments\n\n\n\n\ncondition\n: scalar tensor.\n\n\nthen_expression\n: either a tensor, or a callable that returns a tensor.\n\n\nelse_expression\n: either a tensor, or a callable that returns a tensor.\n\n\n\n\nReturns\n\n\nThe selected tensor.\n\n\n\n\nin_train_phase\n\n\nin_train_phase(x, alt)\n\n\n\n\nSelects \nx\n in train phase, and \nalt\n otherwise.\nNote that \nalt\n should have the \nsame shape\n as \nx\n.\n\n\nReturns\n\n\nEither \nx\n or \nalt\n based on \nK.learning_phase\n.\n\n\n\n\nin_test_phase\n\n\nin_test_phase(x, alt)\n\n\n\n\nSelects \nx\n in test phase, and \nalt\n otherwise.\nNote that \nalt\n should have the \nsame shape\n as \nx\n.\n\n\nReturns\n\n\nEither \nx\n or \nalt\n based on \nK.learning_phase\n.\n\n\n\n\nrelu\n\n\nrelu(x, alpha=0.0, max_value=None)\n\n\n\n\nRectified linear unit.\nWith default values, it returns element-wise \nmax(x, 0)\n.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\nalpha\n: A scalar, slope of negative section (default=\n0.\n).\n\n\nmax_value\n: Saturation threshold.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nelu\n\n\nelu(x, alpha=1.0)\n\n\n\n\nExponential linear unit.\n\n\nArguments\n\n\n\n\nx\n: A tenor or variable to compute the activation function for.\n\n\nalpha\n: A scalar, slope of positive section.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nsoftmax\n\n\nsoftmax(x)\n\n\n\n\nSoftmax of a tensor.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nsoftplus\n\n\nsoftplus(x)\n\n\n\n\nSoftplus of a tensor.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nsoftsign\n\n\nsoftsign(x)\n\n\n\n\nSoftsign of a tensor.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\ncategorical_crossentropy\n\n\ncategorical_crossentropy(output, target, from_logits=False)\n\n\n\n\nCategorical crossentropy between an output tensor\nand a target tensor, where the target is a tensor of the same\nshape as the output.\n\n\n\n\nsparse_categorical_crossentropy\n\n\nsparse_categorical_crossentropy(output, target, from_logits=False)\n\n\n\n\nCategorical crossentropy between an output tensor\nand a target tensor, where the target is an integer tensor.\n\n\n\n\nbinary_crossentropy\n\n\nbinary_crossentropy(output, target, from_logits=False)\n\n\n\n\nBinary crossentropy between an output tensor and a target tensor.\n\n\nArguments\n\n\n\n\noutput\n: A tensor.\n\n\ntarget\n: A tensor with the same shape as \noutput\n.\n\n\nfrom_logits\n: Whether \noutput\n is expected to be a logits tensor.\nBy default, we consider that \noutput\n\nencodes a probability distribution.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nsigmoid\n\n\nsigmoid(x)\n\n\n\n\nElement-wise sigmoid.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nhard_sigmoid\n\n\nhard_sigmoid(x)\n\n\n\n\nSegment-wise linear approximation of sigmoid.\nFaster than sigmoid.\nReturns \n0.\n if \nx \n -2.5\n, \n1.\n if \nx \n 2.5\n.\nIn \n-2.5 \n= x \n= 2.5\n, returns \n0.2 * x + 0.5\n.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\ntanh\n\n\ntanh(x)\n\n\n\n\nElement-wise tanh.\n\n\nArguments\n\n\n\n\nx\n: A tensor or variable.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\ndropout\n\n\ndropout(x, level, noise_shape=None, seed=None)\n\n\n\n\nSets entries in \nx\n to zero at random, while scaling the entire tensor.\n\n\nArguments\n\n\n\n\nx\n: tensor\n\n\nlevel\n: fraction of the entries in the tensor\nthat will be set to 0.\n\n\nnoise_shape\n: shape for randomly generated keep/drop flags,\nmust be broadcastable to the shape of \nx\n\n\nseed\n: random seed to ensure determinism.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nl2_normalize\n\n\nl2_normalize(x, axis)\n\n\n\n\nNormalizes a tensor wrt the L2 norm alongside the specified axis.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\naxis\n: axis along which to perform normalization.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nin_top_k\n\n\nin_top_k(predictions, targets, k)\n\n\n\n\nReturns whether the \ntargets\n are in the top \nk\n \npredictions\n\n\nArguments\n\n\n\n\npredictions\n: A tensor of shape \nbatch_size\n x classes and type \nfloat32\n.\n\n\ntargets\n: A tensor of shape batch_size and type \nint32\n or \nint64\n.\n\n\nk\n: An \nint\n, number of top elements to consider.\n\n\n\n\nReturns\n\n\nA tensor of shape \nbatch_size\n and type \nbool\n. \noutput_i\n is \nTrue\n if\n\ntargets_i\n is within top-k values of \npredictions_i\n\n\n\n\nconv1d\n\n\nconv1d(x, kernel, stride=1, border_mode='valid', image_shape=None, filter_shape=None)\n\n\n\n\n1D convolution.\n\n\nArguments\n\n\n\n\nkernel\n: kernel tensor.\n\n\nstrides\n: stride integer.\n\n\nborder_mode\n: string, \n\"same\"\n or \n\"valid\"\n.\n\n\n\n\nReturns\n\n\nA tensor, result of 1D convolution.\n\n\n\n\nconv2d\n\n\nconv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='default', image_shape=None, filter_shape=None, filter_dilation=(1, 1))\n\n\n\n\n2D convolution.\n\n\nArguments\n\n\n\n\nkernel\n: kernel tensor.\n\n\nstrides\n: strides tuple.\n\n\nborder_mode\n: string, \n\"same\"\n or \n\"valid\"\n.\n\n\ndim_ordering\n: \n\"tf\"\n or \n\"th\"\n.\nWhether to use Theano or TensorFlow dimension ordering\nfor inputs/kernels/ouputs.\n\n\n\n\nReturns\n\n\nA tensor, result of 2D convolution.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\ndeconv2d\n\n\ndeconv2d(x, kernel, output_shape, strides=(1, 1), border_mode='valid', dim_ordering='default', image_shape=None, filter_shape=None)\n\n\n\n\n2D deconvolution (i.e. transposed convolution).\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\nkernel\n: kernel tensor.\n\n\noutput_shape\n: 1D int tensor for the output shape.\n\n\nstrides\n: strides tuple.\n\n\nborder_mode\n: string, \n\"same\"\n or \n\"valid\"\n.\n\n\ndim_ordering\n: \n\"tf\"\n or \n\"th\"\n.\nWhether to use Theano or TensorFlow dimension ordering\nfor inputs/kernels/ouputs.\n\n\n\n\nReturns\n\n\nA tensor, result of transposed 2D convolution.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\natrous_conv2d\n\n\natrous_conv2d(x, kernel, rate=1, border_mode='valid', dim_ordering='default', image_shape=None, filter_shape=None)\n\n\n\n\nAtrous 2D convolution. Also as known as dilated convolution.\n\n\nArguments\n\n\n\n\nx\n: input tensor.\n\n\nkernel\n: kernel tensor.\n\n\nrate\n: integer \n 0, the sample stride.\n\n\noutput_shape\n: 1D int tensor for the output shape.\n\n\nstrides\n: strides tuple.\n\n\nborder_mode\n: string, \n\"same\"\n or \n\"valid\"\n.\n\n\ndim_ordering\n: \n\"tf\"\n or \n\"th\"\n.\nWhether to use Theano or TensorFlow dimension ordering\nfor inputs/kernels/ouputs.\n\n\n\n\nReturns\n\n\nA tensor, result of atrous transposed 2D convolution.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\nseparable_conv2d\n\n\nseparable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), border_mode='valid', dim_ordering='default')\n\n\n\n\n2-D convolution with separable filters.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\nconv3d\n\n\nconv3d(x, kernel, strides=(1, 1, 1), border_mode='valid', dim_ordering='default', volume_shape=None, filter_shape=None)\n\n\n\n\n3D convolution.\n\n\nArguments\n\n\n\n\nkernel\n: kernel tensor.\n\n\nstrides\n: strides tuple.\n\n\nborder_mode\n: string, \n\"same\"\n or \n\"valid\"\n.\n\n\ndim_ordering\n: \n\"tf\"\n or \n\"th\"\n.\nWhether to use Theano or TensorFlow dimension ordering\nfor inputs/kernels/ouputs.\n\n\n\n\nReturns\n\n\nA tensor, result of 3D convolution.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\n\n\n\n\npool2d\n\n\npool2d(x, pool_size, strides=(1, 1), border_mode='valid', dim_ordering='default', pool_mode='max')\n\n\n\n\n2D Pooling.\n\n\nArguments\n\n\n\n\npool_size\n: tuple of 2 integers.\n\n\nstrides\n: tuple of 2 integers.\n\n\nborder_mode\n: one of \n\"valid\"\n, \n\"same\"\n.\n\n\ndim_ordering\n: one of \n\"th\"\n, \n\"tf\"\n.\n\n\npool_mode\n: one of \n\"max\"\n, \n\"avg\"\n.\n\n\n\n\nReturns\n\n\nA tensor, result of 2D pooling.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\nValueError\n: if \npool_mode\n is neither \nmax\n or \navg\n.\n\n\n\n\n\n\npool3d\n\n\npool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid', dim_ordering='default', pool_mode='max')\n\n\n\n\n3D Pooling.\n\n\nArguments\n\n\n\n\npool_size\n: tuple of 3 integers.\n\n\nstrides\n: tuple of 3 integers.\n\n\nborder_mode\n: one of \n\"valid\"\n, \n\"same\"\n.\n\n\ndim_ordering\n: one of \n\"th\"\n, \n\"tf\"\n.\n\n\npool_mode\n: one of \n\"max\"\n, \n\"avg\"\n.\n\n\n\n\nReturns\n\n\nA tensor, result of 3D pooling.\n\n\nRaises\n\n\n\n\nValueError\n: if \ndim_ordering\n is neither \ntf\n or \nth\n.\n\n\nValueError\n: if \npool_mode\n is neither \nmax\n or \navg\n.\n\n\n\n\n\n\nrandom_normal\n\n\nrandom_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None)\n\n\n\n\nReturns a tensor with normal distribution\n\n\nArguments\n\n\n\n\nshape\n: A tuple of integers, the shape of tensor to create.\n\n\nmean\n: A float, mean of the normal distribution to draw samples.\n\n\nstd\n: A float, standard deviation of the normal distribution\nto draw samples.\n\n\ndtype\n: String, dtype of returned tensor.\n\n\nseed\n: Integer, random seed.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nrandom_uniform\n\n\nrandom_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None)\n\n\n\n\nReturns a tensor with uniform distribution\n\n\nArguments\n\n\n\n\nshape\n: A tuple of integers, the shape of tensor to create.\n\n\nlow\n: A float, lower boundary of the uniform distribution\nto draw samples.\n\n\nhigh\n: A float, upper boundary of the uniform distribution\nto draw samples.\n\n\ndtype\n: String, dtype of returned tensor.\n\n\nseed\n: Integer, random seed.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nrandom_binomial\n\n\nrandom_binomial(shape, p=0.0, dtype=None, seed=None)\n\n\n\n\nReturns a tensor with binomlai distribution\n\n\nArguments\n\n\n\n\nshape\n: A tuple of integers, the shape of tensor to create.\n\n\np\n: A float, \n0. \n= p \n= 1\n, probability of binomlai distribution.\n\n\ndtype\n: String, dtype of returned tensor.\n\n\nseed\n: Integer, random seed.\n\n\n\n\nReturns\n\n\nA tensor.\n\n\n\n\nctc_batch_cost\n\n\nctc_batch_cost(y_true, y_pred, input_length, label_length)\n\n\n\n\nRuns CTC loss algorithm on each batch element.\n\n\nArguments\n\n\n\n\ny_true\n: tensor \n(samples, max_string_length)\n containing the truth labels.\n\n\ny_pred\n: tensor \n(samples, time_steps, num_categories)\n containing the prediction,\n    or output of the softmax.\n\n\ninput_length\n: tensor \n(samples, 1)\n containing the sequence length for\n    each batch item in \ny_pred\n.\n\n\nlabel_length\n: tensor \n(samples, 1)\n containing the sequence length for\n    each batch item in \ny_true\n.\n\n\n\n\nReturns\n\n\nTensor with shape (samples,1) containing the\nCTC loss of each element\n\n\n\n\nctc_decode\n\n\nctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1)\n\n\n\n\nDecodes the output of a softmax using either\n   greedy (also known as best path) or a constrained dictionary\n   search.\n\n\nArguments\n\n\n\n\ny_pred\n: tensor \n(samples, time_steps, num_categories)\n containing the prediction,\n    or output of the softmax.\n\n\ninput_length\n: tensor \n(samples, )\n containing the sequence length for\n    each batch item in \ny_pred\n.\n\n\ngreedy\n: perform much faster best-path search if \ntrue\n. This does\n    not use a dictionary\n\n\nbeam_width\n: if \ngreedy\n is \nfalse\n: a beam search decoder will be used\n    with a beam of this width\n\n\ntop_paths\n: if \ngreedy\n is \nfalse\n: how many of the most probable paths will be returned\n\n\n\n\nReturns\n\n\n\n\nTuple\n:\n\n\nList\n: if \ngreedy\n is \ntrue\n, returns a list of one element that contains\n    the decoded sequence. If \nfalse\n, returns the \ntop_paths\n most probable\n    decoded sequences. Important: blank labels are returned as \n-1\n.\nTensor \n(top_paths, )\n that contains the log probability of each decoded sequence\n\n\n\n\n\n\nmap_fn\n\n\nmap_fn(fn, elems, name=None)\n\n\n\n\nMap the function fn over the elements elems and return the outputs.\n\n\nArguments\n\n\n\n\nfn\n: Callable that will be called upon each element in elems\n\n\nelems\n: tensor\n\n\nname\n: A string name for the map node in the graph\n\n\n\n\nReturns\n\n\nTensor with first dimension equal to the elems and second depending on\nfn\n\n\n\n\nfoldl\n\n\nfoldl(fn, elems, initializer=None, name=None)\n\n\n\n\nReduce elems using fn to combine them from left to right.\n\n\nArguments\n\n\n\n\nfn\n: Callable that will be called upon each element in elems and an\naccumulator, for instance \nlambda acc, x: acc + x\n\n\nelems\n: tensor\n\n\ninitializer\n: The first value used (\nelems[0]\n in case of None)\n\n\nname\n: A string name for the foldl node in the graph\n\n\n\n\nReturns\n\n\nSame type and shape as initializer\n\n\n\n\nfoldr\n\n\nfoldr(fn, elems, initializer=None, name=None)\n\n\n\n\nReduce elems using fn to combine them from right to left.\n\n\nArguments\n\n\n\n\nfn\n: Callable that will be called upon each element in elems and an\naccumulator, for instance \nlambda acc, x: acc + x\n\n\nelems\n: tensor\n\n\ninitializer\n: The first value used (\nelems[-1]\n in case of None)\n\n\nname\n: A string name for the foldr node in the graph\n\n\n\n\nReturns\n\n\nSame type and shape as initializer\n\n\n\n\nbackend\n\n\nbackend()\n\n\n\n\nPublicly accessible method\nfor determining the current backend.", 
            "title": "Backend"
        }, 
        {
            "location": "/backend/#keras-backends", 
            "text": "", 
            "title": "Keras backends"
        }, 
        {
            "location": "/backend/#what-is-a-backend", 
            "text": "Keras is a model-level library, providing high-level building blocks for developing deep learning models. It does not handle itself low-level operations such as tensor products, convolutions and so on. Instead, it relies on a specialized, well-optimized tensor manipulation library to do so, serving as the \"backend engine\" of Keras. Rather than picking one single tensor library and making the implementation of Keras tied to that library, Keras handles the problem in a modular way, and several different backend engines can be plugged seamlessly into Keras.  At this time, Keras has two backend implementations available: the  TensorFlow  backend and the  Theano  backend.   TensorFlow  is an open-source symbolic tensor manipulation framework developed by Google, Inc.  Theano  is an open-source symbolic tensor manipulation framework developed by LISA/MILA Lab at Universit\u00e9 de Montr\u00e9al.   In the future, we are likely to add more backend options. If you are interested in developing a new backend, get in touch!", 
            "title": "What is a \"backend\"?"
        }, 
        {
            "location": "/backend/#switching-from-one-backend-to-another", 
            "text": "If you have run Keras at least once, you will find the Keras configuration file at:  ~/.keras/keras.json  If it isn't there, you can create it.  The default configuration file looks like this:  {\n     image_dim_ordering :  tf ,\n     epsilon : 1e-07,\n     floatx :  float32 ,\n     backend :  tensorflow \n}  Simply change the field  backend  to either  \"theano\"  or  \"tensorflow\" , and Keras will use the new configuration next time you run any Keras code.  You can also define the environment variable  KERAS_BACKEND  and this will\noverride what is defined in your config file :  KERAS_BACKEND=tensorflow python -c  from keras import backend \nUsing TensorFlow backend.", 
            "title": "Switching from one backend to another"
        }, 
        {
            "location": "/backend/#using-the-abstract-keras-backend-to-write-new-code", 
            "text": "If you want the Keras modules you write to be compatible with both Theano and TensorFlow, you have to write them via the abstract Keras backend API. Here's an intro.  You can import the backend module via:  from keras import backend as K  The code below instantiates an input placeholder. It's equivalent to  tf.placeholder()  or  T.matrix() ,  T.tensor3() , etc.  input = K.placeholder(shape=(2, 4, 5))\n# also works:\ninput = K.placeholder(shape=(None, 4, 5))\n# also works:\ninput = K.placeholder(ndim=3)  The code below instantiates a shared variable. It's equivalent to  tf.variable()  or  theano.shared() .  val = np.random.random((3, 4, 5))\nvar = K.variable(value=val)\n\n# all-zeros variable:\nvar = K.zeros(shape=(3, 4, 5))\n# all-ones:\nvar = K.ones(shape=(3, 4, 5))  Most tensor operations you will need can be done as you would in TensorFlow or Theano:  a = b + c * K.abs(d)\nc = K.dot(a, K.transpose(b))\na = K.sum(b, axis=2)\na = K.softmax(b)\na = concatenate([b, c], axis=-1)\n# etc...", 
            "title": "Using the abstract Keras backend to write new code"
        }, 
        {
            "location": "/backend/#backend-functions", 
            "text": "", 
            "title": "Backend functions"
        }, 
        {
            "location": "/backend/#epsilon", 
            "text": "epsilon()  Returns the value of the fuzz\nfactor used in numeric expressions.  Returns  A float.  Example   keras.backend.epsilon()\n1e-08", 
            "title": "epsilon"
        }, 
        {
            "location": "/backend/#set_epsilon", 
            "text": "set_epsilon(e)  Sets the value of the fuzz\nfactor used in numeric expressions.  Arguments   e : float. New value of epsilon.   Example   from keras import backend as K  K.epsilon()\n1e-08  K.set_epsilon(1e-05)  K.epsilon()\n1e-05", 
            "title": "set_epsilon"
        }, 
        {
            "location": "/backend/#floatx", 
            "text": "floatx()  Returns the default float type, as a string\n(e.g. 'float16', 'float32', 'float64').  Returns  String, the current default float type.  Example   keras.backend.floatx()\n'float32'", 
            "title": "floatx"
        }, 
        {
            "location": "/backend/#set_floatx", 
            "text": "set_floatx(floatx)  Sets the default float type.  Arguments   String : 'float16', 'float32', or 'float64'.   Example   from keras import backend as K  K.floatx()\n'float32'  K.set_floatx('float16')  K.floatx()\n'float16'", 
            "title": "set_floatx"
        }, 
        {
            "location": "/backend/#cast_to_floatx", 
            "text": "cast_to_floatx(x)  Cast a Numpy array to the default Keras float type.  Arguments   x : Numpy array.   Returns  The same Numpy array, cast to its new type.  Example   from keras import backend as K  K.floatx()\n'float32'  arr = numpy.array([1.0, 2.0], dtype='float64')  arr.dtype\ndtype('float64')  new_arr = K.cast_to_floatx(arr)  new_arr\narray([ 1.,  2.], dtype=float32)  new_arr.dtype\ndtype('float32')", 
            "title": "cast_to_floatx"
        }, 
        {
            "location": "/backend/#image_dim_ordering", 
            "text": "image_dim_ordering()  Returns the default image dimension ordering\nconvention ('th' or 'tf').  Returns  A string, either  'th'  or  'tf'  Example   keras.backend.image_dim_ordering()\n'th'", 
            "title": "image_dim_ordering"
        }, 
        {
            "location": "/backend/#set_image_dim_ordering", 
            "text": "set_image_dim_ordering(dim_ordering)  Sets the value of the image dimension\nordering convention ('th' or 'tf').  Arguments   dim_ordering : string.  'th'  or  'tf' .   Example   from keras import backend as K  K.image_dim_ordering()\n'th'  K.set_image_dim_ordering('tf')  K.image_dim_ordering()\n'tf'", 
            "title": "set_image_dim_ordering"
        }, 
        {
            "location": "/backend/#get_uid", 
            "text": "get_uid(prefix='')  Provides a unique UID given a string prefix.  Arguments   prefix : string.   Returns  An integer.  Example   keras.backend.get_uid('dense')  1  keras.backend.get_uid('dense')  2", 
            "title": "get_uid"
        }, 
        {
            "location": "/backend/#is_keras_tensor", 
            "text": "is_keras_tensor(x)  Returns whether  x  is a Keras tensor.  Arguments   x : a potential tensor.   Returns  A boolean: whether the argument is a Keras tensor.  Examples   from keras import backend as K  np_var = numpy.array([1, 2])  K.is_keras_tensor(np_var)\nFalse  keras_var = K.variable(np_var)  K.is_keras_tensor(keras_var)  # A variable is not a Tensor.\nFalse  keras_placeholder = K.placeholder(shape=(2, 4, 5))  K.is_keras_tensor(keras_placeholder)  # A placeholder is a Tensor.\nTrue", 
            "title": "is_keras_tensor"
        }, 
        {
            "location": "/backend/#clear_session", 
            "text": "clear_session()  Destroys the current TF graph and creates a new one.  Useful to avoid clutter from old models / layers.", 
            "title": "clear_session"
        }, 
        {
            "location": "/backend/#manual_variable_initialization", 
            "text": "manual_variable_initialization(value)  Sets the manual variable initialization flag.  This boolean flag determines whether\nvariables should be initialized\nas they are instantiated (default), or if\nthe user should handle the initialization\n(e.g. via  tf.initialize_all_variables() ).  Arguments   value : Python boolean.", 
            "title": "manual_variable_initialization"
        }, 
        {
            "location": "/backend/#learning_phase", 
            "text": "learning_phase()  Returns the learning phase flag.  The learning phase flag is a bool tensor (0 = test, 1 = train)\nto be passed as input to any Keras function\nthat uses a different behavior at train time and test time.", 
            "title": "learning_phase"
        }, 
        {
            "location": "/backend/#set_learning_phase", 
            "text": "set_learning_phase(value)  Sets the learning phase to a fixed value,\neither 0 or 1 (integers).  Raises   ValueError : if  value  is neither  0  nor  1 .", 
            "title": "set_learning_phase"
        }, 
        {
            "location": "/backend/#is_sparse", 
            "text": "is_sparse(tensor)  Returns whether a tensor is a sparse tensor.  Arguments   tensor : A tensor instance.   Returns  A boolean.  Example   from keras import backend as K  a = K.placeholder((2, 2), sparse=False)  print(K.is_sparse(a))\nFalse  b = K.placeholder((2, 2), sparse=True)  print(K.is_sparse(b))\nTrue", 
            "title": "is_sparse"
        }, 
        {
            "location": "/backend/#to_dense", 
            "text": "to_dense(tensor)  Converts a sparse tensor into a dense tensor\nand returns it.  Arguments   tensor : A tensor instance (potentially sparse).   Returns  A dense tensor.  Examples   from keras import backend as K  b = K.placeholder((2, 2), sparse=True)  print(K.is_sparse(b))\nTrue  c = K.to_dense(b)  print(K.is_sparse(c))\nFalse", 
            "title": "to_dense"
        }, 
        {
            "location": "/backend/#variable", 
            "text": "variable(value, dtype=None, name=None)  Instantiates a variable and returns it.  Arguments   value : Numpy array, initial value of the tensor.  dtype : Tensor type.  name : Optional name string for the tensor.   Returns  A variable instance (with Keras metadata included).  Examples   from keras import backend as K  val = np.array([[1, 2], [3, 4]])  kvar = K.variable(value=val, dtype='float64', name='example_var')  K.dtype(kvar)\n'float64'  print(kvar)\nexample_var  kvar.eval()\narray([[ 1.,  2.],\n   [ 3.,  4.]])", 
            "title": "variable"
        }, 
        {
            "location": "/backend/#placeholder", 
            "text": "placeholder(shape=None, ndim=None, dtype=None, sparse=False, name=None)  Instantiates a placeholder tensor and returns it.  Arguments   shape : Shape of the placeholder\n(integer tuple, may include  None  entries).  ndim : Number of axes of the tensor.\nAt least one of { shape ,  ndim } must be specified.\nIf both are specified,  shape  is used.  dtype : Placeholder type.  name : Optional name string for the placeholder.   Returns  Tensor instance (with Keras metadata included).  Examples   from keras import backend as K  input_ph = K.placeholder(shape=(2, 4, 5))  input_ph._keras_shape\n(2, 4, 5)  input_ph tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32", 
            "title": "placeholder"
        }, 
        {
            "location": "/backend/#shape", 
            "text": "shape(x)  Returns the symbolic shape of a tensor or variable.  Arguments   x : A tensor or variable.   Returns  A symbolic shape (which is itself a tensor).  Examples  __TensorFlow example__  from keras import backend as K  tf_session = K.get_session()  val = np.array([[1, 2], [3, 4]])  kvar = K.variable(value=val)  input = keras.backend.placeholder(shape=(2, 4, 5))  K.shape(kvar) tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32   K.shape(input) tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32 \n__To get integer shape (Instead, you can use K.int_shape(x))__  K.shape(kvar).eval(session=tf_session)\narray([2, 2], dtype=int32)  K.shape(input).eval(session=tf_session)\narray([2, 4, 5], dtype=int32)", 
            "title": "shape"
        }, 
        {
            "location": "/backend/#int_shape", 
            "text": "int_shape(x)  Returns the shape of a Keras tensor or a Keras variable as a tuple of\nintegers or None entries.  Arguments   x : Tensor or variable.   Returns  A tuple of integers (or None entries).  Examples   from keras import backend as K  input = K.placeholder(shape=(2, 4, 5))  K.int_shape(input)\n(2, 4, 5)  val = np.array([[1, 2], [3, 4]])  kvar = K.variable(value=val)  K.int_shape(kvar)\n(2, 2)", 
            "title": "int_shape"
        }, 
        {
            "location": "/backend/#ndim", 
            "text": "ndim(x)  Returns the number of axes in a tensor, as an integer.  Arguments   x : Tensor or variable.   Returns  Integer (scalar), number of axes.  Examples   from keras import backend as K  input = K.placeholder(shape=(2, 4, 5))  val = np.array([[1, 2], [3, 4]])  kvar = K.variable(value=val)  K.ndim(input)\n3  K.ndim(kvar)\n2", 
            "title": "ndim"
        }, 
        {
            "location": "/backend/#dtype", 
            "text": "dtype(x)  Returns the dtype of a Keras tensor or variable, as a string.  Arguments   x : Tensor or variable.   Returns  String, dtype of  x .  Examples   from keras import backend as K  K.dtype(K.placeholder(shape=(2,4,5)))\n'float32'  K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))\n'float32'  K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))\n'float64'\n__Keras variable__  kvar = K.variable(np.array([[1, 2], [3, 4]]))  K.dtype(kvar)\n'float32_ref'  kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')  K.dtype(kvar)\n'float32_ref'", 
            "title": "dtype"
        }, 
        {
            "location": "/backend/#eval", 
            "text": "eval(x)  Evaluates the value of a variable.\nReturns a Numpy array.  Arguments   x : A variable.   Returns  A Numpy array.  Examples   from keras import backend as K  kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')  K.eval(kvar)\narray([[ 1.,  2.],\n   [ 3.,  4.]], dtype=float32)", 
            "title": "eval"
        }, 
        {
            "location": "/backend/#zeros", 
            "text": "zeros(shape, dtype=None, name=None)  Instantiates an all-zeros variable and returns it.  Arguments   shape : Tuple of integers, shape of returned Keras variable  dtype : String, data type of returned Keras variable  name : String, name of returned Keras variable   Returns  A variable (including Keras metadata), filled with  0.0 .  Example   from keras import backend as K  kvar = K.zeros((3,4))  K.eval(kvar)\narray([[ 0.,  0.,  0.,  0.],\n   [ 0.,  0.,  0.,  0.],\n   [ 0.,  0.,  0.,  0.]], dtype=float32)", 
            "title": "zeros"
        }, 
        {
            "location": "/backend/#ones", 
            "text": "ones(shape, dtype=None, name=None)  Instantiates an all-ones tensor variable and returns it.  Arguments   shape : Tuple of integers, shape of returned Keras variable.  dtype : String, data type of returned Keras variable.  name : String, name of returned Keras variable.   Returns  A Keras variable, filled with  1.0 .  Example   from keras import backend as K  kvar = K.ones((3,4))  K.eval(kvar)\narray([[ 1.,  1.,  1.,  1.],\n   [ 1.,  1.,  1.,  1.],\n   [ 1.,  1.,  1.,  1.]], dtype=float32)", 
            "title": "ones"
        }, 
        {
            "location": "/backend/#eye", 
            "text": "eye(size, dtype=None, name=None)  Instantiate an identity matrix and returns it.  Arguments   size : Integer, number of rows/columns.  dtype : String, data type of returned Keras variable.  name : String, name of returned Keras variable.   Returns  A Keras variable, an identity matrix.  Example   from keras import backend as K  kvar = K.eye(3)  K.eval(kvar)\narray([[ 1.,  0.,  0.],\n   [ 0.,  1.,  0.],\n   [ 0.,  0.,  1.]], dtype=float32)", 
            "title": "eye"
        }, 
        {
            "location": "/backend/#zeros_like", 
            "text": "zeros_like(x, dtype=None, name=None)  Instantiates an all-zeros Keras variable\nof the same shape as another Keras variable or tensor and returns it.  Arguments   x : Keras variable or Keras tensor.  dtype : String, dtype of returned Keras variable.\n None uses the dtype of x.   Returns  A Keras variable with the shape of x filled with zeros.  Example   from keras import backend as K  kvar = K.variable(np.random.random((2,3)))  kvar_zeros = K.zeros_like(kvar)  K.eval(kvar_zeros)\narray([[ 0.,  0.,  0.],\n   [ 0.,  0.,  0.]], dtype=float32)", 
            "title": "zeros_like"
        }, 
        {
            "location": "/backend/#ones_like", 
            "text": "ones_like(x, dtype=None, name=None)  Instantiates an all-ones Keras variable\nof the same shape as another Keras variable or tensor and returns it.  Arguments   x : Keras variable or tensor.  dtype : String, dtype of returned Keras variable.\n None uses the dtype of x.   Returns  A Keras variable with the shape of x filled with ones.  Example   from keras import backend as K  kvar = K.variable(np.random.random((2,3)))  kvar_ones = K.ones_like(kvar)  K.eval(kvar_ones)\narray([[ 1.,  1.,  1.],\n   [ 1.,  1.,  1.]], dtype=float32)", 
            "title": "ones_like"
        }, 
        {
            "location": "/backend/#random_uniform_variable", 
            "text": "random_uniform_variable(shape, low, high, dtype=None, name=None, seed=None)  Instantiates an Keras variable filled with\nsamples drawn from a uniform distribution and returns it.  Arguments   shape : Tuple of integers, shape of returned Keras variable.  low : Float, lower boundary of the output inteval.  high : Float, upper boundary of the output interval.  dtype : String, dtype of returned Keras variable.  name : String, name of returned Keras variable.  seed : Integer, random seed.   Returns  A Keras variable, filled with drawn samples.  Example  __TensorFlow example__  kvar = K.random_uniform_variable((2,3), 0, 1)  kvar tensorflow.python.ops.variables.Variable object at 0x10ab40b10   K.eval(kvar)\narray([[ 0.10940075,  0.10047495,  0.476143  ],\n   [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)", 
            "title": "random_uniform_variable"
        }, 
        {
            "location": "/backend/#random_normal_variable", 
            "text": "random_normal_variable(shape, mean, scale, dtype=None, name=None, seed=None)  Instantiates an Keras variable filled with\nsamples drawn from a normal distribution and returns it.  Arguments   shape : Tuple of integers, shape of returned Keras variable.  mean : Float, mean of the normal distribution.  scale : Float, standard deviation of the normal distribution.  dtype : String, dtype of returned Keras variable.  name : String, name of returned Keras variable.  seed : Integer, random seed.   Returns  A Keras variable, filled with drawn samples.  Example  __TensorFlow example__  kvar = K.random_normal_variable((2,3), 0, 1)  kvar tensorflow.python.ops.variables.Variable object at 0x10ab12dd0   K.eval(kvar)\narray([[ 1.19591331,  0.68685907, -0.63814116],\n   [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)", 
            "title": "random_normal_variable"
        }, 
        {
            "location": "/backend/#count_params", 
            "text": "count_params(x)  Returns the number of scalars in a Keras variable.  Arguments   x : Keras variable.   Returns  Integer, the number of scalars in  x .  Example   kvar = K.zeros((2,3))  K.count_params(kvar)\n6  K.eval(kvar)\narray([[ 0.,  0.,  0.],\n   [ 0.,  0.,  0.]], dtype=float32)", 
            "title": "count_params"
        }, 
        {
            "location": "/backend/#cast", 
            "text": "cast(x, dtype)  Casts a tensor to a different dtype and returns it.  You can cast a Keras variable but it still returns a Keras tensor.  Arguments   x : Keras tensor (or variable).  dtype : String, either ( 'float16' ,  'float32' , or  'float64' ).   Returns  Keras tensor with dtype  dtype .  Example   from keras import backend as K  input = K.placeholder((2, 3), dtype='float32')  input tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32 \n__It doesn't work in-place as below.__  K.cast(input, dtype='float16') tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16   input tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32 \n__you need to assign it.__  input = K.cast(input, dtype='float16')  input tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16", 
            "title": "cast"
        }, 
        {
            "location": "/backend/#dot", 
            "text": "dot(x, y)  Multiplies 2 tensors (and/or variables) and returns a  tensor .\nWhen attempting to multiply a ND tensor\nwith a ND tensor, it reproduces the Theano behavior.\n(e.g. (2, 3).(4, 3, 5) = (2, 4, 5))  Arguments   x : Tensor or variable.  y : Tensor or variable.   Returns  A tensor, dot product of  x  and  y .  Examples  __dot product between tensors__  x = K.placeholder(shape=(2, 3))  y = K.placeholder(shape=(3, 4))  xy = K.dot(x, y)  xy tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32   __dot product between tensors__  x = K.placeholder(shape=(32, 28, 3))  y = K.placeholder(shape=(3, 4))  xy = K.dot(x, y)  xy tf.Tensor 'MatMul_9:0' shape=(32, 28, 4) dtype=float32   __Theano-like behavior example__  x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)  y = K.ones((4, 3, 5))  xy = K.dot(x, y)  K.int_shape(xy)\n(2, 4, 5)", 
            "title": "dot"
        }, 
        {
            "location": "/backend/#batch_dot", 
            "text": "batch_dot(x, y, axes=None)  Batchwise dot product.  batch_dot  is used to compute dot product of  x  and  y  when x  and  y  are data in batch, i.e. in a shape of (batch_size, :) . batch_dot  results in a tensor or variable with less dimensions\nthan the input. If the number of dimensions is reduced to 1,\nwe use  expand_dims  to make sure that ndim is at least 2.  Arguments  x, y: Keras tensors or variables with  ndim  = 2 \n-  axes : list of (or single) int with target dimensions.\nThe lengths of  axes[0]  and  axes[1]  should be the same.  Returns  A tensor with shape equal to the concatenation of  x 's shape\n(less the dimension that was summed over) and  y 's shape\n(less the batch dimension and the dimension that was summed over).\nIf the final rank is 1, we reshape it to  (batch_size, 1) .  Examples  Assume  x = [[1, 2], [3, 4]]  and  y = [[5, 6], [7, 8]]  batch_dot(x, y, axes=1) = [[17, 53]]  which is the main diagonal\nof  x.dot(y.T) , although we never have to calculate the off-diagonal\nelements.  Shape inference:\nLet  x 's shape be  (100, 20)  and  y 's shape be  (100, 30, 20) .\nIf  axes  is (1, 2), to find the output shape of resultant tensor,\nloop through each dimension in  x 's shape and  y 's shape:   x.shape[0]  : 100 : append to output shape  x.shape[1]  : 20 : do not append to output shape,\ndimension 1 of  x  has been summed over. ( dot_axes[0]  = 1)  y.shape[0]  : 100 : do not append to output shape,\nalways ignore first dimension of  y  y.shape[1]  : 30 : append to output shape  y.shape[2]  : 20 : do not append to output shape,\ndimension 2 of  y  has been summed over. ( dot_axes[1]  = 2) output_shape  =  (100, 30)    x_batch = K.ones(shape=(32, 20, 1))  y_batch = K.ones(shape=(32, 30, 20))  xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])  K.int_shape(xy_batch_dot)\n(32, 1, 30)", 
            "title": "batch_dot"
        }, 
        {
            "location": "/backend/#transpose", 
            "text": "transpose(x)  Transposes a tensor and returns it.  Arguments   x : Tensor or variable.   Returns  A tensor.  Examples   var = K.variable([[1, 2, 3], [4, 5, 6]])  K.eval(var)\narray([[ 1.,  2.,  3.],\n   [ 4.,  5.,  6.]], dtype=float32)  var_transposed = K.transpose(var)  K.eval(var_transposed)\narray([[ 1.,  4.],\n   [ 2.,  5.],\n   [ 3.,  6.]], dtype=float32)   input = K.placeholder((2, 3))  input tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32   input_transposed = K.transpose(input)  input_transposed tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32", 
            "title": "transpose"
        }, 
        {
            "location": "/backend/#gather", 
            "text": "gather(reference, indices)  Retrieves the elements of indices  indices \nin the tensor  reference .  Arguments   reference : A tensor.  indices : An integer tensor of indices.   Returns  A tensor of same type as  reference .", 
            "title": "gather"
        }, 
        {
            "location": "/backend/#max", 
            "text": "max(x, axis=None, keepdims=False)  Maximum value in a tensor.  Arguments   x : A tensor or variable.  axis : An integer, the axis to find maximum values.  keepdims : A boolean, whether to keep the dimensions or not.\nIf  keepdims  is  False , the rank of the tensor is reduced\nby 1. If  keepdims  is  True ,\nthe reduced dimension is retained with length 1.   Returns  A tensor with maximum values of  x .", 
            "title": "max"
        }, 
        {
            "location": "/backend/#min", 
            "text": "min(x, axis=None, keepdims=False)  Minimum value in a tensor.  Arguments   x : A tensor or variable.  axis : An integer, the axis to find minimum values.  keepdims : A boolean, whether to keep the dimensions or not.\nIf  keepdims  is  False , the rank of the tensor is reduced\nby 1. If  keepdims  is  True ,\nthe reduced dimension is retained with length 1.   Returns  A tensor with miminum values of  x .", 
            "title": "min"
        }, 
        {
            "location": "/backend/#sum", 
            "text": "sum(x, axis=None, keepdims=False)  Sum of the values in a tensor, alongside the specified axis.  Arguments   x : A tensor or variable.  axis : An integer, the axis to sum over.  keepdims : A boolean, whether to keep the dimensions or not.\nIf  keepdims  is  False , the rank of the tensor is reduced\nby 1. If  keepdims  is  True ,\nthe reduced dimension is retained with length 1.   Returns  A tensor with sum of  x .", 
            "title": "sum"
        }, 
        {
            "location": "/backend/#prod", 
            "text": "prod(x, axis=None, keepdims=False)  Multiplies the values in a tensor, alongside the specified axis.  Arguments   x : A tensor or variable.  axis : An integer, the axis to compute the product.  keepdims : A boolean, whether to keep the dimensions or not.\nIf  keepdims  is  False , the rank of the tensor is reduced\nby 1. If  keepdims  is  True ,\nthe reduced dimension is retained with length 1.   Returns  A tensor with the product of elements of  x .", 
            "title": "prod"
        }, 
        {
            "location": "/backend/#var", 
            "text": "var(x, axis=None, keepdims=False)  Variance of a tensor, alongside the specified axis.  Arguments   x : A tensor or variable.  axis : An integer, the axis to compute the variance.  keepdims : A boolean, whether to keep the dimensions or not.\nIf  keepdims  is  False , the rank of the tensor is reduced\nby 1. If  keepdims  is  True ,\nthe reduced dimension is retained with length 1.   Returns  A tensor with the variance of elements of  x .", 
            "title": "var"
        }, 
        {
            "location": "/backend/#std", 
            "text": "std(x, axis=None, keepdims=False)  Standard deviation of a tensor, alongside the specified axis.  Arguments   x : A tensor or variable.  axis : An integer, the axis to compute the standard deviation.  keepdims : A boolean, whether to keep the dimensions or not.\nIf  keepdims  is  False , the rank of the tensor is reduced\nby 1. If  keepdims  is  True ,\nthe reduced dimension is retained with length 1.   Returns  A tensor with the standard deviation of elements of  x .", 
            "title": "std"
        }, 
        {
            "location": "/backend/#mean", 
            "text": "mean(x, axis=None, keepdims=False)  Mean of a tensor, alongside the specified axis.  Arguments   x : A tensor or variable.  axis : A list of integer. Axes to compute the mean.  keepdims : A boolean, whether to keep the dimensions or not.\nIf  keepdims  is  False , the rank of the tensor is reduced\nby 1 for each entry in  axis . If  keep_dims  is  True ,\nthe reduced dimensions are retained with length 1.   Returns  A tensor with the mean of elements of  x .", 
            "title": "mean"
        }, 
        {
            "location": "/backend/#any", 
            "text": "any(x, axis=None, keepdims=False)  Bitwise reduction (logical OR).  Arguments   x : input tensor.  axis : axis along which to perform the reduction.  keepdims : whether the drop or broadcast the reduction axes.   Returns  A uint8 tensor (0s and 1s).", 
            "title": "any"
        }, 
        {
            "location": "/backend/#all", 
            "text": "all(x, axis=None, keepdims=False)  Bitwise reduction (logical AND).  Arguments   x : input tensor.  axis : axis along which to perform the reduction.  keepdims : whether the drop or broadcast the reduction axes.   Returns  A uint8 tensor (0s and 1s).", 
            "title": "all"
        }, 
        {
            "location": "/backend/#argmax", 
            "text": "argmax(x, axis=-1)  Returns the index of the maximum value along an axis.  Arguments   x : input tensor.  axis : axis along which to perform the reduction.  keepdims : whether the drop or broadcast the reduction axes.   Returns  A tensor.", 
            "title": "argmax"
        }, 
        {
            "location": "/backend/#argmin", 
            "text": "argmin(x, axis=-1)  Returns the index of the minimum value along an axis.  Arguments   x : input tensor.  axis : axis along which to perform the reduction.  keepdims : whether the drop or broadcast the reduction axes.   Returns  A tensor.", 
            "title": "argmin"
        }, 
        {
            "location": "/backend/#square", 
            "text": "square(x)  Element-wise square.  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "square"
        }, 
        {
            "location": "/backend/#abs", 
            "text": "abs(x)  Element-wise absolute value.  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "abs"
        }, 
        {
            "location": "/backend/#sqrt", 
            "text": "sqrt(x)  Element-wise square root.  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "sqrt"
        }, 
        {
            "location": "/backend/#exp", 
            "text": "exp(x)  Element-wise exponential.  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "exp"
        }, 
        {
            "location": "/backend/#log", 
            "text": "log(x)  Element-wise log.  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "log"
        }, 
        {
            "location": "/backend/#round", 
            "text": "round(x)  Element-wise rounding to the closest integer.  In case of tie, the rounding mode used is \"half to even\".  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "round"
        }, 
        {
            "location": "/backend/#sign", 
            "text": "sign(x)  Element-wise sign.  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "sign"
        }, 
        {
            "location": "/backend/#pow", 
            "text": "pow(x, a)  Element-wise exponentiation.  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "pow"
        }, 
        {
            "location": "/backend/#clip", 
            "text": "clip(x, min_value, max_value)  Element-wise value clipping.  Returns  A tensor.", 
            "title": "clip"
        }, 
        {
            "location": "/backend/#equal", 
            "text": "equal(x, y)  Element-wise equality between two tensors.  Returns  A bool tensor.", 
            "title": "equal"
        }, 
        {
            "location": "/backend/#not_equal", 
            "text": "not_equal(x, y)  Element-wise inequality between two tensors.  Returns  A bool tensor.", 
            "title": "not_equal"
        }, 
        {
            "location": "/backend/#greater", 
            "text": "greater(x, y)  Element-wise truth value of (x   y).  Returns  A bool tensor.", 
            "title": "greater"
        }, 
        {
            "location": "/backend/#greater_equal", 
            "text": "greater_equal(x, y)  Element-wise truth value of (x  = y).  Returns  A bool tensor.", 
            "title": "greater_equal"
        }, 
        {
            "location": "/backend/#lesser", 
            "text": "lesser(x, y)  Element-wise truth value of (x   y).  Returns  A bool tensor.", 
            "title": "lesser"
        }, 
        {
            "location": "/backend/#lesser_equal", 
            "text": "lesser_equal(x, y)  Element-wise truth value of (x  = y).  Returns  A bool tensor.", 
            "title": "lesser_equal"
        }, 
        {
            "location": "/backend/#maximum", 
            "text": "maximum(x, y)  Element-wise maximum of two tensors.  Returns  A tensor.", 
            "title": "maximum"
        }, 
        {
            "location": "/backend/#minimum", 
            "text": "minimum(x, y)  Element-wise minimum of two tensors.  Returns  A tensor.", 
            "title": "minimum"
        }, 
        {
            "location": "/backend/#sin", 
            "text": "sin(x)  Computes sin of x element-wise.  Returns  A tensor.", 
            "title": "sin"
        }, 
        {
            "location": "/backend/#cos", 
            "text": "cos(x)  Computes cos of x element-wise.  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "cos"
        }, 
        {
            "location": "/backend/#normalize_batch_in_training", 
            "text": "normalize_batch_in_training(x, gamma, beta, reduction_axes, epsilon=0.001)  Computes mean and std for batch then apply batch_normalization on batch.  Returns  A tuple length of 3,  (normalized_tensor, mean, variance) .", 
            "title": "normalize_batch_in_training"
        }, 
        {
            "location": "/backend/#batch_normalization", 
            "text": "batch_normalization(x, mean, var, beta, gamma, epsilon=0.001)  Applies batch normalization on x given mean, var, beta and gamma:  output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta  Returns  A tensor.", 
            "title": "batch_normalization"
        }, 
        {
            "location": "/backend/#concatenate", 
            "text": "concatenate(tensors, axis=-1)  Concatenates a list of tensors alongside the specified axis.  Returns  A tensor.", 
            "title": "concatenate"
        }, 
        {
            "location": "/backend/#reshape", 
            "text": "reshape(x, shape)  Reshapes a tensor to the specified shape.  Returns  A tensor.", 
            "title": "reshape"
        }, 
        {
            "location": "/backend/#permute_dimensions", 
            "text": "permute_dimensions(x, pattern)  Permutes axes in a tensor.  Arguments   pattern : should be a tuple of\ndimension indices, e.g. (0, 2, 1).   Returns  A tensor.", 
            "title": "permute_dimensions"
        }, 
        {
            "location": "/backend/#resize_images", 
            "text": "resize_images(X, height_factor, width_factor, dim_ordering)  Resizes the images contained in a 4D tensor of shape\n-  [batch, channels, height, width]  (for 'th' dim_ordering)\n-  [batch, height, width, channels]  (for 'tf' dim_ordering)\nby a factor of  (height_factor, width_factor) . Both factors should be\npositive integers.  Returns  A tensor.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "resize_images"
        }, 
        {
            "location": "/backend/#resize_volumes", 
            "text": "resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering)  Resizes the volume contained in a 5D tensor of shape\n-  [batch, channels, depth, height, width]  (for 'th' dim_ordering)\n-  [batch, depth, height, width, channels]  (for 'tf' dim_ordering)\nby a factor of  (depth_factor, height_factor, width_factor) .\nAll three factors should be positive integers.  Returns  A tensor.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "resize_volumes"
        }, 
        {
            "location": "/backend/#repeat_elements", 
            "text": "repeat_elements(x, rep, axis)  Repeats the elements of a tensor along an axis, like  np.repeat .  If  x  has shape  (s1, s2, s3)  and  axis  is  1 , the output\nwill have shape  (s1, s2 * rep, s3) .  Returns  A tensor.", 
            "title": "repeat_elements"
        }, 
        {
            "location": "/backend/#repeat", 
            "text": "repeat(x, n)  Repeats a 2D tensor.  if  x  has shape (samples, dim) and  n  is  2 ,\nthe output will have shape  (samples, 2, dim) .  Returns  A tensor.", 
            "title": "repeat"
        }, 
        {
            "location": "/backend/#arange", 
            "text": "arange(start, stop=None, step=1, dtype='int32')  Creates a 1-D tensor containing a sequence of integers.  The function arguments use the same convention as\nTheano's arange: if only one argument is provided,\nit is in fact the \"stop\" argument.  The default type of the returned tensor is  'int32'  to\nmatch TensorFlow's default.", 
            "title": "arange"
        }, 
        {
            "location": "/backend/#tile", 
            "text": "tile(x, n)  Creates a tensor by tiling  x  by  n .  Arguments   x : A tensor or variable  n : A list of integer. The length must be the same as the number of\ndimensions in  x .   Returns  A tiled tensor.", 
            "title": "tile"
        }, 
        {
            "location": "/backend/#flatten", 
            "text": "flatten(x)  Flatten a tensor.  Returns  A tensor, reshaped into 1-D", 
            "title": "flatten"
        }, 
        {
            "location": "/backend/#batch_flatten", 
            "text": "batch_flatten(x)  Turn a n-D tensor into a 2D tensor where\nthe first dimension is conserved.  In other words, it flattens each data samples of a batch.  Returns  A tensor.", 
            "title": "batch_flatten"
        }, 
        {
            "location": "/backend/#expand_dims", 
            "text": "expand_dims(x, dim=-1)  Adds a 1-sized dimension at index \"dim\".  Returns  A tensor with expended dimensions.", 
            "title": "expand_dims"
        }, 
        {
            "location": "/backend/#squeeze", 
            "text": "squeeze(x, axis)  Removes a 1-dimension from the tensor at index \"axis\".  Returns  A tensor with the same data as  x  but reduced dimensions.", 
            "title": "squeeze"
        }, 
        {
            "location": "/backend/#temporal_padding", 
            "text": "temporal_padding(x, padding=1)  Pads the middle dimension of a 3D tensor\nwith \"padding\" zeros left and right.  Returns  A padded 3D tensor.", 
            "title": "temporal_padding"
        }, 
        {
            "location": "/backend/#asymmetric_temporal_padding", 
            "text": "asymmetric_temporal_padding(x, left_pad=1, right_pad=1)  Pad the middle dimension of a 3D tensor\nwith \"left_pad\" zeros left and \"right_pad\" right.  Returns  A padded 3D tensor.", 
            "title": "asymmetric_temporal_padding"
        }, 
        {
            "location": "/backend/#spatial_2d_padding", 
            "text": "spatial_2d_padding(x, padding=(1, 1), dim_ordering='default')  Pads the 2nd and 3rd dimensions of a 4D tensor\nwith \"padding[0]\" and \"padding[1]\" (resp.) zeros left and right.  Returns  A padded 4D tensor.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "spatial_2d_padding"
        }, 
        {
            "location": "/backend/#asymmetric_spatial_2d_padding", 
            "text": "asymmetric_spatial_2d_padding(x, top_pad=1, bottom_pad=1, left_pad=1, right_pad=1, dim_ordering='default')  Pad the rows and columns of a 4D tensor\nwith \"top_pad\", \"bottom_pad\", \"left_pad\", \"right_pad\" (resp.) zeros\nrows on top, bottom; cols on left, right.  Returns  A padded 4D tensor.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "asymmetric_spatial_2d_padding"
        }, 
        {
            "location": "/backend/#spatial_3d_padding", 
            "text": "spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default')  Pads 5D tensor with zeros for the depth, height, width dimension with\n\"padding[0]\", \"padding[1]\" and \"padding[2]\" (resp.) zeros left and right  For 'tf' dim_ordering, the 2nd, 3rd and 4th dimension will be padded.\nFor 'th' dim_ordering, the 3rd, 4th and 5th dimension will be padded.  Returns  A padded 5D tensor.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "spatial_3d_padding"
        }, 
        {
            "location": "/backend/#stack", 
            "text": "stack(x)  Stacks a list of rank  R  tensors into a rank  R+1  tensor.  Arguments   x : input tensor.   Returns  A tensor.", 
            "title": "stack"
        }, 
        {
            "location": "/backend/#one_hot", 
            "text": "one_hot(indices, nb_classes)  Input: nD integer tensor of shape  (batch_size, dim1, dim2, ... dim(n-1)) \n-  Output : (n + 1)D one hot representation of the input\nwith shape  (batch_size, dim1, dim2, ... dim(n-1), nb_classes)  Returns  The one-hot tensor.", 
            "title": "one_hot"
        }, 
        {
            "location": "/backend/#reverse", 
            "text": "reverse(x, axes)  Reverse a tensor along the the specified axes  Returns  A tensor.", 
            "title": "reverse"
        }, 
        {
            "location": "/backend/#get_value", 
            "text": "get_value(x)  Returns the value of a variable.  Arguments   x : input variable.   Returns  A Numpy array.", 
            "title": "get_value"
        }, 
        {
            "location": "/backend/#batch_get_value", 
            "text": "batch_get_value(xs)  Returns the value of more than one tensor variable.  Arguments   x : list of variables.   Returns  A list of Numpy arrays.", 
            "title": "batch_get_value"
        }, 
        {
            "location": "/backend/#set_value", 
            "text": "set_value(x, value)  Sets the value of a variable,\nfrom a Numpy array. It returns  None .", 
            "title": "set_value"
        }, 
        {
            "location": "/backend/#batch_set_value", 
            "text": "batch_set_value(tuples)  Sets the values of many tensor variables at once.\nIt returns  None .  Arguments   tuples : a list of tuples  (tensor, value) . value  should be a Numpy array.", 
            "title": "batch_set_value"
        }, 
        {
            "location": "/backend/#get_variable_shape", 
            "text": "get_variable_shape(x)  Returns shape of a variable.  Arguments  A variable.  Returns  A tuple of integers.", 
            "title": "get_variable_shape"
        }, 
        {
            "location": "/backend/#print_tensor", 
            "text": "print_tensor(x, message='')  Print the message and the tensor when evaluated and return the same\ntensor.", 
            "title": "print_tensor"
        }, 
        {
            "location": "/backend/#function", 
            "text": "function(inputs, outputs, updates=[])  Instantiates a Keras function.  Arguments   inputs : list of placeholder/variable tensors.  outputs : list of output tensors.  updates : list of update tuples (old_tensor, new_tensor).", 
            "title": "function"
        }, 
        {
            "location": "/backend/#gradients", 
            "text": "gradients(loss, variables)  Returns the gradients of  variables  (list of tensor variables)\nwith regard to  loss .", 
            "title": "gradients"
        }, 
        {
            "location": "/backend/#stop_gradient", 
            "text": "stop_gradient(variables)  Returns  variables  but with zero gradient with respect to every other\nvariables.", 
            "title": "stop_gradient"
        }, 
        {
            "location": "/backend/#rnn", 
            "text": "rnn(step_function, inputs, initial_states, go_backwards=False, mask=None, constants=None, unroll=False, input_length=None)  Iterates over the time dimension of a tensor.  Arguments   inputs : tensor of temporal data of shape  (samples, time, ...) \n(at least 3D).  step_function :  Parameters :  input : tensor with shape  (samples, ...)  (no time dimension),\nrepresenting input for the batch of samples at a certain\ntime step.  states : list of tensors.    Returns :  output : tensor with shape  (samples, output_dim) \n(no time dimension).  new_states : list of tensors, same length and shapes\nas 'states'. The first state in the list must be the\noutput tensor at the previous timestep.    initial_states : tensor with shape (samples, output_dim)\n(no time dimension),\ncontaining the initial values for the states used in\nthe step function.  go_backwards : boolean. If True, do the iteration over\nthe time dimension in reverse order.  mask : binary tensor with shape  (samples, time, 1) ,\nwith a zero for every element that is masked.  constants : a list of constant values passed at each step.  unroll : whether to unroll the RNN or to use a symbolic loop ( while_loop  or  scan  depending on backend).  input_length : not relevant in the TensorFlow implementation.\nMust be specified if using unrolling with Theano.   Returns  A tuple,  (last_output, outputs, new_states) .   last_output : the latest output of the rnn, of shape  (samples, ...)  outputs : tensor with shape  (samples, time, ...)  where each\n    entry  outputs[s, t]  is the output of the step function\n    at time  t  for sample  s .  new_states : list of tensors, latest states returned by\n    the step function, of shape  (samples, ...) .   Raises   ValueError : if input dimension is less than 3.  ValueError : if  unroll  is  True  but input timestep is not a fixed number.  ValueError : if  mask  is provided (not  None ) but states is not provided\n( len(states)  == 0).", 
            "title": "rnn"
        }, 
        {
            "location": "/backend/#switch", 
            "text": "switch(condition, then_expression, else_expression)  Switches between two operations\ndepending on a scalar value ( int  or  bool ).\nNote that both  then_expression  and  else_expression \nshould be symbolic tensors of the  same shape .  Arguments   condition : scalar tensor.  then_expression : either a tensor, or a callable that returns a tensor.  else_expression : either a tensor, or a callable that returns a tensor.   Returns  The selected tensor.", 
            "title": "switch"
        }, 
        {
            "location": "/backend/#in_train_phase", 
            "text": "in_train_phase(x, alt)  Selects  x  in train phase, and  alt  otherwise.\nNote that  alt  should have the  same shape  as  x .  Returns  Either  x  or  alt  based on  K.learning_phase .", 
            "title": "in_train_phase"
        }, 
        {
            "location": "/backend/#in_test_phase", 
            "text": "in_test_phase(x, alt)  Selects  x  in test phase, and  alt  otherwise.\nNote that  alt  should have the  same shape  as  x .  Returns  Either  x  or  alt  based on  K.learning_phase .", 
            "title": "in_test_phase"
        }, 
        {
            "location": "/backend/#relu", 
            "text": "relu(x, alpha=0.0, max_value=None)  Rectified linear unit.\nWith default values, it returns element-wise  max(x, 0) .  Arguments   x : A tensor or variable.  alpha : A scalar, slope of negative section (default= 0. ).  max_value : Saturation threshold.   Returns  A tensor.", 
            "title": "relu"
        }, 
        {
            "location": "/backend/#elu", 
            "text": "elu(x, alpha=1.0)  Exponential linear unit.  Arguments   x : A tenor or variable to compute the activation function for.  alpha : A scalar, slope of positive section.   Returns  A tensor.", 
            "title": "elu"
        }, 
        {
            "location": "/backend/#softmax", 
            "text": "softmax(x)  Softmax of a tensor.  Arguments   x : A tensor or variable.   Returns  A tensor.", 
            "title": "softmax"
        }, 
        {
            "location": "/backend/#softplus", 
            "text": "softplus(x)  Softplus of a tensor.  Arguments   x : A tensor or variable.   Returns  A tensor.", 
            "title": "softplus"
        }, 
        {
            "location": "/backend/#softsign", 
            "text": "softsign(x)  Softsign of a tensor.  Arguments   x : A tensor or variable.   Returns  A tensor.", 
            "title": "softsign"
        }, 
        {
            "location": "/backend/#categorical_crossentropy", 
            "text": "categorical_crossentropy(output, target, from_logits=False)  Categorical crossentropy between an output tensor\nand a target tensor, where the target is a tensor of the same\nshape as the output.", 
            "title": "categorical_crossentropy"
        }, 
        {
            "location": "/backend/#sparse_categorical_crossentropy", 
            "text": "sparse_categorical_crossentropy(output, target, from_logits=False)  Categorical crossentropy between an output tensor\nand a target tensor, where the target is an integer tensor.", 
            "title": "sparse_categorical_crossentropy"
        }, 
        {
            "location": "/backend/#binary_crossentropy", 
            "text": "binary_crossentropy(output, target, from_logits=False)  Binary crossentropy between an output tensor and a target tensor.  Arguments   output : A tensor.  target : A tensor with the same shape as  output .  from_logits : Whether  output  is expected to be a logits tensor.\nBy default, we consider that  output \nencodes a probability distribution.   Returns  A tensor.", 
            "title": "binary_crossentropy"
        }, 
        {
            "location": "/backend/#sigmoid", 
            "text": "sigmoid(x)  Element-wise sigmoid.  Arguments   x : A tensor or variable.   Returns  A tensor.", 
            "title": "sigmoid"
        }, 
        {
            "location": "/backend/#hard_sigmoid", 
            "text": "hard_sigmoid(x)  Segment-wise linear approximation of sigmoid.\nFaster than sigmoid.\nReturns  0.  if  x   -2.5 ,  1.  if  x   2.5 .\nIn  -2.5  = x  = 2.5 , returns  0.2 * x + 0.5 .  Arguments   x : A tensor or variable.   Returns  A tensor.", 
            "title": "hard_sigmoid"
        }, 
        {
            "location": "/backend/#tanh", 
            "text": "tanh(x)  Element-wise tanh.  Arguments   x : A tensor or variable.   Returns  A tensor.", 
            "title": "tanh"
        }, 
        {
            "location": "/backend/#dropout", 
            "text": "dropout(x, level, noise_shape=None, seed=None)  Sets entries in  x  to zero at random, while scaling the entire tensor.  Arguments   x : tensor  level : fraction of the entries in the tensor\nthat will be set to 0.  noise_shape : shape for randomly generated keep/drop flags,\nmust be broadcastable to the shape of  x  seed : random seed to ensure determinism.   Returns  A tensor.", 
            "title": "dropout"
        }, 
        {
            "location": "/backend/#l2_normalize", 
            "text": "l2_normalize(x, axis)  Normalizes a tensor wrt the L2 norm alongside the specified axis.  Arguments   x : input tensor.  axis : axis along which to perform normalization.   Returns  A tensor.", 
            "title": "l2_normalize"
        }, 
        {
            "location": "/backend/#in_top_k", 
            "text": "in_top_k(predictions, targets, k)  Returns whether the  targets  are in the top  k   predictions  Arguments   predictions : A tensor of shape  batch_size  x classes and type  float32 .  targets : A tensor of shape batch_size and type  int32  or  int64 .  k : An  int , number of top elements to consider.   Returns  A tensor of shape  batch_size  and type  bool .  output_i  is  True  if targets_i  is within top-k values of  predictions_i", 
            "title": "in_top_k"
        }, 
        {
            "location": "/backend/#conv1d", 
            "text": "conv1d(x, kernel, stride=1, border_mode='valid', image_shape=None, filter_shape=None)  1D convolution.  Arguments   kernel : kernel tensor.  strides : stride integer.  border_mode : string,  \"same\"  or  \"valid\" .   Returns  A tensor, result of 1D convolution.", 
            "title": "conv1d"
        }, 
        {
            "location": "/backend/#conv2d", 
            "text": "conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='default', image_shape=None, filter_shape=None, filter_dilation=(1, 1))  2D convolution.  Arguments   kernel : kernel tensor.  strides : strides tuple.  border_mode : string,  \"same\"  or  \"valid\" .  dim_ordering :  \"tf\"  or  \"th\" .\nWhether to use Theano or TensorFlow dimension ordering\nfor inputs/kernels/ouputs.   Returns  A tensor, result of 2D convolution.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "conv2d"
        }, 
        {
            "location": "/backend/#deconv2d", 
            "text": "deconv2d(x, kernel, output_shape, strides=(1, 1), border_mode='valid', dim_ordering='default', image_shape=None, filter_shape=None)  2D deconvolution (i.e. transposed convolution).  Arguments   x : input tensor.  kernel : kernel tensor.  output_shape : 1D int tensor for the output shape.  strides : strides tuple.  border_mode : string,  \"same\"  or  \"valid\" .  dim_ordering :  \"tf\"  or  \"th\" .\nWhether to use Theano or TensorFlow dimension ordering\nfor inputs/kernels/ouputs.   Returns  A tensor, result of transposed 2D convolution.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "deconv2d"
        }, 
        {
            "location": "/backend/#atrous_conv2d", 
            "text": "atrous_conv2d(x, kernel, rate=1, border_mode='valid', dim_ordering='default', image_shape=None, filter_shape=None)  Atrous 2D convolution. Also as known as dilated convolution.  Arguments   x : input tensor.  kernel : kernel tensor.  rate : integer   0, the sample stride.  output_shape : 1D int tensor for the output shape.  strides : strides tuple.  border_mode : string,  \"same\"  or  \"valid\" .  dim_ordering :  \"tf\"  or  \"th\" .\nWhether to use Theano or TensorFlow dimension ordering\nfor inputs/kernels/ouputs.   Returns  A tensor, result of atrous transposed 2D convolution.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "atrous_conv2d"
        }, 
        {
            "location": "/backend/#separable_conv2d", 
            "text": "separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1), border_mode='valid', dim_ordering='default')  2-D convolution with separable filters.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "separable_conv2d"
        }, 
        {
            "location": "/backend/#conv3d", 
            "text": "conv3d(x, kernel, strides=(1, 1, 1), border_mode='valid', dim_ordering='default', volume_shape=None, filter_shape=None)  3D convolution.  Arguments   kernel : kernel tensor.  strides : strides tuple.  border_mode : string,  \"same\"  or  \"valid\" .  dim_ordering :  \"tf\"  or  \"th\" .\nWhether to use Theano or TensorFlow dimension ordering\nfor inputs/kernels/ouputs.   Returns  A tensor, result of 3D convolution.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .", 
            "title": "conv3d"
        }, 
        {
            "location": "/backend/#pool2d", 
            "text": "pool2d(x, pool_size, strides=(1, 1), border_mode='valid', dim_ordering='default', pool_mode='max')  2D Pooling.  Arguments   pool_size : tuple of 2 integers.  strides : tuple of 2 integers.  border_mode : one of  \"valid\" ,  \"same\" .  dim_ordering : one of  \"th\" ,  \"tf\" .  pool_mode : one of  \"max\" ,  \"avg\" .   Returns  A tensor, result of 2D pooling.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .  ValueError : if  pool_mode  is neither  max  or  avg .", 
            "title": "pool2d"
        }, 
        {
            "location": "/backend/#pool3d", 
            "text": "pool3d(x, pool_size, strides=(1, 1, 1), border_mode='valid', dim_ordering='default', pool_mode='max')  3D Pooling.  Arguments   pool_size : tuple of 3 integers.  strides : tuple of 3 integers.  border_mode : one of  \"valid\" ,  \"same\" .  dim_ordering : one of  \"th\" ,  \"tf\" .  pool_mode : one of  \"max\" ,  \"avg\" .   Returns  A tensor, result of 3D pooling.  Raises   ValueError : if  dim_ordering  is neither  tf  or  th .  ValueError : if  pool_mode  is neither  max  or  avg .", 
            "title": "pool3d"
        }, 
        {
            "location": "/backend/#random_normal", 
            "text": "random_normal(shape, mean=0.0, std=1.0, dtype=None, seed=None)  Returns a tensor with normal distribution  Arguments   shape : A tuple of integers, the shape of tensor to create.  mean : A float, mean of the normal distribution to draw samples.  std : A float, standard deviation of the normal distribution\nto draw samples.  dtype : String, dtype of returned tensor.  seed : Integer, random seed.   Returns  A tensor.", 
            "title": "random_normal"
        }, 
        {
            "location": "/backend/#random_uniform", 
            "text": "random_uniform(shape, low=0.0, high=1.0, dtype=None, seed=None)  Returns a tensor with uniform distribution  Arguments   shape : A tuple of integers, the shape of tensor to create.  low : A float, lower boundary of the uniform distribution\nto draw samples.  high : A float, upper boundary of the uniform distribution\nto draw samples.  dtype : String, dtype of returned tensor.  seed : Integer, random seed.   Returns  A tensor.", 
            "title": "random_uniform"
        }, 
        {
            "location": "/backend/#random_binomial", 
            "text": "random_binomial(shape, p=0.0, dtype=None, seed=None)  Returns a tensor with binomlai distribution  Arguments   shape : A tuple of integers, the shape of tensor to create.  p : A float,  0.  = p  = 1 , probability of binomlai distribution.  dtype : String, dtype of returned tensor.  seed : Integer, random seed.   Returns  A tensor.", 
            "title": "random_binomial"
        }, 
        {
            "location": "/backend/#ctc_batch_cost", 
            "text": "ctc_batch_cost(y_true, y_pred, input_length, label_length)  Runs CTC loss algorithm on each batch element.  Arguments   y_true : tensor  (samples, max_string_length)  containing the truth labels.  y_pred : tensor  (samples, time_steps, num_categories)  containing the prediction,\n    or output of the softmax.  input_length : tensor  (samples, 1)  containing the sequence length for\n    each batch item in  y_pred .  label_length : tensor  (samples, 1)  containing the sequence length for\n    each batch item in  y_true .   Returns  Tensor with shape (samples,1) containing the\nCTC loss of each element", 
            "title": "ctc_batch_cost"
        }, 
        {
            "location": "/backend/#ctc_decode", 
            "text": "ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1)  Decodes the output of a softmax using either\n   greedy (also known as best path) or a constrained dictionary\n   search.  Arguments   y_pred : tensor  (samples, time_steps, num_categories)  containing the prediction,\n    or output of the softmax.  input_length : tensor  (samples, )  containing the sequence length for\n    each batch item in  y_pred .  greedy : perform much faster best-path search if  true . This does\n    not use a dictionary  beam_width : if  greedy  is  false : a beam search decoder will be used\n    with a beam of this width  top_paths : if  greedy  is  false : how many of the most probable paths will be returned   Returns   Tuple :  List : if  greedy  is  true , returns a list of one element that contains\n    the decoded sequence. If  false , returns the  top_paths  most probable\n    decoded sequences. Important: blank labels are returned as  -1 .\nTensor  (top_paths, )  that contains the log probability of each decoded sequence", 
            "title": "ctc_decode"
        }, 
        {
            "location": "/backend/#map_fn", 
            "text": "map_fn(fn, elems, name=None)  Map the function fn over the elements elems and return the outputs.  Arguments   fn : Callable that will be called upon each element in elems  elems : tensor  name : A string name for the map node in the graph   Returns  Tensor with first dimension equal to the elems and second depending on\nfn", 
            "title": "map_fn"
        }, 
        {
            "location": "/backend/#foldl", 
            "text": "foldl(fn, elems, initializer=None, name=None)  Reduce elems using fn to combine them from left to right.  Arguments   fn : Callable that will be called upon each element in elems and an\naccumulator, for instance  lambda acc, x: acc + x  elems : tensor  initializer : The first value used ( elems[0]  in case of None)  name : A string name for the foldl node in the graph   Returns  Same type and shape as initializer", 
            "title": "foldl"
        }, 
        {
            "location": "/backend/#foldr", 
            "text": "foldr(fn, elems, initializer=None, name=None)  Reduce elems using fn to combine them from right to left.  Arguments   fn : Callable that will be called upon each element in elems and an\naccumulator, for instance  lambda acc, x: acc + x  elems : tensor  initializer : The first value used ( elems[-1]  in case of None)  name : A string name for the foldr node in the graph   Returns  Same type and shape as initializer", 
            "title": "foldr"
        }, 
        {
            "location": "/backend/#backend", 
            "text": "backend()  Publicly accessible method\nfor determining the current backend.", 
            "title": "backend"
        }, 
        {
            "location": "/initializations/", 
            "text": "Usage of initializations\n\n\nInitializations define the way to set the initial random weights of Keras layers.\n\n\nThe keyword arguments used for passing initializations to layers will depend on the layer. Usually it is simply \ninit\n:\n\n\nmodel.add(Dense(64, init='uniform'))\n\n\n\n\nAvailable initializations\n\n\n\n\nuniform\n\n\nlecun_uniform\n: Uniform initialization scaled by the square root of the number of inputs (LeCun 98).\n\n\nnormal\n\n\nidentity\n: Use with square 2D layers (\nshape[0] == shape[1]\n).\n\n\northogonal\n: Use with square 2D layers (\nshape[0] == shape[1]\n).\n\n\nzero\n\n\nglorot_normal\n: Gaussian initialization scaled by fan_in + fan_out (Glorot 2010)\n\n\nglorot_uniform\n\n\nhe_normal\n: Gaussian initialization scaled by fan_in (He et al., 2014)\n\n\nhe_uniform\n\n\n\n\nAn initialization may be passed as a string (must match one of the available initializations above), or as a callable.\nIf a callable, then it must take two arguments: \nshape\n (shape of the variable to initialize) and \nname\n (name of the variable),\nand it must return a variable (e.g. output of \nK.variable()\n):\n\n\nfrom keras import backend as K\nimport numpy as np\n\ndef my_init(shape, name=None):\n    value = np.random.random(shape)\n    return K.variable(value, name=name)\n\nmodel.add(Dense(64, init=my_init))\n\n\n\n\nYou could also use functions from \nkeras.initializations\n in this way:\n\n\nfrom keras import initializations\n\ndef my_init(shape, name=None):\n    return initializations.normal(shape, scale=0.01, name=name)\n\nmodel.add(Dense(64, init=my_init))", 
            "title": "Initializations"
        }, 
        {
            "location": "/initializations/#usage-of-initializations", 
            "text": "Initializations define the way to set the initial random weights of Keras layers.  The keyword arguments used for passing initializations to layers will depend on the layer. Usually it is simply  init :  model.add(Dense(64, init='uniform'))", 
            "title": "Usage of initializations"
        }, 
        {
            "location": "/initializations/#available-initializations", 
            "text": "uniform  lecun_uniform : Uniform initialization scaled by the square root of the number of inputs (LeCun 98).  normal  identity : Use with square 2D layers ( shape[0] == shape[1] ).  orthogonal : Use with square 2D layers ( shape[0] == shape[1] ).  zero  glorot_normal : Gaussian initialization scaled by fan_in + fan_out (Glorot 2010)  glorot_uniform  he_normal : Gaussian initialization scaled by fan_in (He et al., 2014)  he_uniform   An initialization may be passed as a string (must match one of the available initializations above), or as a callable.\nIf a callable, then it must take two arguments:  shape  (shape of the variable to initialize) and  name  (name of the variable),\nand it must return a variable (e.g. output of  K.variable() ):  from keras import backend as K\nimport numpy as np\n\ndef my_init(shape, name=None):\n    value = np.random.random(shape)\n    return K.variable(value, name=name)\n\nmodel.add(Dense(64, init=my_init))  You could also use functions from  keras.initializations  in this way:  from keras import initializations\n\ndef my_init(shape, name=None):\n    return initializations.normal(shape, scale=0.01, name=name)\n\nmodel.add(Dense(64, init=my_init))", 
            "title": "Available initializations"
        }, 
        {
            "location": "/regularizers/", 
            "text": "Usage of regularizers\n\n\nRegularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes.\n\n\nThe penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers \nDense\n, \nTimeDistributedDense\n, \nMaxoutDense\n, \nConvolution1D\n and \nConvolution2D\n have a unified API.\n\n\nThese layers expose 3 keyword arguments:\n\n\n\n\nW_regularizer\n: instance of \nkeras.regularizers.WeightRegularizer\n\n\nb_regularizer\n: instance of \nkeras.regularizers.WeightRegularizer\n\n\nactivity_regularizer\n: instance of \nkeras.regularizers.ActivityRegularizer\n\n\n\n\nExample\n\n\nfrom keras.regularizers import l2, activity_l2\nmodel.add(Dense(64, input_dim=64, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))\n\n\n\n\nAvailable penalties\n\n\nkeras.regularizers.WeightRegularizer(l1=0., l2=0.)\n\n\n\n\nkeras.regularizers.ActivityRegularizer(l1=0., l2=0.)\n\n\n\n\nShortcuts\n\n\nThese are shortcut functions available in \nkeras.regularizers\n.\n\n\n\n\nl1\n(l=0.01): L1 weight regularization penalty, also known as LASSO\n\n\nl2\n(l=0.01): L2 weight regularization penalty, also known as weight decay, or Ridge\n\n\nl1l2\n(l1=0.01, l2=0.01): L1-L2 weight regularization penalty, also known as ElasticNet\n\n\nactivity_l1\n(l=0.01): L1 activity regularization\n\n\nactivity_l2\n(l=0.01): L2 activity regularization\n\n\nactivity_l1l2\n(l1=0.01, l2=0.01): L1+L2 activity regularization", 
            "title": "Regularizers"
        }, 
        {
            "location": "/regularizers/#usage-of-regularizers", 
            "text": "Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes.  The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers  Dense ,  TimeDistributedDense ,  MaxoutDense ,  Convolution1D  and  Convolution2D  have a unified API.  These layers expose 3 keyword arguments:   W_regularizer : instance of  keras.regularizers.WeightRegularizer  b_regularizer : instance of  keras.regularizers.WeightRegularizer  activity_regularizer : instance of  keras.regularizers.ActivityRegularizer", 
            "title": "Usage of regularizers"
        }, 
        {
            "location": "/regularizers/#example", 
            "text": "from keras.regularizers import l2, activity_l2\nmodel.add(Dense(64, input_dim=64, W_regularizer=l2(0.01), activity_regularizer=activity_l2(0.01)))", 
            "title": "Example"
        }, 
        {
            "location": "/regularizers/#available-penalties", 
            "text": "keras.regularizers.WeightRegularizer(l1=0., l2=0.)  keras.regularizers.ActivityRegularizer(l1=0., l2=0.)", 
            "title": "Available penalties"
        }, 
        {
            "location": "/regularizers/#shortcuts", 
            "text": "These are shortcut functions available in  keras.regularizers .   l1 (l=0.01): L1 weight regularization penalty, also known as LASSO  l2 (l=0.01): L2 weight regularization penalty, also known as weight decay, or Ridge  l1l2 (l1=0.01, l2=0.01): L1-L2 weight regularization penalty, also known as ElasticNet  activity_l1 (l=0.01): L1 activity regularization  activity_l2 (l=0.01): L2 activity regularization  activity_l1l2 (l1=0.01, l2=0.01): L1+L2 activity regularization", 
            "title": "Shortcuts"
        }, 
        {
            "location": "/constraints/", 
            "text": "Usage of constraints\n\n\nFunctions from the \nconstraints\n module allow setting constraints (eg. non-negativity) on network parameters during optimization.\n\n\nThe penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers \nDense\n, \nTimeDistributedDense\n, \nMaxoutDense\n, \nConvolution1D\n and \nConvolution2D\n have a unified API.\n\n\nThese layers expose 2 keyword arguments:\n\n\n\n\nW_constraint\n for the main weights matrix\n\n\nb_constraint\n for the bias.\n\n\n\n\nfrom keras.constraints import maxnorm\nmodel.add(Dense(64, W_constraint = maxnorm(2)))\n\n\n\n\nAvailable constraints\n\n\n\n\nmaxnorm\n(m=2): maximum-norm constraint\n\n\nnonneg\n(): non-negativity constraint\n\n\nunitnorm\n(): unit-norm constraint, enforces the matrix to have unit norm along the last axis", 
            "title": "Constraints"
        }, 
        {
            "location": "/constraints/#usage-of-constraints", 
            "text": "Functions from the  constraints  module allow setting constraints (eg. non-negativity) on network parameters during optimization.  The penalties are applied on a per-layer basis. The exact API will depend on the layer, but the layers  Dense ,  TimeDistributedDense ,  MaxoutDense ,  Convolution1D  and  Convolution2D  have a unified API.  These layers expose 2 keyword arguments:   W_constraint  for the main weights matrix  b_constraint  for the bias.   from keras.constraints import maxnorm\nmodel.add(Dense(64, W_constraint = maxnorm(2)))", 
            "title": "Usage of constraints"
        }, 
        {
            "location": "/constraints/#available-constraints", 
            "text": "maxnorm (m=2): maximum-norm constraint  nonneg (): non-negativity constraint  unitnorm (): unit-norm constraint, enforces the matrix to have unit norm along the last axis", 
            "title": "Available constraints"
        }, 
        {
            "location": "/visualization/", 
            "text": "Model visualization\n\n\nThe \nkeras.utils.visualize_util\n module provides utility functions to plot\na Keras model (using graphviz).\n\n\nThis will plot a graph of the model and save it to a file:\n\n\nfrom keras.utils.visualize_util import plot\nplot(model, to_file='model.png')\n\n\n\n\nplot\n takes two optional arguments:\n\n\n\n\nshow_shapes\n (defaults to False) controls whether output shapes are shown in the graph.\n\n\nshow_layer_names\n (defaults to True) controls whether layer names are shown in the graph.\n\n\n\n\nYou can also directly obtain the \npydot.Graph\n object and render it yourself,\nfor example to show it in an ipython notebook :\n\n\nfrom IPython.display import SVG\nfrom keras.utils.visualize_util import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))", 
            "title": "Visualization"
        }, 
        {
            "location": "/visualization/#model-visualization", 
            "text": "The  keras.utils.visualize_util  module provides utility functions to plot\na Keras model (using graphviz).  This will plot a graph of the model and save it to a file:  from keras.utils.visualize_util import plot\nplot(model, to_file='model.png')  plot  takes two optional arguments:   show_shapes  (defaults to False) controls whether output shapes are shown in the graph.  show_layer_names  (defaults to True) controls whether layer names are shown in the graph.   You can also directly obtain the  pydot.Graph  object and render it yourself,\nfor example to show it in an ipython notebook :  from IPython.display import SVG\nfrom keras.utils.visualize_util import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))", 
            "title": "Model visualization"
        }, 
        {
            "location": "/scikit-learn-api/", 
            "text": "Wrappers for the Scikit-Learn API\n\n\nYou can use \nSequential\n Keras models (single-input only) as part of your Scikit-Learn workflow via the wrappers found at \nkeras.wrappers.scikit_learn.py\n.\n\n\nThere are two wrappers available:\n\n\nkeras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params)\n, which implements the Scikit-Learn classifier interface,\n\n\nkeras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params)\n, which implements the Scikit-Learn regressor interface.\n\n\nArguments\n\n\n\n\nbuild_fn\n: callable function or class instance\n\n\nsk_params\n: model parameters \n fitting parameters\n\n\n\n\nbuild_fn\n should construct, compile and return a Keras model, which\nwill then be used to fit/predict. One of the following\nthree values could be passed to build_fn:\n\n\n\n\nA function\n\n\nAn instance of a class that implements the \ncall\n method\n\n\nNone. This means you implement a class that inherits from either\n\nKerasClassifier\n or \nKerasRegressor\n. The \ncall\n method of the\npresent class will then be treated as the default build_fn.\n\n\n\n\nsk_params\n takes both model parameters and fitting parameters. Legal model\nparameters are the arguments of \nbuild_fn\n. Note that like all other\nestimators in scikit-learn, 'build_fn' should provide default values for\nits arguments, so that you could create the estimator without passing any\nvalues to \nsk_params\n.\n\n\nsk_params\n could also accept parameters for calling \nfit\n, \npredict\n,\n\npredict_proba\n, and \nscore\n methods (e.g., \nnb_epoch\n, \nbatch_size\n).\nfitting (predicting) parameters are selected in the following order:\n\n\n\n\nValues passed to the dictionary arguments of\n\nfit\n, \npredict\n, \npredict_proba\n, and \nscore\n methods\n\n\nValues passed to \nsk_params\n\n\nThe default values of the \nkeras.models.Sequential\n\n\nfit\n, \npredict\n, \npredict_proba\n and \nscore\n methods\n\n\n\n\nWhen using scikit-learn's \ngrid_search\n API, legal tunable parameters are\nthose you could pass to \nsk_params\n, including fitting parameters.\nIn other words, you could use \ngrid_search\n to search for the best\n\nbatch_size\n or \nnb_epoch\n as well as the model parameters.", 
            "title": "Scikit-learn API"
        }, 
        {
            "location": "/scikit-learn-api/#wrappers-for-the-scikit-learn-api", 
            "text": "You can use  Sequential  Keras models (single-input only) as part of your Scikit-Learn workflow via the wrappers found at  keras.wrappers.scikit_learn.py .  There are two wrappers available:  keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params) , which implements the Scikit-Learn classifier interface,  keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params) , which implements the Scikit-Learn regressor interface.", 
            "title": "Wrappers for the Scikit-Learn API"
        }, 
        {
            "location": "/scikit-learn-api/#arguments", 
            "text": "build_fn : callable function or class instance  sk_params : model parameters   fitting parameters   build_fn  should construct, compile and return a Keras model, which\nwill then be used to fit/predict. One of the following\nthree values could be passed to build_fn:   A function  An instance of a class that implements the  call  method  None. This means you implement a class that inherits from either KerasClassifier  or  KerasRegressor . The  call  method of the\npresent class will then be treated as the default build_fn.   sk_params  takes both model parameters and fitting parameters. Legal model\nparameters are the arguments of  build_fn . Note that like all other\nestimators in scikit-learn, 'build_fn' should provide default values for\nits arguments, so that you could create the estimator without passing any\nvalues to  sk_params .  sk_params  could also accept parameters for calling  fit ,  predict , predict_proba , and  score  methods (e.g.,  nb_epoch ,  batch_size ).\nfitting (predicting) parameters are selected in the following order:   Values passed to the dictionary arguments of fit ,  predict ,  predict_proba , and  score  methods  Values passed to  sk_params  The default values of the  keras.models.Sequential  fit ,  predict ,  predict_proba  and  score  methods   When using scikit-learn's  grid_search  API, legal tunable parameters are\nthose you could pass to  sk_params , including fitting parameters.\nIn other words, you could use  grid_search  to search for the best batch_size  or  nb_epoch  as well as the model parameters.", 
            "title": "Arguments"
        }, 
        {
            "location": "/utils/data_utils/", 
            "text": "get_file\n\n\nget_file(fname, origin, untar=False, md5_hash=None, cache_subdir='datasets')\n\n\n\n\nDownloads a file from a URL if it not already in the cache.\n\n\nPassing the MD5 hash will verify the file after download\nas well as if it is already present in the cache.\n\n\nArguments\n\n\n\n\nfname\n: name of the file\n\n\norigin\n: original URL of the file\n\n\nuntar\n: boolean, whether the file should be decompressed\n\n\nmd5_hash\n: MD5 hash of the file for verification\n\n\ncache_subdir\n: directory being used as the cache\n\n\n\n\nReturns\n\n\nPath to the downloaded file", 
            "title": "Data Utils"
        }, 
        {
            "location": "/utils/data_utils/#get_file", 
            "text": "get_file(fname, origin, untar=False, md5_hash=None, cache_subdir='datasets')  Downloads a file from a URL if it not already in the cache.  Passing the MD5 hash will verify the file after download\nas well as if it is already present in the cache.  Arguments   fname : name of the file  origin : original URL of the file  untar : boolean, whether the file should be decompressed  md5_hash : MD5 hash of the file for verification  cache_subdir : directory being used as the cache   Returns  Path to the downloaded file", 
            "title": "get_file"
        }, 
        {
            "location": "/utils/io_utils/", 
            "text": "[source]\n\n\nHDF5Matrix\n\n\nkeras.utils.io_utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None)\n\n\n\n\nRepresentation of HDF5 dataset to be used instead of a Numpy array.\n\n\nExample\n\n\nx_data = HDF5Matrix('input/file.hdf5', 'data')\nmodel.predict(x_data)\n\n\n\n\nProviding \nstart\n and \nend\n allows use of a slice of the dataset.\n\n\nOptionally, a normalizer function (or lambda) can be given. This will\nbe called on every slice of data retrieved.\n\n\nArguments\n\n\n\n\ndatapath\n: string, path to a HDF5 file\n\n\ndataset\n: string, name of the HDF5 dataset in the file specified\n    in datapath\n\n\nstart\n: int, start of desired slice of the specified dataset\n\n\nend\n: int, end of desired slice of the specified dataset\n\n\nnormalizer\n: function to be called on data when retrieved\n\n\n\n\nReturns\n\n\nAn array-like HDF5 dataset.", 
            "title": "I/O Utils"
        }, 
        {
            "location": "/utils/io_utils/#hdf5matrix", 
            "text": "keras.utils.io_utils.HDF5Matrix(datapath, dataset, start=0, end=None, normalizer=None)  Representation of HDF5 dataset to be used instead of a Numpy array.  Example  x_data = HDF5Matrix('input/file.hdf5', 'data')\nmodel.predict(x_data)  Providing  start  and  end  allows use of a slice of the dataset.  Optionally, a normalizer function (or lambda) can be given. This will\nbe called on every slice of data retrieved.  Arguments   datapath : string, path to a HDF5 file  dataset : string, name of the HDF5 dataset in the file specified\n    in datapath  start : int, start of desired slice of the specified dataset  end : int, end of desired slice of the specified dataset  normalizer : function to be called on data when retrieved   Returns  An array-like HDF5 dataset.", 
            "title": "HDF5Matrix"
        }, 
        {
            "location": "/utils/layer_utils/", 
            "text": "layer_from_config\n\n\nlayer_from_config(config, custom_objects=None)\n\n\n\n\nInstantiate a layer from a config dictionary.\n\n\nArguments\n\n\n\n\nconfig\n: dict of the form {'class_name': str, 'config': dict}\n\n\ncustom_objects\n: dict mapping class names (or function names)\nof custom (non-Keras) objects to class/functions\n\n\n\n\nReturns\n\n\nLayer instance (may be Model, Sequential, Layer...)", 
            "title": "Layer Utils"
        }, 
        {
            "location": "/utils/layer_utils/#layer_from_config", 
            "text": "layer_from_config(config, custom_objects=None)  Instantiate a layer from a config dictionary.  Arguments   config : dict of the form {'class_name': str, 'config': dict}  custom_objects : dict mapping class names (or function names)\nof custom (non-Keras) objects to class/functions   Returns  Layer instance (may be Model, Sequential, Layer...)", 
            "title": "layer_from_config"
        }, 
        {
            "location": "/utils/np_utils/", 
            "text": "to_categorical\n\n\nto_categorical(y, nb_classes=None)\n\n\n\n\nConverts a class vector (integers) to binary class matrix.\n\n\nE.g. for use with categorical_crossentropy.\n\n\nArguments\n\n\n\n\ny\n: class vector to be converted into a matrix\n(integers from 0 to nb_classes).\n\n\nnb_classes\n: total number of classes.\n\n\n\n\nReturns\n\n\nA binary matrix representation of the input.\n\n\n\n\nconvert_kernel\n\n\nconvert_kernel(kernel, dim_ordering=None)\n\n\n\n\nConverts a Numpy kernel matrix from Theano format to TensorFlow format.\n\n\nAlso works reciprocally, since the transformation is its own inverse.\n\n\nArguments\n\n\n\n\nkernel\n: Numpy array (4D or 5D).\n\n\ndim_ordering\n: the data format.\n\n\n\n\nReturns\n\n\nThe converted kernel.\n\n\nRaises\n\n\n\n\nValueError\n: in case of invalid kernel shape or invalid dim_ordering.\n\n\n\n\n\n\nconv_output_length\n\n\nconv_output_length(input_length, filter_size, border_mode, stride, dilation=1)\n\n\n\n\nDetermines output length of a convolution given input length.\n\n\nArguments\n\n\n\n\ninput_length\n: integer.\n\n\nfilter_size\n: integer.\n\n\nborder_mode\n: one of \"same\", \"valid\", \"full\".\n\n\nstride\n: integer.\n\n\ndilation\n: dilation rate, integer.\n\n\n\n\nReturns\n\n\nThe output length (integer).\n\n\n\n\nconv_input_length\n\n\nconv_input_length(output_length, filter_size, border_mode, stride)\n\n\n\n\nDetermines input length of a convolution given output length.\n\n\nArguments\n\n\n\n\noutput_length\n: integer.\n\n\nfilter_size\n: integer.\n\n\nborder_mode\n: one of \"same\", \"valid\", \"full\".\n\n\nstride\n: integer.\n\n\n\n\nReturns\n\n\nThe input length (integer).", 
            "title": "Numpy Utils"
        }, 
        {
            "location": "/utils/np_utils/#to_categorical", 
            "text": "to_categorical(y, nb_classes=None)  Converts a class vector (integers) to binary class matrix.  E.g. for use with categorical_crossentropy.  Arguments   y : class vector to be converted into a matrix\n(integers from 0 to nb_classes).  nb_classes : total number of classes.   Returns  A binary matrix representation of the input.", 
            "title": "to_categorical"
        }, 
        {
            "location": "/utils/np_utils/#convert_kernel", 
            "text": "convert_kernel(kernel, dim_ordering=None)  Converts a Numpy kernel matrix from Theano format to TensorFlow format.  Also works reciprocally, since the transformation is its own inverse.  Arguments   kernel : Numpy array (4D or 5D).  dim_ordering : the data format.   Returns  The converted kernel.  Raises   ValueError : in case of invalid kernel shape or invalid dim_ordering.", 
            "title": "convert_kernel"
        }, 
        {
            "location": "/utils/np_utils/#conv_output_length", 
            "text": "conv_output_length(input_length, filter_size, border_mode, stride, dilation=1)  Determines output length of a convolution given input length.  Arguments   input_length : integer.  filter_size : integer.  border_mode : one of \"same\", \"valid\", \"full\".  stride : integer.  dilation : dilation rate, integer.   Returns  The output length (integer).", 
            "title": "conv_output_length"
        }, 
        {
            "location": "/utils/np_utils/#conv_input_length", 
            "text": "conv_input_length(output_length, filter_size, border_mode, stride)  Determines input length of a convolution given output length.  Arguments   output_length : integer.  filter_size : integer.  border_mode : one of \"same\", \"valid\", \"full\".  stride : integer.   Returns  The input length (integer).", 
            "title": "conv_input_length"
        }
    ]
}